{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "apple/swift", "file": "Optional.swift", "language": "swift", "commit_date": "2016-06-22 23:19:24.000 UTC", "content": "//===----------------------------------------------------------------------===//\n//\n// This source file is part of the Swift.org open source project\n//\n// Copyright (c) 2014 - 2017 Apple Inc. and the Swift project authors\n// Licensed under Apache License v2.0 with Runtime Library Exception\n//\n// See https://swift.org/LICENSE.txt for license information\n// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors\n//\n//===----------------------------------------------------------------------===//\n\n/// A type that represents either a wrapped value or `nil`, the absence of a\n/// value.\n///\n/// You use the `Optional` type whenever you use optional values, even if you\n/// never type the word `Optional`. Swift's type system usually shows the\n/// wrapped type's name with a trailing question mark (`?`) instead of showing\n/// the full type name. For example, if a variable has the type `Int?`, that's\n/// just another way of writing `Optional<Int>`. The shortened form is\n/// preferred for ease of reading and writing code.\n///\n/// The types of `shortForm` and `longForm` in the following code sample are\n/// the same:\n///\n///     let shortForm: Int? = Int(\"42\")\n///     let longForm: Optional<Int> = Int(\"42\")\n///\n/// The `Optional` type is an enumeration with two cases. `Optional.none` is\n/// equivalent to the `nil` literal. `Optional.some(Wrapped)` stores a wrapped\n/// value. For example:\n///\n///     let number: Int? = Optional.some(42)\n///     let noNumber: Int? = Optional.none\n///     print(noNumber == nil)\n///     // Prints \"true\"\n///\n/// You must unwrap the value of an `Optional` instance before you can use it\n/// in many contexts. Because Swift provides several ways to safely unwrap\n/// optional values, you can choose the one that helps you write clear,\n/// concise code.\n///\n/// The following examples use this dictionary of image names and file paths:\n///\n///     let imagePaths = [\"star\": \"/glyphs/star.png\",\n///                       \"portrait\": \"/images/content/portrait.jpg\",\n///                       \"spacer\": \"/images/shared/spacer.gif\"]\n///\n/// Getting a dictionary's value using a key returns an optional value, so\n/// `imagePaths[\"star\"]` has type `Optional<String>` or, written in the\n/// preferred manner, `String?`.\n///\n/// Optional Binding\n/// ----------------\n///\n/// To conditionally bind the wrapped value of an `Optional` instance to a new\n/// variable, use one of the optional binding control structures, including\n/// `if let`, `guard let`, and `switch`.\n///\n///     if let starPath = imagePaths[\"star\"] {\n///         print(\"The star image is at '\\(starPath)'\")\n///     } else {\n///         print(\"Couldn't find the star image\")\n///     }\n///     // Prints \"The star image is at '/glyphs/star.png'\"\n///\n/// Optional Chaining\n/// -----------------\n///\n/// To safely access the properties and methods of a wrapped instance, use the\n/// postfix optional chaining operator (postfix `?`). The following example uses\n/// optional chaining to access the `hasSuffix(_:)` method on a `String?`\n/// instance.\n///\n///     if let isPNG = imagePaths[\"star\"]?.hasSuffix(\".png\") {\n///         print(\"The star image is in PNG format\")\n///     }\n///     // Prints \"The star image is in PNG format\"\n///\n/// Using the Nil-Coalescing Operator\n/// ---------------------------------\n///\n/// Use the nil-coalescing operator (`??`) to supply a default value in case\n/// the `Optional` instance is `nil`. Here a default path is supplied for an\n/// image that is missing from `imagePaths`.\n///\n///     let defaultImagePath = \"/images/default.png\"\n///     let heartPath = imagePaths[\"heart\"] ?? defaultImagePath\n///     print(heartPath)\n///     // Prints \"/images/default.png\"\n///\n/// The `??` operator also works with another `Optional` instance on the\n/// right-hand side. As a result, you can chain multiple `??` operators\n/// together.\n///\n///     let shapePath = imagePaths[\"cir\"] ?? imagePaths[\"squ\"] ?? defaultImagePath\n///     print(shapePath)\n///     // Prints \"/images/default.png\"\n///\n/// Unconditional Unwrapping\n/// ------------------------\n///\n/// When you're certain that an instance of `Optional` contains a value, you\n/// can unconditionally unwrap the value by using the forced\n/// unwrap operator (postfix `!`). For example, the result of the failable `Int`\n/// initializer is unconditionally unwrapped in the example below.\n///\n///     let number = Int(\"42\")!\n///     print(number)\n///     // Prints \"42\"\n///\n/// You can also perform unconditional optional chaining by using the postfix\n/// `!` operator.\n///\n///     let isPNG = imagePaths[\"star\"]!.hasSuffix(\".png\")\n///     print(isPNG)\n///     // Prints \"true\"\n///\n/// Unconditionally unwrapping a `nil` instance with `!` triggers a runtime\n/// error.\n@_fixed_layout\npublic enum Optional<Wrapped> : ExpressibleByNilLiteral {\n  // The compiler has special knowledge of Optional<Wrapped>, including the fact\n  // that it is an `enum` with cases named `none` and `some`.\n\n  /// The absence of a value.\n  ///\n  /// In code, the absence of a value is typically written using the `nil`\n  /// literal rather than the explicit `.none` enumeration case.\n  case none\n\n  /// The presence of a value, stored as `Wrapped`.\n  case some(Wrapped)\n\n  /// Creates an instance that stores the given value.\n  @_inlineable // FIXME(sil-serialize-all)\n  @_transparent\n  public init(_ some: Wrapped) { self = .some(some) }\n\n  /// Evaluates the given closure when this `Optional` instance is not `nil`,\n  /// passing the unwrapped value as a parameter.\n  ///\n  /// Use the `map` method with a closure that returns a nonoptional value.\n  /// This example performs an arithmetic operation on an\n  /// optional integer.\n  ///\n  ///     let possibleNumber: Int? = Int(\"42\")\n  ///     let possibleSquare = possibleNumber.map { $0 * $0 }\n  ///     print(possibleSquare)\n  ///     // Prints \"Optional(1764)\"\n  ///\n  ///     let noNumber: Int? = nil\n  ///     let noSquare = noNumber.map { $0 * $0 }\n  ///     print(noSquare)\n  ///     // Prints \"nil\"\n  ///\n  /// - Parameter transform: A closure that takes the unwrapped value\n  ///   of the instance.\n  /// - Returns: The result of the given closure. If this instance is `nil`,\n  ///   returns `nil`.\n  @_inlineable\n  public func map<U>(\n    _ transform: (Wrapped) throws -> U\n  ) rethrows -> U? {\n    switch self {\n    case .some(let y):\n      return .some(try transform(y))\n    case .none:\n      return .none\n    }\n  }\n\n  /// Evaluates the given closure when this `Optional` instance is not `nil`,\n  /// passing the unwrapped value as a parameter.\n  ///\n  /// Use the `flatMap` method with a closure that returns an optional value.\n  /// This example performs an arithmetic operation with an optional result on\n  /// an optional integer.\n  ///\n  ///     let possibleNumber: Int? = Int(\"42\")\n  ///     let nonOverflowingSquare = possibleNumber.flatMap { x -> Int? in\n  ///         let (result, overflowed) = x.multipliedReportingOverflow(by: x)\n  ///         return overflowed ? nil : result\n  ///     }\n  ///     print(nonOverflowingSquare)\n  ///     // Prints \"Optional(1764)\"\n  ///\n  /// - Parameter transform: A closure that takes the unwrapped value\n  ///   of the instance.  \n  /// - Returns: The result of the given closure. If this instance is `nil`,\n  ///   returns `nil`.\n  @_inlineable\n  public func flatMap<U>(\n    _ transform: (Wrapped) throws -> U?\n  ) rethrows -> U? {\n    switch self {\n    case .some(let y):\n      return try transform(y)\n    case .none:\n      return .none\n    }\n  }\n\n  /// Creates an instance initialized with `nil`.\n  ///\n  /// Do not call this initializer directly. It is used by the compiler when you\n  /// initialize an `Optional` instance with a `nil` literal. For example:\n  ///\n  ///     var i: Index? = nil\n  ///\n  /// In this example, the assignment to the `i` variable calls this\n  /// initializer behind the scenes.\n  @_inlineable // FIXME(sil-serialize-all)\n  @_transparent\n  public init(nilLiteral: ()) {\n    self = .none\n  }\n\n  /// The wrapped value of this instance, unwrapped without checking whether\n  /// the instance is `nil`.\n  ///\n  /// The `unsafelyUnwrapped` property provides the same value as the forced\n  /// unwrap operator (postfix `!`). However, in optimized builds (`-O`), no\n  /// check is performed to ensure that the current instance actually has a\n  /// value. Accessing this property in the case of a `nil` value is a serious\n  /// programming error and could lead to undefined behavior or a runtime\n  /// error.\n  ///\n  /// In debug builds (`-Onone`), the `unsafelyUnwrapped` property has the same\n  /// behavior as using the postfix `!` operator and triggers a runtime error\n  /// if the instance is `nil`.\n  ///\n  /// The `unsafelyUnwrapped` property is recommended over calling the\n  /// `unsafeBitCast(_:)` function because the property is more restrictive\n  /// and because accessing the property still performs checking in debug\n  /// builds.\n  ///\n  /// - Warning: This property trades safety for performance.  Use\n  ///   `unsafelyUnwrapped` only when you are confident that this instance\n  ///   will never be equal to `nil` and only after you've tried using the\n  ///   postfix `!` operator.\n  @_inlineable\n  public var unsafelyUnwrapped: Wrapped {\n    @inline(__always)\n    get {\n      if let x = self {\n        return x\n      }\n      _debugPreconditionFailure(\"unsafelyUnwrapped of nil optional\")\n    }\n  }\n\n  /// - Returns: `unsafelyUnwrapped`.\n  ///\n  /// This version is for internal stdlib use; it avoids any checking\n  /// overhead for users, even in Debug builds.\n  @_inlineable\n  public // SPI(SwiftExperimental)\n  var _unsafelyUnwrappedUnchecked: Wrapped {\n    @inline(__always)\n    get {\n      if let x = self {\n        return x\n      }\n      _sanityCheckFailure(\"_unsafelyUnwrappedUnchecked of nil optional\")\n    }\n  }\n}\n\nextension Optional : CustomDebugStringConvertible {\n  /// A textual representation of this instance, suitable for debugging.\n  @_inlineable // FIXME(sil-serialize-all)\n  public var debugDescription: String {\n    switch self {\n    case .some(let value):\n      var result = \"Optional(\"\n      debugPrint(value, terminator: \"\", to: &result)\n      result += \")\"\n      return result\n    case .none:\n      return \"nil\"\n    }\n  }\n}\n\nextension Optional : CustomReflectable {\n  @_inlineable // FIXME(sil-serialize-all)\n  public var customMirror: Mirror {\n    switch self {\n    case .some(let value):\n      return Mirror(\n        self,\n        children: [ \"some\": value ],\n        displayStyle: .optional)\n    case .none:\n      return Mirror(self, children: [:], displayStyle: .optional)\n    }\n  }\n}\n\n@_inlineable // FIXME(sil-serialize-all)\n@_transparent\npublic // COMPILER_INTRINSIC\nfunc _diagnoseUnexpectedNilOptional(_filenameStart: Builtin.RawPointer,\n                                    _filenameLength: Builtin.Word,\n                                    _filenameIsASCII: Builtin.Int1,\n                                    _line: Builtin.Word) {\n  _preconditionFailure(\n    \"Unexpectedly found nil while unwrapping an Optional value\",\n    file: StaticString(_start: _filenameStart,\n                       utf8CodeUnitCount: _filenameLength,\n                       isASCII: _filenameIsASCII),\n    line: UInt(_line))\n}\n\nextension Optional : Equatable where Wrapped : Equatable {\n  /// Returns a Boolean value indicating whether two optional instances are\n  /// equal.\n  ///\n  /// Use this equal-to operator (`==`) to compare any two optional instances of\n  /// a type that conforms to the `Equatable` protocol. The comparison returns\n  /// `true` if both arguments are `nil` or if the two arguments wrap values\n  /// that are equal. Conversely, the comparison returns `false` if only one of\n  /// the arguments is `nil` or if the two arguments wrap values that are not\n  /// equal.\n  ///\n  ///     let group1 = [1, 2, 3, 4, 5]\n  ///     let group2 = [1, 3, 5, 7, 9]\n  ///     if group1.first == group2.first {\n  ///         print(\"The two groups start the same.\")\n  ///     }\n  ///     // Prints \"The two groups start the same.\"\n  ///\n  /// You can also use this operator to compare a non-optional value to an\n  /// optional that wraps the same type. The non-optional value is wrapped as an\n  /// optional before the comparison is made. In the following example, the\n  /// `numberToMatch` constant is wrapped as an optional before comparing to the\n  /// optional `numberFromString`:\n  ///\n  ///     let numberToFind: Int = 23\n  ///     let numberFromString: Int? = Int(\"23\")      // Optional(23)\n  ///     if numberToFind == numberFromString {\n  ///         print(\"It's a match!\")\n  ///     }\n  ///     // Prints \"It's a match!\"\n  ///\n  /// An instance that is expressed as a literal can also be used with this\n  /// operator. In the next example, an integer literal is compared with the\n  /// optional integer `numberFromString`. The literal `23` is inferred as an\n  /// `Int` instance and then wrapped as an optional before the comparison is\n  /// performed.\n  ///\n  ///     if 23 == numberFromString {\n  ///         print(\"It's a match!\")\n  ///     }\n  ///     // Prints \"It's a match!\"\n  ///\n  /// - Parameters:\n  ///   - lhs: An optional value to compare.\n  ///   - rhs: Another optional value to compare.\n  @_inlineable\n  public static func ==(lhs: Wrapped?, rhs: Wrapped?) -> Bool {\n    switch (lhs, rhs) {\n    case let (l?, r?):\n      return l == r\n    case (nil, nil):\n      return true\n    default:\n      return false\n    }\n  }\n  \n  /// Returns a Boolean value indicating whether two optional instances are not\n  /// equal.\n  ///\n  /// Use this not-equal-to operator (`!=`) to compare any two optional instances\n  /// of a type that conforms to the `Equatable` protocol. The comparison\n  /// returns `true` if only one of the arguments is `nil` or if the two\n  /// arguments wrap values that are not equal. The comparison returns `false`\n  /// if both arguments are `nil` or if the two arguments wrap values that are\n  /// equal.\n  ///\n  ///     let group1 = [2, 4, 6, 8, 10]\n  ///     let group2 = [1, 3, 5, 7, 9]\n  ///     if group1.first != group2.first {\n  ///         print(\"The two groups start differently.\")\n  ///     }\n  ///     // Prints \"The two groups start differently.\"\n  ///\n  /// You can also use this operator to compare a non-optional value to an\n  /// optional that wraps the same type. The non-optional value is wrapped as an\n  /// optional before the comparison is made. In this example, the\n  /// `numberToMatch` constant is wrapped as an optional before comparing to the\n  /// optional `numberFromString`:\n  ///\n  ///     let numberToFind: Int = 23\n  ///     let numberFromString: Int? = Int(\"not-a-number\")      // nil\n  ///     if numberToFind != numberFromString {\n  ///         print(\"No match.\")\n  ///     }\n  ///     // Prints \"No match.\"\n  ///\n  /// - Parameters:\n  ///   - lhs: An optional value to compare.\n  ///   - rhs: Another optional value to compare.\n  @_inlineable\n  public static func !=(lhs: Wrapped?, rhs: Wrapped?) -> Bool {\n    return !(lhs == rhs)\n  }\n}\n\n// Enable pattern matching against the nil literal, even if the element type\n// isn't equatable.\n@_fixed_layout\npublic struct _OptionalNilComparisonType : ExpressibleByNilLiteral {\n  /// Create an instance initialized with `nil`.\n  @_inlineable // FIXME(sil-serialize-all)\n  @_transparent\n  public init(nilLiteral: ()) {\n  }\n}\n\nextension Optional {\n  /// Returns a Boolean value indicating whether an argument matches `nil`.\n  ///\n  /// You can use the pattern-matching operator (`~=`) to test whether an\n  /// optional instance is `nil` even when the wrapped value's type does not\n  /// conform to the `Equatable` protocol. The pattern-matching operator is used\n  /// internally in `case` statements for pattern matching.\n  ///\n  /// The following example declares the `stream` variable as an optional\n  /// instance of a hypothetical `DataStream` type, and then uses a `switch`\n  /// statement to determine whether the stream is `nil` or has a configured\n  /// value. When evaluating the `nil` case of the `switch` statement, this\n  /// operator is called behind the scenes.\n  ///\n  ///     var stream: DataStream? = nil\n  ///     switch stream {\n  ///     case nil:\n  ///         print(\"No data stream is configured.\")\n  ///     case let x?:\n  ///         print(\"The data stream has \\(x.availableBytes) bytes available.\")\n  ///     }\n  ///     // Prints \"No data stream is configured.\"\n  ///\n  /// - Note: To test whether an instance is `nil` in an `if` statement, use the\n  ///   equal-to operator (`==`) instead of the pattern-matching operator. The\n  ///   pattern-matching operator is primarily intended to enable `case`\n  ///   statement pattern matching.\n  ///\n  /// - Parameters:\n  ///   - lhs: A `nil` literal.\n  ///   - rhs: A value to match against `nil`.\n  @_inlineable // FIXME(sil-serialize-all)\n  @_transparent\n  static public func ~=(lhs: _OptionalNilComparisonType, rhs: Wrapped?) -> Bool {\n    switch rhs {\n    case .some(_):\n      return false\n    case .none:\n      return true\n    }\n  }\n\n  // Enable equality comparisons against the nil literal, even if the\n  // element type isn't equatable\n\n  /// Returns a Boolean value indicating whether the left-hand-side argument is\n  /// `nil`.\n  ///\n  /// You can use this equal-to operator (`==`) to test whether an optional\n  /// instance is `nil` even when the wrapped value's type does not conform to\n  /// the `Equatable` protocol.\n  ///\n  /// The following example declares the `stream` variable as an optional\n  /// instance of a hypothetical `DataStream` type. Although `DataStream` is not\n  /// an `Equatable` type, this operator allows checking whether `stream` is\n  /// `nil`.\n  ///\n  ///     var stream: DataStream? = nil\n  ///     if stream == nil {\n  ///         print(\"No data stream is configured.\")\n  ///     }\n  ///     // Prints \"No data stream is configured.\"\n  ///\n  /// - Parameters:\n  ///   - lhs: A value to compare to `nil`.\n  ///   - rhs: A `nil` literal.\n  @_inlineable // FIXME(sil-serialize-all)\n  @_transparent\n  static public func ==(lhs: Wrapped?, rhs: _OptionalNilComparisonType) -> Bool {\n    switch lhs {\n    case .some(_):\n      return false\n    case .none:\n      return true\n    }\n  }\n\n  /// Returns a Boolean value indicating whether the left-hand-side argument is\n  /// not `nil`.\n  ///\n  /// You can use this not-equal-to operator (`!=`) to test whether an optional\n  /// instance is not `nil` even when the wrapped value's type does not conform\n  /// to the `Equatable` protocol.\n  ///\n  /// The following example declares the `stream` variable as an optional\n  /// instance of a hypothetical `DataStream` type. Although `DataStream` is not\n  /// an `Equatable` type, this operator allows checking whether `stream` wraps\n  /// a value and is therefore not `nil`.\n  ///\n  ///     var stream: DataStream? = fetchDataStream()\n  ///     if stream != nil {\n  ///         print(\"The data stream has been configured.\")\n  ///     }\n  ///     // Prints \"The data stream has been configured.\"\n  ///\n  /// - Parameters:\n  ///   - lhs: A value to compare to `nil`.\n  ///   - rhs: A `nil` literal.\n  @_inlineable // FIXME(sil-serialize-all)\n  @_transparent\n  static public func !=(lhs: Wrapped?, rhs: _OptionalNilComparisonType) -> Bool {\n    switch lhs {\n    case .some(_):\n      return true\n    case .none:\n      return false\n    }\n  }\n\n  /// Returns a Boolean value indicating whether the right-hand-side argument is\n  /// `nil`.\n  ///\n  /// You can use this equal-to operator (`==`) to test whether an optional\n  /// instance is `nil` even when the wrapped value's type does not conform to\n  /// the `Equatable` protocol.\n  ///\n  /// The following example declares the `stream` variable as an optional\n  /// instance of a hypothetical `DataStream` type. Although `DataStream` is not\n  /// an `Equatable` type, this operator allows checking whether `stream` is\n  /// `nil`.\n  ///\n  ///     var stream: DataStream? = nil\n  ///     if nil == stream {\n  ///         print(\"No data stream is configured.\")\n  ///     }\n  ///     // Prints \"No data stream is configured.\"\n  ///\n  /// - Parameters:\n  ///   - lhs: A `nil` literal.\n  ///   - rhs: A value to compare to `nil`.\n  @_inlineable // FIXME(sil-serialize-all)\n  @_transparent\n  static public func ==(lhs: _OptionalNilComparisonType, rhs: Wrapped?) -> Bool {\n    switch rhs {\n    case .some(_):\n      return false\n    case .none:\n      return true\n    }\n  }\n\n  /// Returns a Boolean value indicating whether the right-hand-side argument is\n  /// not `nil`.\n  ///\n  /// You can use this not-equal-to operator (`!=`) to test whether an optional\n  /// instance is not `nil` even when the wrapped value's type does not conform\n  /// to the `Equatable` protocol.\n  ///\n  /// The following example declares the `stream` variable as an optional\n  /// instance of a hypothetical `DataStream` type. Although `DataStream` is not\n  /// an `Equatable` type, this operator allows checking whether `stream` wraps\n  /// a value and is therefore not `nil`.\n  ///\n  ///     var stream: DataStream? = fetchDataStream()\n  ///     if nil != stream {\n  ///         print(\"The data stream has been configured.\")\n  ///     }\n  ///     // Prints \"The data stream has been configured.\"\n  ///\n  /// - Parameters:\n  ///   - lhs: A `nil` literal.\n  ///   - rhs: A value to compare to `nil`.\n  @_inlineable // FIXME(sil-serialize-all)\n  @_transparent\n  static public func !=(lhs: _OptionalNilComparisonType, rhs: Wrapped?) -> Bool {\n    switch rhs {\n    case .some(_):\n      return true\n    case .none:\n      return false\n    }\n  }\n}\n\n/// Performs a nil-coalescing operation, returning the wrapped value of an\n/// `Optional` instance or a default value.\n///\n/// A nil-coalescing operation unwraps the left-hand side if it has a value, or\n/// it returns the right-hand side as a default. The result of this operation\n/// will have the nonoptional type of the left-hand side's `Wrapped` type.\n///\n/// This operator uses short-circuit evaluation: `optional` is checked first,\n/// and `defaultValue` is evaluated only if `optional` is `nil`. For example:\n///\n///     func getDefault() -> Int {\n///         print(\"Calculating default...\")\n///         return 42\n///     }\n///\n///     let goodNumber = Int(\"100\") ?? getDefault()\n///     // goodNumber == 100\n///\n///     let notSoGoodNumber = Int(\"invalid-input\") ?? getDefault()\n///     // Prints \"Calculating default...\"\n///     // notSoGoodNumber == 42\n///\n/// In this example, `goodNumber` is assigned a value of `100` because\n/// `Int(\"100\")` succeeded in returning a non-`nil` result. When\n/// `notSoGoodNumber` is initialized, `Int(\"invalid-input\")` fails and returns\n/// `nil`, and so the `getDefault()` method is called to supply a default\n/// value.\n///\n/// - Parameters:\n///   - optional: An optional value.\n///   - defaultValue: A value to use as a default. `defaultValue` is the same\n///     type as the `Wrapped` type of `optional`.\n@_inlineable // FIXME(sil-serialize-all)\n@_transparent\npublic func ?? <T>(optional: T?, defaultValue: @autoclosure () throws -> T)\n    rethrows -> T {\n  switch optional {\n  case .some(let value):\n    return value\n  case .none:\n    return try defaultValue()\n  }\n}\n\n/// Performs a nil-coalescing operation, returning the wrapped value of an\n/// `Optional` instance or a default `Optional` value.\n///\n/// A nil-coalescing operation unwraps the left-hand side if it has a value, or\n/// returns the right-hand side as a default. The result of this operation\n/// will be the same type as its arguments.\n///\n/// This operator uses short-circuit evaluation: `optional` is checked first,\n/// and `defaultValue` is evaluated only if `optional` is `nil`. For example:\n///\n///     let goodNumber = Int(\"100\") ?? Int(\"42\")\n///     print(goodNumber)\n///     // Prints \"Optional(100)\"\n///\n///     let notSoGoodNumber = Int(\"invalid-input\") ?? Int(\"42\")\n///     print(notSoGoodNumber)\n///     // Prints \"Optional(42)\"\n///\n/// In this example, `goodNumber` is assigned a value of `100` because\n/// `Int(\"100\")` succeeds in returning a non-`nil` result. When\n/// `notSoGoodNumber` is initialized, `Int(\"invalid-input\")` fails and returns\n/// `nil`, and so `Int(\"42\")` is called to supply a default value.\n///\n/// Because the result of this nil-coalescing operation is itself an optional\n/// value, you can chain default values by using `??` multiple times. The\n/// first optional value that isn't `nil` stops the chain and becomes the\n/// result of the whole expression. The next example tries to find the correct\n/// text for a greeting in two separate dictionaries before falling back to a\n/// static default.\n///\n///     let greeting = userPrefs[greetingKey] ??\n///         defaults[greetingKey] ?? \"Greetings!\"\n///\n/// If `userPrefs[greetingKey]` has a value, that value is assigned to\n/// `greeting`. If not, any value in `defaults[greetingKey]` will succeed, and\n/// if not that, `greeting` will be set to the non-optional default value,\n/// `\"Greetings!\"`.\n///\n/// - Parameters:\n///   - optional: An optional value.\n///   - defaultValue: A value to use as a default. `defaultValue` and\n///     `optional` have the same type.\n@_inlineable // FIXME(sil-serialize-all)\n@_transparent\npublic func ?? <T>(optional: T?, defaultValue: @autoclosure () throws -> T?)\n    rethrows -> T? {\n  switch optional {\n  case .some(let value):\n    return value\n  case .none:\n    return try defaultValue()\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// Bridging\n//===----------------------------------------------------------------------===//\n\n#if _runtime(_ObjC)\nextension Optional : _ObjectiveCBridgeable {\n  // The object that represents `none` for an Optional of this type.\n  @_inlineable // FIXME(sil-serialize-all)\n  @_versioned // FIXME(sil-serialize-all)\n  internal static var _nilSentinel : AnyObject {\n    @_silgen_name(\"_swift_Foundation_getOptionalNilSentinelObject\")\n    get\n  }\n\n  @_inlineable // FIXME(sil-serialize-all)\n  public func _bridgeToObjectiveC() -> AnyObject {\n    // Bridge a wrapped value by unwrapping.\n    if let value = self {\n      return _bridgeAnythingToObjectiveC(value)\n    }\n    // Bridge nil using a sentinel.\n    return type(of: self)._nilSentinel\n  }\n\n  @_inlineable // FIXME(sil-serialize-all)\n  public static func _forceBridgeFromObjectiveC(\n    _ source: AnyObject,\n    result: inout Optional<Wrapped>?\n  ) {\n    // Map the nil sentinel back to .none.\n    // NB that the signature of _forceBridgeFromObjectiveC adds another level\n    // of optionality, so we need to wrap the immediate result of the conversion\n    // in `.some`.\n    if source === _nilSentinel {\n      result = .some(.none)\n      return\n    }\n    // Otherwise, force-bridge the underlying value.\n    let unwrappedResult = source as! Wrapped\n    result = .some(.some(unwrappedResult))\n  }\n\n  @_inlineable // FIXME(sil-serialize-all)\n  public static func _conditionallyBridgeFromObjectiveC(\n    _ source: AnyObject,\n    result: inout Optional<Wrapped>?\n  ) -> Bool {\n    // Map the nil sentinel back to .none.\n    // NB that the signature of _forceBridgeFromObjectiveC adds another level\n    // of optionality, so we need to wrap the immediate result of the conversion\n    // in `.some` to indicate success of the bridging operation, with a nil\n    // result.\n    if source === _nilSentinel {\n      result = .some(.none)\n      return true\n    }\n    // Otherwise, try to bridge the underlying value.\n    if let unwrappedResult = source as? Wrapped {\n      result = .some(.some(unwrappedResult))\n      return true\n    } else {\n      result = .none\n      return false\n    }\n  }\n\n  @_inlineable // FIXME(sil-serialize-all)\n  public static func _unconditionallyBridgeFromObjectiveC(_ source: AnyObject?)\n      -> Optional<Wrapped> {\n    if let nonnullSource = source {\n      // Map the nil sentinel back to none.\n      if nonnullSource === _nilSentinel {\n        return .none\n      } else {\n        return .some(nonnullSource as! Wrapped)\n      }\n    } else {\n      // If we unexpectedly got nil, just map it to `none` too.\n      return .none\n    }\n  }\n}\n#endif\n", "subject": "[stdlib] constistently name param to map/flatMap", "message": "[stdlib] constistently name param to map/flatMap\n"}
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "apple/swift", "file": "CollectionTransformers.swift", "language": "swift", "commit_date": "2016-06-22 23:19:24.000 UTC", "content": "//===----------------------------------------------------------------------===//\n//\n// This source file is part of the Swift.org open source project\n//\n// Copyright (c) 2014 - 2017 Apple Inc. and the Swift project authors\n// Licensed under Apache License v2.0 with Runtime Library Exception\n//\n// See https://swift.org/LICENSE.txt for license information\n// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors\n//\n//===----------------------------------------------------------------------===//\n// RUN: %target-run-stdlib-swift\n// REQUIRES: executable_test\n\n// FIXME: This test runs very slowly on watchOS.\n// UNSUPPORTED: OS=watchos\n\npublic enum ApproximateCount {\n  case Unknown\n  case Precise(IntMax)\n  case Underestimate(IntMax)\n  case Overestimate(IntMax)\n}\n\npublic protocol ApproximateCountableSequence : Sequence {\n  /// Complexity: amortized O(1).\n  var approximateCount: ApproximateCount { get }\n}\n\n/// A collection that provides an efficient way to split its index ranges.\npublic protocol SplittableCollection : Collection {\n  // We need this protocol so that collections with only forward or bidirectional\n  // traversals could customize their splitting behavior.\n  //\n  // FIXME: all collections with random access should conform to this protocol\n  // automatically.\n\n  /// Splits a given range of indices into a set of disjoint ranges covering\n  /// the same elements.\n  ///\n  /// Complexity: amortized O(1).\n  ///\n  /// FIXME: should that be O(log n) to cover some strange collections?\n  ///\n  /// FIXME: index invalidation rules?\n  ///\n  /// FIXME: a better name.  Users will never want to call this method\n  /// directly.\n  ///\n  /// FIXME: return an optional for the common case when split() cannot\n  /// subdivide the range further.\n  func split(_ range: Range<Index>) -> [Range<Index>]\n}\n\ninternal func _splitRandomAccessIndexRange<\n  C : RandomAccessCollection\n>(\n  _ elements: C,\n  _ range: Range<C.Index>\n) -> [Range<C.Index>] {\n  let startIndex = range.lowerBound\n  let endIndex = range.upperBound\n  let length = elements.distance(from: startIndex, to: endIndex)\n  if length < 2 {\n    return [range]\n  }\n  let middle = elements.index(startIndex, offsetBy: length / 2)\n  return [startIndex ..< middle, middle ..< endIndex]\n}\n\n/// A helper object to build a collection incrementally in an efficient way.\n///\n/// Using a builder can be more efficient than creating an empty collection\n/// instance and adding elements one by one.\npublic protocol CollectionBuilder {\n  associatedtype Destination : Collection\n    \n  associatedtype Element = Destination.Iterator.Element\n\n  init()\n\n  /// Gives a hint about the expected approximate number of elements in the\n  /// collection that is being built.\n  mutating func sizeHint(_ approximateSize: Int)\n\n  /// Append `element` to `self`.\n  ///\n  /// If a collection being built supports a user-defined order, the element is\n  /// added at the end.\n  ///\n  /// Complexity: amortized O(1).\n  mutating func append(_ element: Destination.Iterator.Element)\n\n  /// Append `elements` to `self`.\n  ///\n  /// If a collection being built supports a user-defined order, the element is\n  /// added at the end.\n  ///\n  /// Complexity: amortized O(n), where `n` is equal to `count(elements)`.\n  mutating func append<\n    C : Collection\n  >(contentsOf elements: C)    \n  where C.Iterator.Element == Element\n\n\n  /// Append elements from `otherBuilder` to `self`, emptying `otherBuilder`.\n  ///\n  /// Equivalent to::\n  ///\n  ///   self.append(contentsOf: otherBuilder.takeResult())\n  ///\n  /// but is more efficient.\n  ///\n  /// Complexity: O(1).\n  mutating func moveContentsOf(_ otherBuilder: inout Self)\n\n  /// Build the collection from the elements that were added to this builder.\n  ///\n  /// Once this function is called, the builder may not be reused and no other\n  /// methods should be called.\n  ///\n  /// Complexity: O(n) or better (where `n` is the number of elements that were\n  /// added to this builder); typically O(1).\n  mutating func takeResult() -> Destination\n}\n\npublic protocol BuildableCollectionProtocol : Collection {\n  associatedtype Builder : CollectionBuilder\n}\n\nextension Array : SplittableCollection {\n  public func split(_ range: Range<Int>) -> [Range<Int>] {\n    return _splitRandomAccessIndexRange(self, range)\n  }\n}\n\npublic struct ArrayBuilder<T> : CollectionBuilder {\n  // FIXME: the compiler didn't complain when I remove public on 'Collection'.\n  // File a bug.\n  public typealias Destination = Array<T>\n  public typealias Element = T\n\n  internal var _resultParts = [[T]]()\n  internal var _resultTail = [T]()\n\n  public init() {}\n\n  public mutating func sizeHint(_ approximateSize: Int) {\n    _resultTail.reserveCapacity(approximateSize)\n  }\n\n  public mutating func append(_ element: T) {\n    _resultTail.append(element)\n  }\n\n  public mutating func append<\n    C : Collection\n  >(contentsOf elements: C)\n  where C.Iterator.Element == T {\n    _resultTail.append(contentsOf: elements)\n  }\n\n  public mutating func moveContentsOf(_ otherBuilder: inout ArrayBuilder<T>) {\n    // FIXME: do something smart with the capacity set in this builder and the\n    // other builder.\n    _resultParts.append(_resultTail)\n    _resultTail = []\n    // FIXME: not O(1)!\n    _resultParts.append(contentsOf: otherBuilder._resultParts)\n    otherBuilder._resultParts = []\n    swap(&_resultTail, &otherBuilder._resultTail)\n  }\n\n  public mutating func takeResult() -> Destination {\n    _resultParts.append(_resultTail)\n    _resultTail = []\n    // FIXME: optimize.  parallelize.\n    return Array(_resultParts.joined())\n  }\n}\n\nextension Array : BuildableCollectionProtocol {\n  public typealias Builder = ArrayBuilder<Element>\n}\n\n//===----------------------------------------------------------------------===//\n// Fork-join\n//===----------------------------------------------------------------------===//\n\n// As sad as it is, I think for practical performance reasons we should rewrite\n// the inner parts of the fork-join framework in C++.  In way too many cases\n// than necessary Swift requires an extra allocation to pin objects in memory\n// for safe multithreaded access.  -Dmitri\n\nimport SwiftShims\nimport SwiftPrivate\nimport Darwin\nimport Dispatch\n\n// FIXME: port to Linux.\n// XFAIL: linux\n\n// A wrapper for pthread_t with platform-independent interface.\npublic struct _stdlib_pthread_t : Equatable, Hashable {\n  internal let _value: pthread_t\n\n  public var hashValue: Int {\n    return _value.hashValue\n  }\n}\n\npublic func == (lhs: _stdlib_pthread_t, rhs: _stdlib_pthread_t) -> Bool {\n  return lhs._value == rhs._value\n}\n\npublic func _stdlib_pthread_self() -> _stdlib_pthread_t {\n  return _stdlib_pthread_t(_value: pthread_self())\n}\n\nstruct _ForkJoinMutex {\n  var _mutex: UnsafeMutablePointer<pthread_mutex_t>\n\n  init() {\n    _mutex = UnsafeMutablePointer.allocate(capacity: 1)\n    if pthread_mutex_init(_mutex, nil) != 0 {\n      fatalError(\"pthread_mutex_init\")\n    }\n  }\n\n  func `deinit`() {\n    if pthread_mutex_destroy(_mutex) != 0 {\n      fatalError(\"pthread_mutex_init\")\n    }\n    _mutex.deinitialize(count: 1)\n    _mutex.deallocate()\n  }\n\n  func withLock<Result>(_ body: () -> Result) -> Result {\n    if pthread_mutex_lock(_mutex) != 0 {\n      fatalError(\"pthread_mutex_lock\")\n    }\n    let result = body()\n    if pthread_mutex_unlock(_mutex) != 0 {\n      fatalError(\"pthread_mutex_unlock\")\n    }\n    return result\n  }\n}\n\nstruct _ForkJoinCond {\n  var _cond: UnsafeMutablePointer<pthread_cond_t>\n\n  init() {\n    _cond = UnsafeMutablePointer.allocate(capacity: 1)\n    if pthread_cond_init(_cond, nil) != 0 {\n      fatalError(\"pthread_cond_init\")\n    }\n  }\n\n  func `deinit`() {\n    if pthread_cond_destroy(_cond) != 0 {\n      fatalError(\"pthread_cond_destroy\")\n    }\n    _cond.deinitialize(count: 1)\n    _cond.deallocate()\n  }\n\n  func signal() {\n    pthread_cond_signal(_cond)\n  }\n\n  func wait(_ mutex: _ForkJoinMutex) {\n    pthread_cond_wait(_cond, mutex._mutex)\n  }\n}\n\nfinal class _ForkJoinOneShotEvent {\n  var _mutex: _ForkJoinMutex = _ForkJoinMutex()\n  var _cond: _ForkJoinCond = _ForkJoinCond()\n  var _isSet: Bool = false\n\n  init() {}\n\n  deinit {\n    _cond.`deinit`()\n    _mutex.`deinit`()\n  }\n\n  func set() {\n    _mutex.withLock {\n      if !_isSet {\n        _isSet = true\n        _cond.signal()\n      }\n    }\n  }\n\n  /// Establishes a happens-before relation between calls to set() and wait().\n  func wait() {\n    _mutex.withLock {\n      while !_isSet {\n        _cond.wait(_mutex)\n      }\n    }\n  }\n\n  /// If the function returns true, it establishes a happens-before relation\n  /// between calls to set() and isSet().\n  func isSet() -> Bool {\n    return _mutex.withLock {\n      return _isSet\n    }\n  }\n}\n\nfinal class _ForkJoinWorkDeque<T> {\n  // FIXME: this is just a proof-of-concept; very inefficient.\n\n  // Implementation note: adding elements to the head of the deque is common in\n  // fork-join, so _deque is stored reversed (appending to an array is cheap).\n  // FIXME: ^ that is false for submission queues though.\n  var _deque: ContiguousArray<T> = []\n  var _dequeMutex: _ForkJoinMutex = _ForkJoinMutex()\n\n  init() {}\n\n  deinit {\n    precondition(_deque.isEmpty)\n\n    _dequeMutex.`deinit`()\n  }\n\n  var isEmpty: Bool {\n    return _dequeMutex.withLock {\n      return _deque.isEmpty\n    }\n  }\n\n  func prepend(_ element: T) {\n    _dequeMutex.withLock {\n      _deque.append(element)\n    }\n  }\n\n  func tryTakeFirst() -> T? {\n    return _dequeMutex.withLock {\n      let result = _deque.last\n      if _deque.count > 0 {\n        _deque.removeLast()\n      }\n      return result\n    }\n  }\n\n  func tryTakeFirstTwo() -> (T?, T?) {\n    return _dequeMutex.withLock {\n      let result1 = _deque.last\n      if _deque.count > 0 {\n        _deque.removeLast()\n      }\n      let result2 = _deque.last\n      if _deque.count > 0 {\n        _deque.removeLast()\n      }\n      return (result1, result2)\n    }\n  }\n\n  func append(_ element: T) {\n    _dequeMutex.withLock {\n      _deque.insert(element, at: 0)\n    }\n  }\n\n  func tryTakeLast() -> T? {\n    return _dequeMutex.withLock {\n      let result = _deque.first\n      if _deque.count > 0 {\n        _deque.remove(at: 0)\n      }\n      return result\n    }\n  }\n\n  func takeAll() -> ContiguousArray<T> {\n    return _dequeMutex.withLock {\n      let result = _deque\n      _deque = []\n      return result\n    }\n  }\n\n  func tryReplace(\n    _ value: T,\n    makeReplacement: @escaping () -> T,\n    isEquivalent: @escaping (T, T) -> Bool\n  ) -> Bool {\n    return _dequeMutex.withLock {\n      for i in _deque.indices {\n        if isEquivalent(_deque[i], value) {\n          _deque[i] = makeReplacement()\n          return true\n        }\n      }\n      return false\n    }\n  }\n}\n\nfinal class _ForkJoinWorkerThread {\n  internal var _tid: _stdlib_pthread_t?\n  internal let _pool: ForkJoinPool\n  internal let _submissionQueue: _ForkJoinWorkDeque<ForkJoinTaskBase>\n  internal let _workDeque: _ForkJoinWorkDeque<ForkJoinTaskBase>\n\n  internal init(\n    _pool: ForkJoinPool,\n    submissionQueue: _ForkJoinWorkDeque<ForkJoinTaskBase>,\n    workDeque: _ForkJoinWorkDeque<ForkJoinTaskBase>\n  ) {\n    self._tid = nil\n    self._pool = _pool\n    self._submissionQueue = submissionQueue\n    self._workDeque = workDeque\n  }\n\n  internal func startAsync() {\n    var queue: DispatchQueue?\n    if #available(OSX 10.10, iOS 8.0, *) {\n      queue = DispatchQueue.global(qos: .background)\n    } else {\n      queue = DispatchQueue.global(priority: .background)\n    }\n    queue!.async {\n      self._thread()\n    }\n  }\n\n  internal func _thread() {\n    print(\"_ForkJoinWorkerThread begin\")\n    _tid = _stdlib_pthread_self()\n    outer: while !_workDeque.isEmpty || !_submissionQueue.isEmpty {\n      _pool._addRunningThread(self)\n      while true {\n        if _pool._tryStopThread() {\n          print(\"_ForkJoinWorkerThread detected too many threads\")\n          _pool._removeRunningThread(self)\n          _pool._submitTasksToRandomWorkers(_workDeque.takeAll())\n          _pool._submitTasksToRandomWorkers(_submissionQueue.takeAll())\n          print(\"_ForkJoinWorkerThread end\")\n          return\n        }\n\n        // Process tasks in FIFO order: first the work queue, then the\n        // submission queue.\n        if let task = _workDeque.tryTakeFirst() {\n          task._run()\n          continue\n        }\n        if let task = _submissionQueue.tryTakeFirst() {\n          task._run()\n          continue\n        }\n\n        print(\"_ForkJoinWorkerThread stealing tasks\")\n        if let task = _pool._stealTask() {\n          task._run()\n          continue\n        }\n\n        // FIXME: steal from submission queues?\n\n        break\n      }\n      _pool._removeRunningThread(self)\n    }\n    assert(_workDeque.isEmpty)\n    assert(_submissionQueue.isEmpty)\n    _ = _pool._totalThreads.fetchAndAdd(-1)\n    print(\"_ForkJoinWorkerThread end\")\n  }\n\n  internal func _forkTask(_ task: ForkJoinTaskBase) {\n    // Try to inflate the pool.\n    if !_pool._tryCreateThread({ task }) {\n      _workDeque.prepend(task)\n    }\n  }\n\n  internal func _waitForTask(_ task: ForkJoinTaskBase) {\n    while true {\n      if task._isComplete() {\n        return\n      }\n\n      // If the task is in work queue of the current thread, run the task.\n      if _workDeque.tryReplace(\n        task,\n        makeReplacement: { ForkJoinTask<()>() {} },\n        isEquivalent: { $0 === $1 }) {\n\n        // We found the task.  Run it in-place.\n        task._run()\n        return\n      }\n\n      // FIXME: also check the submission queue, maybe the task is there?\n\n      // FIXME: try to find the task in other threads' queues.\n\n      // FIXME: try to find tasks that were forked from this task in other\n      // threads' queues.  Help thieves by stealing those tasks back.\n\n      // At this point, we can't do any work to help with running this task.\n      // We can't start new work either (if we do, we might end up creating\n      // more in-flight work than we can chew, and crash with out-of-memory\n      // errors).\n      _pool._compensateForBlockedWorkerThread() {\n        task._blockingWait()\n        // FIXME: do a timed wait, and retry stealing.\n      }\n    }\n  }\n}\n\ninternal protocol _Future {\n  associatedtype Result\n\n  /// Establishes a happens-before relation between completing the future and\n  /// the call to wait().\n  func wait()\n\n  func tryGetResult() -> Result?\n  func tryTakeResult() -> Result?\n\n  func waitAndGetResult() -> Result\n  func waitAndTakeResult() -> Result\n}\n\npublic class ForkJoinTaskBase {\n  final internal var _pool: ForkJoinPool?\n\n  // FIXME(performance): there is no need to create heavy-weight\n  // synchronization primitives every time.  We could start with a lightweight\n  // atomic int for the flag and inflate to a full event when needed.  Unless\n  // we really need to block in wait(), we would avoid creating an event.\n  final internal let _completedEvent: _ForkJoinOneShotEvent =\n    _ForkJoinOneShotEvent()\n\n  final internal func _isComplete() -> Bool {\n    return _completedEvent.isSet()\n  }\n\n  final internal func _blockingWait() {\n    _completedEvent.wait()\n  }\n\n  internal func _run() {\n    fatalError(\"implement\")\n  }\n\n  final public func fork() {\n    precondition(_pool == nil)\n    if let thread = ForkJoinPool._getCurrentThread() {\n      thread._forkTask(self)\n    } else {\n      // FIXME: decide if we want to allow this.\n      precondition(false)\n      ForkJoinPool.commonPool.forkTask(self)\n    }\n  }\n\n  final public func wait() {\n    if let thread = ForkJoinPool._getCurrentThread() {\n      thread._waitForTask(self)\n    } else {\n      _blockingWait()\n    }\n  }\n}\n\nfinal public class ForkJoinTask<Result> : ForkJoinTaskBase, _Future {\n  internal let _task: () -> Result\n  internal var _result: Result?\n\n  public init(_task: @escaping () -> Result) {\n    self._task = _task\n  }\n\n  override internal func _run() {\n    _complete(_task())\n  }\n\n  /// It is not allowed to call _complete() in a racy way.  Only one thread\n  /// should ever call _complete().\n  internal func _complete(_ result: Result) {\n    precondition(!_completedEvent.isSet())\n    _result = result\n    _completedEvent.set()\n  }\n\n  public func tryGetResult() -> Result? {\n    if _completedEvent.isSet() {\n      return _result\n    }\n    return nil\n  }\n\n  public func tryTakeResult() -> Result? {\n    if _completedEvent.isSet() {\n      let result = _result\n      _result = nil\n      return result\n    }\n    return nil\n  }\n\n  public func waitAndGetResult() -> Result {\n    wait()\n    return tryGetResult()!\n  }\n\n  public func waitAndTakeResult() -> Result {\n    wait()\n    return tryTakeResult()!\n  }\n}\n\nfinal public class ForkJoinPool {\n  internal static var _threadRegistry: [_stdlib_pthread_t : _ForkJoinWorkerThread] = [:]\n  internal static var _threadRegistryMutex: _ForkJoinMutex = _ForkJoinMutex()\n\n  internal static func _getCurrentThread() -> _ForkJoinWorkerThread? {\n    return _threadRegistryMutex.withLock {\n      return _threadRegistry[_stdlib_pthread_self()]\n    }\n  }\n\n  internal let _maxThreads: Int\n  /// Total number of threads: number of running threads plus the number of\n  /// threads that are preparing to start).\n  internal let _totalThreads: _stdlib_AtomicInt = _stdlib_AtomicInt(0)\n\n  internal var _runningThreads: [_ForkJoinWorkerThread] = []\n  internal var _runningThreadsMutex: _ForkJoinMutex = _ForkJoinMutex()\n\n  internal var _submissionQueues: [_ForkJoinWorkDeque<ForkJoinTaskBase>] = []\n  internal var _submissionQueuesMutex: _ForkJoinMutex = _ForkJoinMutex()\n\n  internal var _workDeques: [_ForkJoinWorkDeque<ForkJoinTaskBase>] = []\n  internal var _workDequesMutex: _ForkJoinMutex = _ForkJoinMutex()\n\n  internal init(_commonPool: ()) {\n    self._maxThreads = _stdlib_getHardwareConcurrency()\n  }\n\n  deinit {\n    _runningThreadsMutex.`deinit`()\n    _submissionQueuesMutex.`deinit`()\n    _workDequesMutex.`deinit`()\n  }\n\n  internal func _addRunningThread(_ thread: _ForkJoinWorkerThread) {\n    ForkJoinPool._threadRegistryMutex.withLock {\n      _runningThreadsMutex.withLock {\n        _submissionQueuesMutex.withLock {\n          _workDequesMutex.withLock {\n            ForkJoinPool._threadRegistry[thread._tid!] = thread\n            _runningThreads.append(thread)\n            _submissionQueues.append(thread._submissionQueue)\n            _workDeques.append(thread._workDeque)\n          }\n        }\n      }\n    }\n  }\n\n  internal func _removeRunningThread(_ thread: _ForkJoinWorkerThread) {\n    ForkJoinPool._threadRegistryMutex.withLock {\n      _runningThreadsMutex.withLock {\n        _submissionQueuesMutex.withLock {\n          _workDequesMutex.withLock {\n            let i = _runningThreads.index { $0 === thread }!\n            ForkJoinPool._threadRegistry[thread._tid!] = nil\n            _runningThreads.remove(at: i)\n            _submissionQueues.remove(at: i)\n            _workDeques.remove(at: i)\n          }\n        }\n      }\n    }\n  }\n\n  internal func _compensateForBlockedWorkerThread(_ blockingBody: @escaping () -> ()) {\n    // FIXME: limit the number of compensating threads.\n    let submissionQueue = _ForkJoinWorkDeque<ForkJoinTaskBase>()\n    let workDeque = _ForkJoinWorkDeque<ForkJoinTaskBase>()\n    let thread = _ForkJoinWorkerThread(\n      _pool: self, submissionQueue: submissionQueue, workDeque: workDeque)\n    thread.startAsync()\n    blockingBody()\n    _ = _totalThreads.fetchAndAdd(1)\n  }\n\n  internal func _tryCreateThread(\n    _ makeTask: () -> ForkJoinTaskBase?\n  ) -> Bool {\n    var success = false\n    var oldNumThreads = _totalThreads.load()\n    repeat {\n      if oldNumThreads >= _maxThreads {\n        return false\n      }\n      success = _totalThreads.compareExchange(\n        expected: &oldNumThreads, desired: oldNumThreads + 1)\n    } while !success\n    if let task = makeTask() {\n      let submissionQueue = _ForkJoinWorkDeque<ForkJoinTaskBase>()\n      let workDeque = _ForkJoinWorkDeque<ForkJoinTaskBase>()\n      workDeque.prepend(task)\n      let thread = _ForkJoinWorkerThread(\n        _pool: self, submissionQueue: submissionQueue, workDeque: workDeque)\n      thread.startAsync()\n    } else {\n      _ = _totalThreads.fetchAndAdd(-1)\n    }\n    return true\n  }\n\n  internal func _stealTask() -> ForkJoinTaskBase? {\n    return _workDequesMutex.withLock {\n      let randomOffset = pickRandom(_workDeques.indices)\n      let count = _workDeques.count\n      for i in _workDeques.indices {\n        let index = (i + randomOffset) % count\n        if let task = _workDeques[index].tryTakeLast() {\n          return task\n        }\n      }\n      return nil\n    }\n  }\n\n  /// Check if the pool has grown too large because of compensating\n  /// threads.\n  internal func _tryStopThread() -> Bool {\n    var success = false\n    var oldNumThreads = _totalThreads.load()\n    repeat {\n      // FIXME: magic number 2.\n      if oldNumThreads <= _maxThreads + 2 {\n        return false\n      }\n      success = _totalThreads.compareExchange(\n        expected: &oldNumThreads, desired: oldNumThreads - 1)\n    } while !success\n    return true\n  }\n\n  internal func _submitTasksToRandomWorkers<\n    C : Collection\n  >(_ tasks: C)\n  where C.Iterator.Element == ForkJoinTaskBase {\n    if tasks.isEmpty {\n      return\n    }\n    _submissionQueuesMutex.withLock {\n      precondition(!_submissionQueues.isEmpty)\n      for task in tasks {\n        pickRandom(_submissionQueues).append(task)\n      }\n    }\n  }\n\n  public func forkTask(_ task: ForkJoinTaskBase) {\n    while true {\n      // Try to inflate the pool first.\n      if _tryCreateThread({ task }) {\n        return\n      }\n\n      // Looks like we can't create more threads.  Submit the task to\n      // a random thread.\n      let done = _submissionQueuesMutex.withLock {\n        () -> Bool in\n        if !_submissionQueues.isEmpty {\n          pickRandom(_submissionQueues).append(task)\n          return true\n        }\n        return false\n      }\n      if done {\n        return\n      }\n    }\n  }\n\n  // FIXME: return a Future instead?\n  public func forkTask<Result>(task: @escaping () -> Result) -> ForkJoinTask<Result> {\n    let forkJoinTask = ForkJoinTask(_task: task)\n    forkTask(forkJoinTask)\n    return forkJoinTask\n  }\n\n  public static var commonPool = ForkJoinPool(_commonPool: ())\n\n  public static func invokeAll(_ tasks: ForkJoinTaskBase...) {\n    ForkJoinPool.invokeAll(tasks)\n  }\n\n  public static func invokeAll(_ tasks: [ForkJoinTaskBase]) {\n    if tasks.isEmpty {\n      return\n    }\n    if ForkJoinPool._getCurrentThread() != nil {\n      // Run the first task in this thread, fork the rest.\n      let first = tasks.first\n      for t in tasks.dropFirst() {\n        // FIXME: optimize forking in bulk.\n        t.fork()\n      }\n      first!._run()\n    } else {\n      // FIXME: decide if we want to allow this.\n      precondition(false)\n    }\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// Collection transformation DSL: implementation\n//===----------------------------------------------------------------------===//\n\ninternal protocol _CollectionTransformerStepProtocol /*: class*/ {\n  associatedtype PipelineInputElement\n  associatedtype OutputElement\n\n  func transform<\n    InputCollection : Collection,\n    Collector : _ElementCollector\n  >(\n    _ c: InputCollection,\n    _ range: Range<InputCollection.Index>,\n    _ collector: inout Collector\n  )\n  where\n  InputCollection.Iterator.Element == PipelineInputElement,\n  Collector.Element == OutputElement\n}\n\ninternal class _CollectionTransformerStep<PipelineInputElement_, OutputElement_>\n  : _CollectionTransformerStepProtocol {\n\n  typealias PipelineInputElement = PipelineInputElement_\n  typealias OutputElement = OutputElement_\n\n  func map<U>(_ transform: @escaping (OutputElement) -> U)\n    -> _CollectionTransformerStep<PipelineInputElement, U> {\n\n    fatalError(\"abstract method\")\n  }\n\n  func filter(_ isIncluded: @escaping (OutputElement) -> Bool)\n    -> _CollectionTransformerStep<PipelineInputElement, OutputElement> {\n\n    fatalError(\"abstract method\")\n  }\n\n  func reduce<U>(_ initial: U, _ combine: @escaping (U, OutputElement) -> U)\n    -> _CollectionTransformerFinalizer<PipelineInputElement, U> {\n\n    fatalError(\"abstract method\")\n  }\n\n  func collectTo<\n    C : BuildableCollectionProtocol\n  >(_: C.Type) -> _CollectionTransformerFinalizer<PipelineInputElement, C>\n  where\n  C.Builder.Destination == C,\n  C.Builder.Element == C.Iterator.Element,\n  C.Iterator.Element == OutputElement {\n\n    fatalError(\"abstract method\")\n  }\n\n  func transform<\n    InputCollection : Collection,\n    Collector : _ElementCollector\n  >(\n    _ c: InputCollection,\n    _ range: Range<InputCollection.Index>,\n    _ collector: inout Collector\n  )\n  where\n  InputCollection.Iterator.Element == PipelineInputElement,\n  Collector.Element == OutputElement {\n    fatalError(\"abstract method\")\n  }\n}\n\nfinal internal class _CollectionTransformerStepCollectionSource<\n  PipelineInputElement\n> : _CollectionTransformerStep<PipelineInputElement, PipelineInputElement> {\n\n  typealias InputElement = PipelineInputElement\n\n  override func map<U>(_ transform: @escaping (InputElement) -> U)\n    -> _CollectionTransformerStep<PipelineInputElement, U> {\n\n    return _CollectionTransformerStepOneToMaybeOne(self) {\n      transform($0)\n    }\n  }\n\n  override func filter(_ isIncluded: @escaping (InputElement) -> Bool)\n    -> _CollectionTransformerStep<PipelineInputElement, InputElement> {\n\n    return _CollectionTransformerStepOneToMaybeOne(self) {\n      isIncluded($0) ? $0 : nil\n    }\n  }\n\n  override func reduce<U>(_ initial: U, _ combine: @escaping (U, InputElement) -> U)\n    -> _CollectionTransformerFinalizer<PipelineInputElement, U> {\n\n    return _CollectionTransformerFinalizerReduce(self, initial, combine)\n  }\n\n  override func collectTo<\n    C : BuildableCollectionProtocol\n  >(_ c: C.Type) -> _CollectionTransformerFinalizer<PipelineInputElement, C>\n  where\n  C.Builder.Destination == C,\n  C.Builder.Element == C.Iterator.Element,\n  C.Iterator.Element == OutputElement {\n\n    return _CollectionTransformerFinalizerCollectTo(self, c)\n  }\n\n  override func transform<\n    InputCollection : Collection,\n    Collector : _ElementCollector\n  >(\n    _ c: InputCollection,\n    _ range: Range<InputCollection.Index>,\n    _ collector: inout Collector\n  )\n  where\n  InputCollection.Iterator.Element == PipelineInputElement,\n  Collector.Element == OutputElement {\n    var i = range.lowerBound\n    while i != range.upperBound {\n      let e = c[i]\n      collector.append(e)\n      c.formIndex(after: &i)\n    }\n  }\n}\n\nfinal internal class _CollectionTransformerStepOneToMaybeOne<\n  PipelineInputElement,\n  OutputElement,\n  InputStep : _CollectionTransformerStepProtocol\n> : _CollectionTransformerStep<PipelineInputElement, OutputElement>\nwhere InputStep.PipelineInputElement == PipelineInputElement {\n\n  typealias _Self = _CollectionTransformerStepOneToMaybeOne\n  typealias InputElement = InputStep.OutputElement\n\n  let _input: InputStep\n  let _transform: (InputElement) -> OutputElement?\n\n  init(_ input: InputStep, _ transform: @escaping (InputElement) -> OutputElement?) {\n    self._input = input\n    self._transform = transform\n    super.init()\n  }\n\n  override func map<U>(_ transform: @escaping (OutputElement) -> U)\n    -> _CollectionTransformerStep<PipelineInputElement, U> {\n\n    // Let the closure below capture only one variable, not the whole `self`.\n    let localTransform = _transform\n    return _CollectionTransformerStepOneToMaybeOne<PipelineInputElement, U, InputStep>(_input) {\n      (input: InputElement) -> U? in\n      if let e = localTransform(input) {\n        return transform(e)\n      }\n      return nil\n    }\n  }\n\n  override func filter(_ isIncluded: @escaping (OutputElement) -> Bool)\n    -> _CollectionTransformerStep<PipelineInputElement, OutputElement> {\n\n    // Let the closure below capture only one variable, not the whole `self`.\n    let localTransform = _transform\n    return _CollectionTransformerStepOneToMaybeOne<PipelineInputElement, OutputElement, InputStep>(_input) {\n      (input: InputElement) -> OutputElement? in\n      if let e = localTransform(input) {\n        return isIncluded(e) ? e : nil\n      }\n      return nil\n    }\n  }\n\n  override func reduce<U>(_ initial: U, _ combine: @escaping (U, OutputElement) -> U)\n    -> _CollectionTransformerFinalizer<PipelineInputElement, U> {\n\n    return _CollectionTransformerFinalizerReduce(self, initial, combine)\n  }\n\n  override func collectTo<\n    C : BuildableCollectionProtocol\n  >(_ c: C.Type) -> _CollectionTransformerFinalizer<PipelineInputElement, C>\n  where\n  C.Builder.Destination == C,\n  C.Builder.Element == C.Iterator.Element,\n  C.Iterator.Element == OutputElement {\n\n    return _CollectionTransformerFinalizerCollectTo(self, c)\n  }\n\n  override func transform<\n    InputCollection : Collection,\n    Collector : _ElementCollector\n  >(\n    _ c: InputCollection,\n    _ range: Range<InputCollection.Index>,\n    _ collector: inout Collector\n  )\n  where\n  InputCollection.Iterator.Element == PipelineInputElement,\n  Collector.Element == OutputElement {\n    var collectorWrapper =\n      _ElementCollectorOneToMaybeOne(collector, _transform)\n    _input.transform(c, range, &collectorWrapper)\n    collector = collectorWrapper._baseCollector\n  }\n}\n\nstruct _ElementCollectorOneToMaybeOne<\n  BaseCollector : _ElementCollector,\n  Element_\n> : _ElementCollector {\n  typealias Element = Element_\n\n  var _baseCollector: BaseCollector\n  var _transform: (Element) -> BaseCollector.Element?\n\n  init(\n    _ baseCollector: BaseCollector,\n    _ transform: @escaping (Element) -> BaseCollector.Element?\n  ) {\n    self._baseCollector = baseCollector\n    self._transform = transform\n  }\n\n  mutating func sizeHint(_ approximateSize: Int) {}\n\n  mutating func append(_ element: Element) {\n    if let e = _transform(element) {\n      _baseCollector.append(e)\n    }\n  }\n\n  mutating func append<\n    C : Collection\n  >(contentsOf elements: C)\n  where C.Iterator.Element == Element {\n    for e in elements {\n      append(e)\n    }\n  }\n}\n\nprotocol _ElementCollector {\n  associatedtype Element\n\n  mutating func sizeHint(_ approximateSize: Int)\n\n  mutating func append(_ element: Element)\n\n  mutating func append<\n    C : Collection\n  >(contentsOf elements: C)\n  where C.Iterator.Element == Element\n}\n\nclass _CollectionTransformerFinalizer<PipelineInputElement, Result> {\n  func transform<\n    InputCollection : Collection\n  >(_ c: InputCollection) -> Result\n  where InputCollection.Iterator.Element == PipelineInputElement {\n    fatalError(\"implement\")\n  }\n}\n\nfinal class _CollectionTransformerFinalizerReduce<\n  PipelineInputElement,\n  U,\n  InputElementTy,\n  InputStep : _CollectionTransformerStepProtocol\n> : _CollectionTransformerFinalizer<PipelineInputElement, U>\nwhere\nInputStep.OutputElement == InputElementTy,\nInputStep.PipelineInputElement == PipelineInputElement {\n\n  var _input: InputStep\n  var _initial: U\n  var _combine: (U, InputElementTy) -> U\n\n  init(_ input: InputStep, _ initial: U, _ combine: @escaping (U, InputElementTy) -> U) {\n    self._input = input\n    self._initial = initial\n    self._combine = combine\n  }\n\n  override func transform<\n    InputCollection : Collection\n  >(_ c: InputCollection) -> U\n  where InputCollection.Iterator.Element == PipelineInputElement {\n    var collector = _ElementCollectorReduce(_initial, _combine)\n    _input.transform(c, c.startIndex..<c.endIndex, &collector)\n    return collector.takeResult()\n  }\n}\n\nstruct _ElementCollectorReduce<Element_, Result> : _ElementCollector {\n  typealias Element = Element_\n\n  var _current: Result\n  var _combine: (Result, Element) -> Result\n\n  init(_ initial: Result, _ combine: @escaping (Result, Element) -> Result) {\n    self._current = initial\n    self._combine = combine\n  }\n\n  mutating func sizeHint(_ approximateSize: Int) {}\n\n  mutating func append(_ element: Element) {\n    _current = _combine(_current, element)\n  }\n\n  mutating func append<\n    C : Collection\n  >(contentsOf elements: C)\n  where C.Iterator.Element == Element {\n    for e in elements {\n      append(e)\n    }\n  }\n\n  mutating func takeResult() -> Result {\n    return _current\n  }\n}\n\nfinal class _CollectionTransformerFinalizerCollectTo<\n  PipelineInputElement,\n  U : BuildableCollectionProtocol,\n  InputElementTy,\n  InputStep : _CollectionTransformerStepProtocol\n> : _CollectionTransformerFinalizer<PipelineInputElement, U>\nwhere\nInputStep.OutputElement == InputElementTy,\nInputStep.PipelineInputElement == PipelineInputElement,\nU.Builder.Destination == U,\nU.Builder.Element == U.Iterator.Element,\nU.Iterator.Element == InputStep.OutputElement {\n\n  var _input: InputStep\n\n  init(_ input: InputStep, _: U.Type) {\n    self._input = input\n  }\n\n  override func transform<\n    InputCollection : Collection\n  >(_ c: InputCollection) -> U\n  where InputCollection.Iterator.Element == PipelineInputElement {\n    var collector = _ElementCollectorCollectTo<U>()\n    _input.transform(c, c.startIndex..<c.endIndex, &collector)\n    return collector.takeResult()\n  }\n}\n\nstruct _ElementCollectorCollectTo<\n  BuildableCollection : BuildableCollectionProtocol\n> : _ElementCollector\nwhere\nBuildableCollection.Builder.Destination == BuildableCollection,\nBuildableCollection.Builder.Element == BuildableCollection.Iterator.Element {\n\n  typealias Element = BuildableCollection.Iterator.Element\n\n  var _builder: BuildableCollection.Builder\n\n  init() {\n    self._builder = BuildableCollection.Builder()\n  }\n\n  mutating func sizeHint(_ approximateSize: Int) {\n    _builder.sizeHint(approximateSize)\n  }\n\n  mutating func append(_ element: Element) {\n    _builder.append(element)\n  }\n\n  mutating func append<\n    C : Collection\n  >(contentsOf elements: C)\n  where C.Iterator.Element == Element {\n    _builder.append(contentsOf: elements)\n  }\n\n  mutating func takeResult() -> BuildableCollection {\n    return _builder.takeResult()\n  }\n}\n\ninternal func _optimizeCollectionTransformer<PipelineInputElement, Result>(\n  _ transformer: _CollectionTransformerFinalizer<PipelineInputElement, Result>\n) -> _CollectionTransformerFinalizer<PipelineInputElement, Result> {\n  return transformer\n}\n\ninternal func _runCollectionTransformer<\n  InputCollection : Collection, Result\n>(\n  _ c: InputCollection,\n  _ transformer: _CollectionTransformerFinalizer<InputCollection.Iterator.Element, Result>\n) -> Result {\n  dump(transformer)\n  let optimized = _optimizeCollectionTransformer(transformer)\n  dump(optimized)\n  return transformer.transform(c)\n}\n\n//===----------------------------------------------------------------------===//\n// Collection transformation DSL: public interface\n//===----------------------------------------------------------------------===//\n\npublic struct CollectionTransformerPipeline<\n  InputCollection : Collection, T\n> {\n  internal var _input: InputCollection\n  internal var _step: _CollectionTransformerStep<InputCollection.Iterator.Element, T>\n\n  public func map<U>(_ transform: @escaping (T) -> U)\n    -> CollectionTransformerPipeline<InputCollection, U> {\n\n    return CollectionTransformerPipeline<InputCollection, U>(\n      _input: _input,\n      _step: _step.map(transform)\n    )\n  }\n\n  public func filter(_ isIncluded: @escaping (T) -> Bool)\n    -> CollectionTransformerPipeline<InputCollection, T> {\n\n    return CollectionTransformerPipeline<InputCollection, T>(\n      _input: _input,\n      _step: _step.filter(isIncluded)\n    )\n  }\n\n  public func reduce<U>(\n    _ initial: U, _ combine: @escaping (U, T) -> U\n  ) -> U {\n    return _runCollectionTransformer(_input, _step.reduce(initial, combine))\n  }\n\n  public func collectTo<\n    C : BuildableCollectionProtocol\n  >(_ c: C.Type) -> C\n  where\n  C.Builder.Destination == C,\n  C.Iterator.Element == T,\n  C.Builder.Element == T {\n    return _runCollectionTransformer(_input, _step.collectTo(c))\n  }\n\n  public func toArray() -> [T] {\n    return collectTo(Array<T>.self)\n  }\n}\n\npublic func transform<C : Collection>(_ c: C)\n  -> CollectionTransformerPipeline<C, C.Iterator.Element> {\n\n  return CollectionTransformerPipeline<C, C.Iterator.Element>(\n    _input: c,\n    _step: _CollectionTransformerStepCollectionSource<C.Iterator.Element>())\n}\n\n//===----------------------------------------------------------------------===//\n// Collection transformation DSL: tests\n//===----------------------------------------------------------------------===//\n\nimport StdlibUnittest\n\n\nvar t = TestSuite(\"t\")\n\nt.test(\"fusion/map+reduce\") {\n  let xs = [ 1, 2, 3 ]\n  let result =\n    transform(xs)\n    .map { $0 * 2 }\n    .reduce(0, { $0 + $1 })\n  expectEqual(12, result)\n}\n\nt.test(\"fusion/map+filter+reduce\") {\n  let xs = [ 1, 2, 3 ]\n  let result = transform(xs)\n    .map { $0 * 2 }\n    .filter { $0 != 0 }\n    .reduce(0, { $0 + $1 })\n  expectEqual(12, result)\n}\n\nt.test(\"fusion/map+collectTo\") {\n  let xs = [ 1, 2, 3 ]\n  let result =\n    transform(xs)\n    .map { $0 * 2 }\n    .collectTo(Array<Int>.self)\n  expectEqual([ 2, 4, 6 ], result)\n}\n\nt.test(\"fusion/map+toArray\") {\n  let xs = [ 1, 2, 3 ]\n  let result =\n    transform(xs)\n    .map { $0 * 2 }\n    .toArray()\n  expectEqual([ 2, 4, 6 ], result)\n}\n\nt.test(\"ForkJoinPool.forkTask\") {\n  var tasks: [ForkJoinTask<()>] = []\n  for i in 0..<100 {\n    tasks.append(ForkJoinPool.commonPool.forkTask {\n      () -> () in\n      var result = 1\n      for i in 0..<10000 {\n        result = result &* i\n        _blackHole(result)\n      }\n      return ()\n    })\n  }\n  for t in tasks {\n    t.wait()\n  }\n}\n\nfunc fib(_ n: Int) -> Int {\n  if n == 1 || n == 2 {\n    return 1\n  }\n  if n == 38 {\n    print(\"\\(pthread_self()) fib(\\(n))\")\n  }\n  if n < 39 {\n    let r = fib(n - 1) + fib(n - 2)\n    _blackHole(r)\n    return r\n  }\n  print(\"fib(\\(n))\")\n  let t1 = ForkJoinTask() { fib(n - 1) }\n  let t2 = ForkJoinTask() { fib(n - 2) }\n  ForkJoinPool.invokeAll(t1, t2)\n  return t2.waitAndGetResult() + t1.waitAndGetResult()\n}\n\nt.test(\"ForkJoinPool.forkTask/Fibonacci\") {\n  let t = ForkJoinPool.commonPool.forkTask { fib(40) }\n  expectEqual(102334155, t.waitAndGetResult())\n}\n\nfunc _parallelMap(_ input: [Int], transform: @escaping (Int) -> Int, range: Range<Int>)\n  -> Array<Int>.Builder {\n\n  var builder = Array<Int>.Builder()\n  if range.count < 1_000 {\n    builder.append(contentsOf: input[range].map(transform))\n  } else {\n    let tasks = input.split(range).map {\n      (subRange) in\n      ForkJoinTask<Array<Int>.Builder> {\n        _parallelMap(input, transform: transform, range: subRange)\n      }\n    }\n    ForkJoinPool.invokeAll(tasks)\n    for t in tasks {\n      var otherBuilder = t.waitAndGetResult()\n      builder.moveContentsOf(&otherBuilder)\n    }\n  }\n  return builder\n}\n\nfunc parallelMap(_ input: [Int], transform: @escaping (Int) -> Int) -> [Int] {\n  let t = ForkJoinPool.commonPool.forkTask {\n    _parallelMap(\n      input,\n      transform: transform,\n      range: input.startIndex..<input.endIndex)\n  }\n  var builder = t.waitAndGetResult()\n  return builder.takeResult()\n}\n\nt.test(\"ForkJoinPool.forkTask/MapArray\") {\n  expectEqual(\n    Array(2..<1_001),\n    parallelMap(Array(1..<1_000)) { $0 + 1 }\n  )\n}\n\n/*\n * FIXME: reduce compiler crasher\nt.test(\"ForkJoinPool.forkTask\") {\n  func fib(_ n: Int) -> Int {\n    if n == 0 || n == 1 {\n      return 1\n    }\n    let t1 = ForkJoinPool.commonPool.forkTask { fib(n - 1) }\n    let t2 = ForkJoinPool.commonPool.forkTask { fib(n - 2) }\n    return t2.waitAndGetResult() + t1.waitAndGetResult()\n  }\n  expectEqual(0, fib(10))\n}\n*/\n\n\n/*\n\nUseful links:\n\nhttp://habrahabr.ru/post/255659/\n\n*/\n\nrunAllTests()\n", "subject": "[stdlib] constistently name param to map/flatMap", "message": "[stdlib] constistently name param to map/flatMap\n" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "apple/swift", "file": "CollectionTransformers.swift", "language": "swift", "commit_date": "2016-06-22 23:18:56.000 UTC", "content": "//===----------------------------------------------------------------------===//\n//\n// This source file is part of the Swift.org open source project\n//\n// Copyright (c) 2014 - 2017 Apple Inc. and the Swift project authors\n// Licensed under Apache License v2.0 with Runtime Library Exception\n//\n// See https://swift.org/LICENSE.txt for license information\n// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors\n//\n//===----------------------------------------------------------------------===//\n// RUN: %target-run-stdlib-swift\n// REQUIRES: executable_test\n\n// FIXME: This test runs very slowly on watchOS.\n// UNSUPPORTED: OS=watchos\n\npublic enum ApproximateCount {\n  case Unknown\n  case Precise(IntMax)\n  case Underestimate(IntMax)\n  case Overestimate(IntMax)\n}\n\npublic protocol ApproximateCountableSequence : Sequence {\n  /// Complexity: amortized O(1).\n  var approximateCount: ApproximateCount { get }\n}\n\n/// A collection that provides an efficient way to split its index ranges.\npublic protocol SplittableCollection : Collection {\n  // We need this protocol so that collections with only forward or bidirectional\n  // traversals could customize their splitting behavior.\n  //\n  // FIXME: all collections with random access should conform to this protocol\n  // automatically.\n\n  /// Splits a given range of indices into a set of disjoint ranges covering\n  /// the same elements.\n  ///\n  /// Complexity: amortized O(1).\n  ///\n  /// FIXME: should that be O(log n) to cover some strange collections?\n  ///\n  /// FIXME: index invalidation rules?\n  ///\n  /// FIXME: a better name.  Users will never want to call this method\n  /// directly.\n  ///\n  /// FIXME: return an optional for the common case when split() cannot\n  /// subdivide the range further.\n  func split(_ range: Range<Index>) -> [Range<Index>]\n}\n\ninternal func _splitRandomAccessIndexRange<\n  C : RandomAccessCollection\n>(\n  _ elements: C,\n  _ range: Range<C.Index>\n) -> [Range<C.Index>] {\n  let startIndex = range.lowerBound\n  let endIndex = range.upperBound\n  let length = elements.distance(from: startIndex, to: endIndex)\n  if length < 2 {\n    return [range]\n  }\n  let middle = elements.index(startIndex, offsetBy: length / 2)\n  return [startIndex ..< middle, middle ..< endIndex]\n}\n\n/// A helper object to build a collection incrementally in an efficient way.\n///\n/// Using a builder can be more efficient than creating an empty collection\n/// instance and adding elements one by one.\npublic protocol CollectionBuilder {\n  associatedtype Destination : Collection\n    \n  associatedtype Element = Destination.Iterator.Element\n\n  init()\n\n  /// Gives a hint about the expected approximate number of elements in the\n  /// collection that is being built.\n  mutating func sizeHint(_ approximateSize: Int)\n\n  /// Append `element` to `self`.\n  ///\n  /// If a collection being built supports a user-defined order, the element is\n  /// added at the end.\n  ///\n  /// Complexity: amortized O(1).\n  mutating func append(_ element: Destination.Iterator.Element)\n\n  /// Append `elements` to `self`.\n  ///\n  /// If a collection being built supports a user-defined order, the element is\n  /// added at the end.\n  ///\n  /// Complexity: amortized O(n), where `n` is equal to `count(elements)`.\n  mutating func append<\n    C : Collection\n  >(contentsOf elements: C)    \n  where C.Iterator.Element == Element\n\n\n  /// Append elements from `otherBuilder` to `self`, emptying `otherBuilder`.\n  ///\n  /// Equivalent to::\n  ///\n  ///   self.append(contentsOf: otherBuilder.takeResult())\n  ///\n  /// but is more efficient.\n  ///\n  /// Complexity: O(1).\n  mutating func moveContentsOf(_ otherBuilder: inout Self)\n\n  /// Build the collection from the elements that were added to this builder.\n  ///\n  /// Once this function is called, the builder may not be reused and no other\n  /// methods should be called.\n  ///\n  /// Complexity: O(n) or better (where `n` is the number of elements that were\n  /// added to this builder); typically O(1).\n  mutating func takeResult() -> Destination\n}\n\npublic protocol BuildableCollectionProtocol : Collection {\n  associatedtype Builder : CollectionBuilder\n}\n\nextension Array : SplittableCollection {\n  public func split(_ range: Range<Int>) -> [Range<Int>] {\n    return _splitRandomAccessIndexRange(self, range)\n  }\n}\n\npublic struct ArrayBuilder<T> : CollectionBuilder {\n  // FIXME: the compiler didn't complain when I remove public on 'Collection'.\n  // File a bug.\n  public typealias Destination = Array<T>\n  public typealias Element = T\n\n  internal var _resultParts = [[T]]()\n  internal var _resultTail = [T]()\n\n  public init() {}\n\n  public mutating func sizeHint(_ approximateSize: Int) {\n    _resultTail.reserveCapacity(approximateSize)\n  }\n\n  public mutating func append(_ element: T) {\n    _resultTail.append(element)\n  }\n\n  public mutating func append<\n    C : Collection\n  >(contentsOf elements: C)\n  where C.Iterator.Element == T {\n    _resultTail.append(contentsOf: elements)\n  }\n\n  public mutating func moveContentsOf(_ otherBuilder: inout ArrayBuilder<T>) {\n    // FIXME: do something smart with the capacity set in this builder and the\n    // other builder.\n    _resultParts.append(_resultTail)\n    _resultTail = []\n    // FIXME: not O(1)!\n    _resultParts.append(contentsOf: otherBuilder._resultParts)\n    otherBuilder._resultParts = []\n    swap(&_resultTail, &otherBuilder._resultTail)\n  }\n\n  public mutating func takeResult() -> Destination {\n    _resultParts.append(_resultTail)\n    _resultTail = []\n    // FIXME: optimize.  parallelize.\n    return Array(_resultParts.joined())\n  }\n}\n\nextension Array : BuildableCollectionProtocol {\n  public typealias Builder = ArrayBuilder<Element>\n}\n\n//===----------------------------------------------------------------------===//\n// Fork-join\n//===----------------------------------------------------------------------===//\n\n// As sad as it is, I think for practical performance reasons we should rewrite\n// the inner parts of the fork-join framework in C++.  In way too many cases\n// than necessary Swift requires an extra allocation to pin objects in memory\n// for safe multithreaded access.  -Dmitri\n\nimport SwiftShims\nimport SwiftPrivate\nimport Darwin\nimport Dispatch\n\n// FIXME: port to Linux.\n// XFAIL: linux\n\n// A wrapper for pthread_t with platform-independent interface.\npublic struct _stdlib_pthread_t : Equatable, Hashable {\n  internal let _value: pthread_t\n\n  public var hashValue: Int {\n    return _value.hashValue\n  }\n}\n\npublic func == (lhs: _stdlib_pthread_t, rhs: _stdlib_pthread_t) -> Bool {\n  return lhs._value == rhs._value\n}\n\npublic func _stdlib_pthread_self() -> _stdlib_pthread_t {\n  return _stdlib_pthread_t(_value: pthread_self())\n}\n\nstruct _ForkJoinMutex {\n  var _mutex: UnsafeMutablePointer<pthread_mutex_t>\n\n  init() {\n    _mutex = UnsafeMutablePointer.allocate(capacity: 1)\n    if pthread_mutex_init(_mutex, nil) != 0 {\n      fatalError(\"pthread_mutex_init\")\n    }\n  }\n\n  func `deinit`() {\n    if pthread_mutex_destroy(_mutex) != 0 {\n      fatalError(\"pthread_mutex_init\")\n    }\n    _mutex.deinitialize(count: 1)\n    _mutex.deallocate()\n  }\n\n  func withLock<Result>(_ body: () -> Result) -> Result {\n    if pthread_mutex_lock(_mutex) != 0 {\n      fatalError(\"pthread_mutex_lock\")\n    }\n    let result = body()\n    if pthread_mutex_unlock(_mutex) != 0 {\n      fatalError(\"pthread_mutex_unlock\")\n    }\n    return result\n  }\n}\n\nstruct _ForkJoinCond {\n  var _cond: UnsafeMutablePointer<pthread_cond_t>\n\n  init() {\n    _cond = UnsafeMutablePointer.allocate(capacity: 1)\n    if pthread_cond_init(_cond, nil) != 0 {\n      fatalError(\"pthread_cond_init\")\n    }\n  }\n\n  func `deinit`() {\n    if pthread_cond_destroy(_cond) != 0 {\n      fatalError(\"pthread_cond_destroy\")\n    }\n    _cond.deinitialize(count: 1)\n    _cond.deallocate()\n  }\n\n  func signal() {\n    pthread_cond_signal(_cond)\n  }\n\n  func wait(_ mutex: _ForkJoinMutex) {\n    pthread_cond_wait(_cond, mutex._mutex)\n  }\n}\n\nfinal class _ForkJoinOneShotEvent {\n  var _mutex: _ForkJoinMutex = _ForkJoinMutex()\n  var _cond: _ForkJoinCond = _ForkJoinCond()\n  var _isSet: Bool = false\n\n  init() {}\n\n  deinit {\n    _cond.`deinit`()\n    _mutex.`deinit`()\n  }\n\n  func set() {\n    _mutex.withLock {\n      if !_isSet {\n        _isSet = true\n        _cond.signal()\n      }\n    }\n  }\n\n  /// Establishes a happens-before relation between calls to set() and wait().\n  func wait() {\n    _mutex.withLock {\n      while !_isSet {\n        _cond.wait(_mutex)\n      }\n    }\n  }\n\n  /// If the function returns true, it establishes a happens-before relation\n  /// between calls to set() and isSet().\n  func isSet() -> Bool {\n    return _mutex.withLock {\n      return _isSet\n    }\n  }\n}\n\nfinal class _ForkJoinWorkDeque<T> {\n  // FIXME: this is just a proof-of-concept; very inefficient.\n\n  // Implementation note: adding elements to the head of the deque is common in\n  // fork-join, so _deque is stored reversed (appending to an array is cheap).\n  // FIXME: ^ that is false for submission queues though.\n  var _deque: ContiguousArray<T> = []\n  var _dequeMutex: _ForkJoinMutex = _ForkJoinMutex()\n\n  init() {}\n\n  deinit {\n    precondition(_deque.isEmpty)\n\n    _dequeMutex.`deinit`()\n  }\n\n  var isEmpty: Bool {\n    return _dequeMutex.withLock {\n      return _deque.isEmpty\n    }\n  }\n\n  func prepend(_ element: T) {\n    _dequeMutex.withLock {\n      _deque.append(element)\n    }\n  }\n\n  func tryTakeFirst() -> T? {\n    return _dequeMutex.withLock {\n      let result = _deque.last\n      if _deque.count > 0 {\n        _deque.removeLast()\n      }\n      return result\n    }\n  }\n\n  func tryTakeFirstTwo() -> (T?, T?) {\n    return _dequeMutex.withLock {\n      let result1 = _deque.last\n      if _deque.count > 0 {\n        _deque.removeLast()\n      }\n      let result2 = _deque.last\n      if _deque.count > 0 {\n        _deque.removeLast()\n      }\n      return (result1, result2)\n    }\n  }\n\n  func append(_ element: T) {\n    _dequeMutex.withLock {\n      _deque.insert(element, at: 0)\n    }\n  }\n\n  func tryTakeLast() -> T? {\n    return _dequeMutex.withLock {\n      let result = _deque.first\n      if _deque.count > 0 {\n        _deque.remove(at: 0)\n      }\n      return result\n    }\n  }\n\n  func takeAll() -> ContiguousArray<T> {\n    return _dequeMutex.withLock {\n      let result = _deque\n      _deque = []\n      return result\n    }\n  }\n\n  func tryReplace(\n    _ value: T,\n    makeReplacement: @escaping () -> T,\n    isEquivalent: @escaping (T, T) -> Bool\n  ) -> Bool {\n    return _dequeMutex.withLock {\n      for i in _deque.indices {\n        if isEquivalent(_deque[i], value) {\n          _deque[i] = makeReplacement()\n          return true\n        }\n      }\n      return false\n    }\n  }\n}\n\nfinal class _ForkJoinWorkerThread {\n  internal var _tid: _stdlib_pthread_t?\n  internal let _pool: ForkJoinPool\n  internal let _submissionQueue: _ForkJoinWorkDeque<ForkJoinTaskBase>\n  internal let _workDeque: _ForkJoinWorkDeque<ForkJoinTaskBase>\n\n  internal init(\n    _pool: ForkJoinPool,\n    submissionQueue: _ForkJoinWorkDeque<ForkJoinTaskBase>,\n    workDeque: _ForkJoinWorkDeque<ForkJoinTaskBase>\n  ) {\n    self._tid = nil\n    self._pool = _pool\n    self._submissionQueue = submissionQueue\n    self._workDeque = workDeque\n  }\n\n  internal func startAsync() {\n    var queue: DispatchQueue?\n    if #available(OSX 10.10, iOS 8.0, *) {\n      queue = DispatchQueue.global(qos: .background)\n    } else {\n      queue = DispatchQueue.global(priority: .background)\n    }\n    queue!.async {\n      self._thread()\n    }\n  }\n\n  internal func _thread() {\n    print(\"_ForkJoinWorkerThread begin\")\n    _tid = _stdlib_pthread_self()\n    outer: while !_workDeque.isEmpty || !_submissionQueue.isEmpty {\n      _pool._addRunningThread(self)\n      while true {\n        if _pool._tryStopThread() {\n          print(\"_ForkJoinWorkerThread detected too many threads\")\n          _pool._removeRunningThread(self)\n          _pool._submitTasksToRandomWorkers(_workDeque.takeAll())\n          _pool._submitTasksToRandomWorkers(_submissionQueue.takeAll())\n          print(\"_ForkJoinWorkerThread end\")\n          return\n        }\n\n        // Process tasks in FIFO order: first the work queue, then the\n        // submission queue.\n        if let task = _workDeque.tryTakeFirst() {\n          task._run()\n          continue\n        }\n        if let task = _submissionQueue.tryTakeFirst() {\n          task._run()\n          continue\n        }\n\n        print(\"_ForkJoinWorkerThread stealing tasks\")\n        if let task = _pool._stealTask() {\n          task._run()\n          continue\n        }\n\n        // FIXME: steal from submission queues?\n\n        break\n      }\n      _pool._removeRunningThread(self)\n    }\n    assert(_workDeque.isEmpty)\n    assert(_submissionQueue.isEmpty)\n    _ = _pool._totalThreads.fetchAndAdd(-1)\n    print(\"_ForkJoinWorkerThread end\")\n  }\n\n  internal func _forkTask(_ task: ForkJoinTaskBase) {\n    // Try to inflate the pool.\n    if !_pool._tryCreateThread({ task }) {\n      _workDeque.prepend(task)\n    }\n  }\n\n  internal func _waitForTask(_ task: ForkJoinTaskBase) {\n    while true {\n      if task._isComplete() {\n        return\n      }\n\n      // If the task is in work queue of the current thread, run the task.\n      if _workDeque.tryReplace(\n        task,\n        makeReplacement: { ForkJoinTask<()>() {} },\n        isEquivalent: { $0 === $1 }) {\n\n        // We found the task.  Run it in-place.\n        task._run()\n        return\n      }\n\n      // FIXME: also check the submission queue, maybe the task is there?\n\n      // FIXME: try to find the task in other threads' queues.\n\n      // FIXME: try to find tasks that were forked from this task in other\n      // threads' queues.  Help thieves by stealing those tasks back.\n\n      // At this point, we can't do any work to help with running this task.\n      // We can't start new work either (if we do, we might end up creating\n      // more in-flight work than we can chew, and crash with out-of-memory\n      // errors).\n      _pool._compensateForBlockedWorkerThread() {\n        task._blockingWait()\n        // FIXME: do a timed wait, and retry stealing.\n      }\n    }\n  }\n}\n\ninternal protocol _Future {\n  associatedtype Result\n\n  /// Establishes a happens-before relation between completing the future and\n  /// the call to wait().\n  func wait()\n\n  func tryGetResult() -> Result?\n  func tryTakeResult() -> Result?\n\n  func waitAndGetResult() -> Result\n  func waitAndTakeResult() -> Result\n}\n\npublic class ForkJoinTaskBase {\n  final internal var _pool: ForkJoinPool?\n\n  // FIXME(performance): there is no need to create heavy-weight\n  // synchronization primitives every time.  We could start with a lightweight\n  // atomic int for the flag and inflate to a full event when needed.  Unless\n  // we really need to block in wait(), we would avoid creating an event.\n  final internal let _completedEvent: _ForkJoinOneShotEvent =\n    _ForkJoinOneShotEvent()\n\n  final internal func _isComplete() -> Bool {\n    return _completedEvent.isSet()\n  }\n\n  final internal func _blockingWait() {\n    _completedEvent.wait()\n  }\n\n  internal func _run() {\n    fatalError(\"implement\")\n  }\n\n  final public func fork() {\n    precondition(_pool == nil)\n    if let thread = ForkJoinPool._getCurrentThread() {\n      thread._forkTask(self)\n    } else {\n      // FIXME: decide if we want to allow this.\n      precondition(false)\n      ForkJoinPool.commonPool.forkTask(self)\n    }\n  }\n\n  final public func wait() {\n    if let thread = ForkJoinPool._getCurrentThread() {\n      thread._waitForTask(self)\n    } else {\n      _blockingWait()\n    }\n  }\n}\n\nfinal public class ForkJoinTask<Result> : ForkJoinTaskBase, _Future {\n  internal let _task: () -> Result\n  internal var _result: Result?\n\n  public init(_task: @escaping () -> Result) {\n    self._task = _task\n  }\n\n  override internal func _run() {\n    _complete(_task())\n  }\n\n  /// It is not allowed to call _complete() in a racy way.  Only one thread\n  /// should ever call _complete().\n  internal func _complete(_ result: Result) {\n    precondition(!_completedEvent.isSet())\n    _result = result\n    _completedEvent.set()\n  }\n\n  public func tryGetResult() -> Result? {\n    if _completedEvent.isSet() {\n      return _result\n    }\n    return nil\n  }\n\n  public func tryTakeResult() -> Result? {\n    if _completedEvent.isSet() {\n      let result = _result\n      _result = nil\n      return result\n    }\n    return nil\n  }\n\n  public func waitAndGetResult() -> Result {\n    wait()\n    return tryGetResult()!\n  }\n\n  public func waitAndTakeResult() -> Result {\n    wait()\n    return tryTakeResult()!\n  }\n}\n\nfinal public class ForkJoinPool {\n  internal static var _threadRegistry: [_stdlib_pthread_t : _ForkJoinWorkerThread] = [:]\n  internal static var _threadRegistryMutex: _ForkJoinMutex = _ForkJoinMutex()\n\n  internal static func _getCurrentThread() -> _ForkJoinWorkerThread? {\n    return _threadRegistryMutex.withLock {\n      return _threadRegistry[_stdlib_pthread_self()]\n    }\n  }\n\n  internal let _maxThreads: Int\n  /// Total number of threads: number of running threads plus the number of\n  /// threads that are preparing to start).\n  internal let _totalThreads: _stdlib_AtomicInt = _stdlib_AtomicInt(0)\n\n  internal var _runningThreads: [_ForkJoinWorkerThread] = []\n  internal var _runningThreadsMutex: _ForkJoinMutex = _ForkJoinMutex()\n\n  internal var _submissionQueues: [_ForkJoinWorkDeque<ForkJoinTaskBase>] = []\n  internal var _submissionQueuesMutex: _ForkJoinMutex = _ForkJoinMutex()\n\n  internal var _workDeques: [_ForkJoinWorkDeque<ForkJoinTaskBase>] = []\n  internal var _workDequesMutex: _ForkJoinMutex = _ForkJoinMutex()\n\n  internal init(_commonPool: ()) {\n    self._maxThreads = _stdlib_getHardwareConcurrency()\n  }\n\n  deinit {\n    _runningThreadsMutex.`deinit`()\n    _submissionQueuesMutex.`deinit`()\n    _workDequesMutex.`deinit`()\n  }\n\n  internal func _addRunningThread(_ thread: _ForkJoinWorkerThread) {\n    ForkJoinPool._threadRegistryMutex.withLock {\n      _runningThreadsMutex.withLock {\n        _submissionQueuesMutex.withLock {\n          _workDequesMutex.withLock {\n            ForkJoinPool._threadRegistry[thread._tid!] = thread\n            _runningThreads.append(thread)\n            _submissionQueues.append(thread._submissionQueue)\n            _workDeques.append(thread._workDeque)\n          }\n        }\n      }\n    }\n  }\n\n  internal func _removeRunningThread(_ thread: _ForkJoinWorkerThread) {\n    ForkJoinPool._threadRegistryMutex.withLock {\n      _runningThreadsMutex.withLock {\n        _submissionQueuesMutex.withLock {\n          _workDequesMutex.withLock {\n            let i = _runningThreads.index { $0 === thread }!\n            ForkJoinPool._threadRegistry[thread._tid!] = nil\n            _runningThreads.remove(at: i)\n            _submissionQueues.remove(at: i)\n            _workDeques.remove(at: i)\n          }\n        }\n      }\n    }\n  }\n\n  internal func _compensateForBlockedWorkerThread(_ blockingBody: @escaping () -> ()) {\n    // FIXME: limit the number of compensating threads.\n    let submissionQueue = _ForkJoinWorkDeque<ForkJoinTaskBase>()\n    let workDeque = _ForkJoinWorkDeque<ForkJoinTaskBase>()\n    let thread = _ForkJoinWorkerThread(\n      _pool: self, submissionQueue: submissionQueue, workDeque: workDeque)\n    thread.startAsync()\n    blockingBody()\n    _ = _totalThreads.fetchAndAdd(1)\n  }\n\n  internal func _tryCreateThread(\n    _ makeTask: () -> ForkJoinTaskBase?\n  ) -> Bool {\n    var success = false\n    var oldNumThreads = _totalThreads.load()\n    repeat {\n      if oldNumThreads >= _maxThreads {\n        return false\n      }\n      success = _totalThreads.compareExchange(\n        expected: &oldNumThreads, desired: oldNumThreads + 1)\n    } while !success\n    if let task = makeTask() {\n      let submissionQueue = _ForkJoinWorkDeque<ForkJoinTaskBase>()\n      let workDeque = _ForkJoinWorkDeque<ForkJoinTaskBase>()\n      workDeque.prepend(task)\n      let thread = _ForkJoinWorkerThread(\n        _pool: self, submissionQueue: submissionQueue, workDeque: workDeque)\n      thread.startAsync()\n    } else {\n      _ = _totalThreads.fetchAndAdd(-1)\n    }\n    return true\n  }\n\n  internal func _stealTask() -> ForkJoinTaskBase? {\n    return _workDequesMutex.withLock {\n      let randomOffset = pickRandom(_workDeques.indices)\n      let count = _workDeques.count\n      for i in _workDeques.indices {\n        let index = (i + randomOffset) % count\n        if let task = _workDeques[index].tryTakeLast() {\n          return task\n        }\n      }\n      return nil\n    }\n  }\n\n  /// Check if the pool has grown too large because of compensating\n  /// threads.\n  internal func _tryStopThread() -> Bool {\n    var success = false\n    var oldNumThreads = _totalThreads.load()\n    repeat {\n      // FIXME: magic number 2.\n      if oldNumThreads <= _maxThreads + 2 {\n        return false\n      }\n      success = _totalThreads.compareExchange(\n        expected: &oldNumThreads, desired: oldNumThreads - 1)\n    } while !success\n    return true\n  }\n\n  internal func _submitTasksToRandomWorkers<\n    C : Collection\n  >(_ tasks: C)\n  where C.Iterator.Element == ForkJoinTaskBase {\n    if tasks.isEmpty {\n      return\n    }\n    _submissionQueuesMutex.withLock {\n      precondition(!_submissionQueues.isEmpty)\n      for task in tasks {\n        pickRandom(_submissionQueues).append(task)\n      }\n    }\n  }\n\n  public func forkTask(_ task: ForkJoinTaskBase) {\n    while true {\n      // Try to inflate the pool first.\n      if _tryCreateThread({ task }) {\n        return\n      }\n\n      // Looks like we can't create more threads.  Submit the task to\n      // a random thread.\n      let done = _submissionQueuesMutex.withLock {\n        () -> Bool in\n        if !_submissionQueues.isEmpty {\n          pickRandom(_submissionQueues).append(task)\n          return true\n        }\n        return false\n      }\n      if done {\n        return\n      }\n    }\n  }\n\n  // FIXME: return a Future instead?\n  public func forkTask<Result>(task: @escaping () -> Result) -> ForkJoinTask<Result> {\n    let forkJoinTask = ForkJoinTask(_task: task)\n    forkTask(forkJoinTask)\n    return forkJoinTask\n  }\n\n  public static var commonPool = ForkJoinPool(_commonPool: ())\n\n  public static func invokeAll(_ tasks: ForkJoinTaskBase...) {\n    ForkJoinPool.invokeAll(tasks)\n  }\n\n  public static func invokeAll(_ tasks: [ForkJoinTaskBase]) {\n    if tasks.isEmpty {\n      return\n    }\n    if ForkJoinPool._getCurrentThread() != nil {\n      // Run the first task in this thread, fork the rest.\n      let first = tasks.first\n      for t in tasks.dropFirst() {\n        // FIXME: optimize forking in bulk.\n        t.fork()\n      }\n      first!._run()\n    } else {\n      // FIXME: decide if we want to allow this.\n      precondition(false)\n    }\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// Collection transformation DSL: implementation\n//===----------------------------------------------------------------------===//\n\ninternal protocol _CollectionTransformerStepProtocol /*: class*/ {\n  associatedtype PipelineInputElement\n  associatedtype OutputElement\n\n  func transform<\n    InputCollection : Collection,\n    Collector : _ElementCollector\n  >(\n    _ c: InputCollection,\n    _ range: Range<InputCollection.Index>,\n    _ collector: inout Collector\n  )\n  where\n  InputCollection.Iterator.Element == PipelineInputElement,\n  Collector.Element == OutputElement\n}\n\ninternal class _CollectionTransformerStep<PipelineInputElement_, OutputElement_>\n  : _CollectionTransformerStepProtocol {\n\n  typealias PipelineInputElement = PipelineInputElement_\n  typealias OutputElement = OutputElement_\n\n  func map<U>(_ transform: @escaping (OutputElement) -> U)\n    -> _CollectionTransformerStep<PipelineInputElement, U> {\n\n    fatalError(\"abstract method\")\n  }\n\n  func filter(_ isIncluded: @escaping (OutputElement) -> Bool)\n    -> _CollectionTransformerStep<PipelineInputElement, OutputElement> {\n\n    fatalError(\"abstract method\")\n  }\n\n  func reduce<U>(_ initial: U, _ combine: @escaping (U, OutputElement) -> U)\n    -> _CollectionTransformerFinalizer<PipelineInputElement, U> {\n\n    fatalError(\"abstract method\")\n  }\n\n  func collectTo<\n    C : BuildableCollectionProtocol\n  >(_: C.Type) -> _CollectionTransformerFinalizer<PipelineInputElement, C>\n  where\n  C.Builder.Destination == C,\n  C.Builder.Element == C.Iterator.Element,\n  C.Iterator.Element == OutputElement {\n\n    fatalError(\"abstract method\")\n  }\n\n  func transform<\n    InputCollection : Collection,\n    Collector : _ElementCollector\n  >(\n    _ c: InputCollection,\n    _ range: Range<InputCollection.Index>,\n    _ collector: inout Collector\n  )\n  where\n  InputCollection.Iterator.Element == PipelineInputElement,\n  Collector.Element == OutputElement {\n    fatalError(\"abstract method\")\n  }\n}\n\nfinal internal class _CollectionTransformerStepCollectionSource<\n  PipelineInputElement\n> : _CollectionTransformerStep<PipelineInputElement, PipelineInputElement> {\n\n  typealias InputElement = PipelineInputElement\n\n  override func map<U>(_ transform: @escaping (InputElement) -> U)\n    -> _CollectionTransformerStep<PipelineInputElement, U> {\n\n    return _CollectionTransformerStepOneToMaybeOne(self) {\n      transform($0)\n    }\n  }\n\n  override func filter(_ isIncluded: @escaping (InputElement) -> Bool)\n    -> _CollectionTransformerStep<PipelineInputElement, InputElement> {\n\n    return _CollectionTransformerStepOneToMaybeOne(self) {\n      isIncluded($0) ? $0 : nil\n    }\n  }\n\n  override func reduce<U>(_ initial: U, _ combine: @escaping (U, InputElement) -> U)\n    -> _CollectionTransformerFinalizer<PipelineInputElement, U> {\n\n    return _CollectionTransformerFinalizerReduce(self, initial, combine)\n  }\n\n  override func collectTo<\n    C : BuildableCollectionProtocol\n  >(_ c: C.Type) -> _CollectionTransformerFinalizer<PipelineInputElement, C>\n  where\n  C.Builder.Destination == C,\n  C.Builder.Element == C.Iterator.Element,\n  C.Iterator.Element == OutputElement {\n\n    return _CollectionTransformerFinalizerCollectTo(self, c)\n  }\n\n  override func transform<\n    InputCollection : Collection,\n    Collector : _ElementCollector\n  >(\n    _ c: InputCollection,\n    _ range: Range<InputCollection.Index>,\n    _ collector: inout Collector\n  )\n  where\n  InputCollection.Iterator.Element == PipelineInputElement,\n  Collector.Element == OutputElement {\n    var i = range.lowerBound\n    while i != range.upperBound {\n      let e = c[i]\n      collector.append(e)\n      c.formIndex(after: &i)\n    }\n  }\n}\n\nfinal internal class _CollectionTransformerStepOneToMaybeOne<\n  PipelineInputElement,\n  OutputElement,\n  InputStep : _CollectionTransformerStepProtocol\n> : _CollectionTransformerStep<PipelineInputElement, OutputElement>\nwhere InputStep.PipelineInputElement == PipelineInputElement {\n\n  typealias _Self = _CollectionTransformerStepOneToMaybeOne\n  typealias InputElement = InputStep.OutputElement\n\n  let _input: InputStep\n  let _transform: (InputElement) -> OutputElement?\n\n  init(_ input: InputStep, _ transform: @escaping (InputElement) -> OutputElement?) {\n    self._input = input\n    self._transform = transform\n    super.init()\n  }\n\n  override func map<U>(_ transform: @escaping (OutputElement) -> U)\n    -> _CollectionTransformerStep<PipelineInputElement, U> {\n\n    // Let the closure below capture only one variable, not the whole `self`.\n    let localTransform = _transform\n    return _CollectionTransformerStepOneToMaybeOne<PipelineInputElement, U, InputStep>(_input) {\n      (input: InputElement) -> U? in\n      if let e = localTransform(input) {\n        return transform(e)\n      }\n      return nil\n    }\n  }\n\n  override func filter(_ isIncluded: @escaping (OutputElement) -> Bool)\n    -> _CollectionTransformerStep<PipelineInputElement, OutputElement> {\n\n    // Let the closure below capture only one variable, not the whole `self`.\n    let localTransform = _transform\n    return _CollectionTransformerStepOneToMaybeOne<PipelineInputElement, OutputElement, InputStep>(_input) {\n      (input: InputElement) -> OutputElement? in\n      if let e = localTransform(input) {\n        return isIncluded(e) ? e : nil\n      }\n      return nil\n    }\n  }\n\n  override func reduce<U>(_ initial: U, _ combine: @escaping (U, OutputElement) -> U)\n    -> _CollectionTransformerFinalizer<PipelineInputElement, U> {\n\n    return _CollectionTransformerFinalizerReduce(self, initial, combine)\n  }\n\n  override func collectTo<\n    C : BuildableCollectionProtocol\n  >(_ c: C.Type) -> _CollectionTransformerFinalizer<PipelineInputElement, C>\n  where\n  C.Builder.Destination == C,\n  C.Builder.Element == C.Iterator.Element,\n  C.Iterator.Element == OutputElement {\n\n    return _CollectionTransformerFinalizerCollectTo(self, c)\n  }\n\n  override func transform<\n    InputCollection : Collection,\n    Collector : _ElementCollector\n  >(\n    _ c: InputCollection,\n    _ range: Range<InputCollection.Index>,\n    _ collector: inout Collector\n  )\n  where\n  InputCollection.Iterator.Element == PipelineInputElement,\n  Collector.Element == OutputElement {\n    var collectorWrapper =\n      _ElementCollectorOneToMaybeOne(collector, _transform)\n    _input.transform(c, range, &collectorWrapper)\n    collector = collectorWrapper._baseCollector\n  }\n}\n\nstruct _ElementCollectorOneToMaybeOne<\n  BaseCollector : _ElementCollector,\n  Element_\n> : _ElementCollector {\n  typealias Element = Element_\n\n  var _baseCollector: BaseCollector\n  var _transform: (Element) -> BaseCollector.Element?\n\n  init(\n    _ baseCollector: BaseCollector,\n    _ transform: @escaping (Element) -> BaseCollector.Element?\n  ) {\n    self._baseCollector = baseCollector\n    self._transform = transform\n  }\n\n  mutating func sizeHint(_ approximateSize: Int) {}\n\n  mutating func append(_ element: Element) {\n    if let e = _transform(element) {\n      _baseCollector.append(e)\n    }\n  }\n\n  mutating func append<\n    C : Collection\n  >(contentsOf elements: C)\n  where C.Iterator.Element == Element {\n    for e in elements {\n      append(e)\n    }\n  }\n}\n\nprotocol _ElementCollector {\n  associatedtype Element\n\n  mutating func sizeHint(_ approximateSize: Int)\n\n  mutating func append(_ element: Element)\n\n  mutating func append<\n    C : Collection\n  >(contentsOf elements: C)\n  where C.Iterator.Element == Element\n}\n\nclass _CollectionTransformerFinalizer<PipelineInputElement, Result> {\n  func transform<\n    InputCollection : Collection\n  >(_ c: InputCollection) -> Result\n  where InputCollection.Iterator.Element == PipelineInputElement {\n    fatalError(\"implement\")\n  }\n}\n\nfinal class _CollectionTransformerFinalizerReduce<\n  PipelineInputElement,\n  U,\n  InputElementTy,\n  InputStep : _CollectionTransformerStepProtocol\n> : _CollectionTransformerFinalizer<PipelineInputElement, U>\nwhere\nInputStep.OutputElement == InputElementTy,\nInputStep.PipelineInputElement == PipelineInputElement {\n\n  var _input: InputStep\n  var _initial: U\n  var _combine: (U, InputElementTy) -> U\n\n  init(_ input: InputStep, _ initial: U, _ combine: @escaping (U, InputElementTy) -> U) {\n    self._input = input\n    self._initial = initial\n    self._combine = combine\n  }\n\n  override func transform<\n    InputCollection : Collection\n  >(_ c: InputCollection) -> U\n  where InputCollection.Iterator.Element == PipelineInputElement {\n    var collector = _ElementCollectorReduce(_initial, _combine)\n    _input.transform(c, c.startIndex..<c.endIndex, &collector)\n    return collector.takeResult()\n  }\n}\n\nstruct _ElementCollectorReduce<Element_, Result> : _ElementCollector {\n  typealias Element = Element_\n\n  var _current: Result\n  var _combine: (Result, Element) -> Result\n\n  init(_ initial: Result, _ combine: @escaping (Result, Element) -> Result) {\n    self._current = initial\n    self._combine = combine\n  }\n\n  mutating func sizeHint(_ approximateSize: Int) {}\n\n  mutating func append(_ element: Element) {\n    _current = _combine(_current, element)\n  }\n\n  mutating func append<\n    C : Collection\n  >(contentsOf elements: C)\n  where C.Iterator.Element == Element {\n    for e in elements {\n      append(e)\n    }\n  }\n\n  mutating func takeResult() -> Result {\n    return _current\n  }\n}\n\nfinal class _CollectionTransformerFinalizerCollectTo<\n  PipelineInputElement,\n  U : BuildableCollectionProtocol,\n  InputElementTy,\n  InputStep : _CollectionTransformerStepProtocol\n> : _CollectionTransformerFinalizer<PipelineInputElement, U>\nwhere\nInputStep.OutputElement == InputElementTy,\nInputStep.PipelineInputElement == PipelineInputElement,\nU.Builder.Destination == U,\nU.Builder.Element == U.Iterator.Element,\nU.Iterator.Element == InputStep.OutputElement {\n\n  var _input: InputStep\n\n  init(_ input: InputStep, _: U.Type) {\n    self._input = input\n  }\n\n  override func transform<\n    InputCollection : Collection\n  >(_ c: InputCollection) -> U\n  where InputCollection.Iterator.Element == PipelineInputElement {\n    var collector = _ElementCollectorCollectTo<U>()\n    _input.transform(c, c.startIndex..<c.endIndex, &collector)\n    return collector.takeResult()\n  }\n}\n\nstruct _ElementCollectorCollectTo<\n  BuildableCollection : BuildableCollectionProtocol\n> : _ElementCollector\nwhere\nBuildableCollection.Builder.Destination == BuildableCollection,\nBuildableCollection.Builder.Element == BuildableCollection.Iterator.Element {\n\n  typealias Element = BuildableCollection.Iterator.Element\n\n  var _builder: BuildableCollection.Builder\n\n  init() {\n    self._builder = BuildableCollection.Builder()\n  }\n\n  mutating func sizeHint(_ approximateSize: Int) {\n    _builder.sizeHint(approximateSize)\n  }\n\n  mutating func append(_ element: Element) {\n    _builder.append(element)\n  }\n\n  mutating func append<\n    C : Collection\n  >(contentsOf elements: C)\n  where C.Iterator.Element == Element {\n    _builder.append(contentsOf: elements)\n  }\n\n  mutating func takeResult() -> BuildableCollection {\n    return _builder.takeResult()\n  }\n}\n\ninternal func _optimizeCollectionTransformer<PipelineInputElement, Result>(\n  _ transformer: _CollectionTransformerFinalizer<PipelineInputElement, Result>\n) -> _CollectionTransformerFinalizer<PipelineInputElement, Result> {\n  return transformer\n}\n\ninternal func _runCollectionTransformer<\n  InputCollection : Collection, Result\n>(\n  _ c: InputCollection,\n  _ transformer: _CollectionTransformerFinalizer<InputCollection.Iterator.Element, Result>\n) -> Result {\n  dump(transformer)\n  let optimized = _optimizeCollectionTransformer(transformer)\n  dump(optimized)\n  return transformer.transform(c)\n}\n\n//===----------------------------------------------------------------------===//\n// Collection transformation DSL: public interface\n//===----------------------------------------------------------------------===//\n\npublic struct CollectionTransformerPipeline<\n  InputCollection : Collection, T\n> {\n  internal var _input: InputCollection\n  internal var _step: _CollectionTransformerStep<InputCollection.Iterator.Element, T>\n\n  public func map<U>(_ transform: @escaping (T) -> U)\n    -> CollectionTransformerPipeline<InputCollection, U> {\n\n    return CollectionTransformerPipeline<InputCollection, U>(\n      _input: _input,\n      _step: _step.map(transform)\n    )\n  }\n\n  public func filter(_ isIncluded: @escaping (T) -> Bool)\n    -> CollectionTransformerPipeline<InputCollection, T> {\n\n    return CollectionTransformerPipeline<InputCollection, T>(\n      _input: _input,\n      _step: _step.filter(isIncluded)\n    )\n  }\n\n  public func reduce<U>(\n    _ initial: U, _ combine: @escaping (U, T) -> U\n  ) -> U {\n    return _runCollectionTransformer(_input, _step.reduce(initial, combine))\n  }\n\n  public func collectTo<\n    C : BuildableCollectionProtocol\n  >(_ c: C.Type) -> C\n  where\n  C.Builder.Destination == C,\n  C.Iterator.Element == T,\n  C.Builder.Element == T {\n    return _runCollectionTransformer(_input, _step.collectTo(c))\n  }\n\n  public func toArray() -> [T] {\n    return collectTo(Array<T>.self)\n  }\n}\n\npublic func transform<C : Collection>(_ c: C)\n  -> CollectionTransformerPipeline<C, C.Iterator.Element> {\n\n  return CollectionTransformerPipeline<C, C.Iterator.Element>(\n    _input: c,\n    _step: _CollectionTransformerStepCollectionSource<C.Iterator.Element>())\n}\n\n//===----------------------------------------------------------------------===//\n// Collection transformation DSL: tests\n//===----------------------------------------------------------------------===//\n\nimport StdlibUnittest\n\n\nvar t = TestSuite(\"t\")\n\nt.test(\"fusion/map+reduce\") {\n  let xs = [ 1, 2, 3 ]\n  let result =\n    transform(xs)\n    .map { $0 * 2 }\n    .reduce(0, { $0 + $1 })\n  expectEqual(12, result)\n}\n\nt.test(\"fusion/map+filter+reduce\") {\n  let xs = [ 1, 2, 3 ]\n  let result = transform(xs)\n    .map { $0 * 2 }\n    .filter { $0 != 0 }\n    .reduce(0, { $0 + $1 })\n  expectEqual(12, result)\n}\n\nt.test(\"fusion/map+collectTo\") {\n  let xs = [ 1, 2, 3 ]\n  let result =\n    transform(xs)\n    .map { $0 * 2 }\n    .collectTo(Array<Int>.self)\n  expectEqual([ 2, 4, 6 ], result)\n}\n\nt.test(\"fusion/map+toArray\") {\n  let xs = [ 1, 2, 3 ]\n  let result =\n    transform(xs)\n    .map { $0 * 2 }\n    .toArray()\n  expectEqual([ 2, 4, 6 ], result)\n}\n\nt.test(\"ForkJoinPool.forkTask\") {\n  var tasks: [ForkJoinTask<()>] = []\n  for i in 0..<100 {\n    tasks.append(ForkJoinPool.commonPool.forkTask {\n      () -> () in\n      var result = 1\n      for i in 0..<10000 {\n        result = result &* i\n        _blackHole(result)\n      }\n      return ()\n    })\n  }\n  for t in tasks {\n    t.wait()\n  }\n}\n\nfunc fib(_ n: Int) -> Int {\n  if n == 1 || n == 2 {\n    return 1\n  }\n  if n == 38 {\n    print(\"\\(pthread_self()) fib(\\(n))\")\n  }\n  if n < 39 {\n    let r = fib(n - 1) + fib(n - 2)\n    _blackHole(r)\n    return r\n  }\n  print(\"fib(\\(n))\")\n  let t1 = ForkJoinTask() { fib(n - 1) }\n  let t2 = ForkJoinTask() { fib(n - 2) }\n  ForkJoinPool.invokeAll(t1, t2)\n  return t2.waitAndGetResult() + t1.waitAndGetResult()\n}\n\nt.test(\"ForkJoinPool.forkTask/Fibonacci\") {\n  let t = ForkJoinPool.commonPool.forkTask { fib(40) }\n  expectEqual(102334155, t.waitAndGetResult())\n}\n\nfunc _parallelMap(_ input: [Int], transform: @escaping (Int) -> Int, range: Range<Int>)\n  -> Array<Int>.Builder {\n\n  var builder = Array<Int>.Builder()\n  if range.count < 1_000 {\n    builder.append(contentsOf: input[range].map(transform))\n  } else {\n    let tasks = input.split(range).map {\n      (subRange) in\n      ForkJoinTask<Array<Int>.Builder> {\n        _parallelMap(input, transform: transform, range: subRange)\n      }\n    }\n    ForkJoinPool.invokeAll(tasks)\n    for t in tasks {\n      var otherBuilder = t.waitAndGetResult()\n      builder.moveContentsOf(&otherBuilder)\n    }\n  }\n  return builder\n}\n\nfunc parallelMap(_ input: [Int], transform: @escaping (Int) -> Int) -> [Int] {\n  let t = ForkJoinPool.commonPool.forkTask {\n    _parallelMap(\n      input,\n      transform: transform,\n      range: input.startIndex..<input.endIndex)\n  }\n  var builder = t.waitAndGetResult()\n  return builder.takeResult()\n}\n\nt.test(\"ForkJoinPool.forkTask/MapArray\") {\n  expectEqual(\n    Array(2..<1_001),\n    parallelMap(Array(1..<1_000)) { $0 + 1 }\n  )\n}\n\n/*\n * FIXME: reduce compiler crasher\nt.test(\"ForkJoinPool.forkTask\") {\n  func fib(_ n: Int) -> Int {\n    if n == 0 || n == 1 {\n      return 1\n    }\n    let t1 = ForkJoinPool.commonPool.forkTask { fib(n - 1) }\n    let t2 = ForkJoinPool.commonPool.forkTask { fib(n - 2) }\n    return t2.waitAndGetResult() + t1.waitAndGetResult()\n  }\n  expectEqual(0, fib(10))\n}\n*/\n\n\n/*\n\nUseful links:\n\nhttp://habrahabr.ru/post/255659/\n\n*/\n\nrunAllTests()\n", "subject": "warning suppression", "message": "warning suppression\n" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "apple/swift", "file": "SequenceType.swift.gyb", "language": "gyb", "commit_date": "2016-06-22 23:18:56.000 UTC", "content": "// -*- swift -*-\n// RUN: %target-run-simple-swiftgyb\n// REQUIRES: executable_test\n\n% import os.path\n% import gyb\n\nimport StdlibUnittest\nimport StdlibCollectionUnittest\n\n\n// Extend LoggingSequence to shadow the new operation.  When\n// requirements are added to a protocol 'P', there should be an\n// implementation in 'InstrumentedP' that does some bookkeeping for\n// testing (like counting calls to the operation) and then dispatches\n// to the same operation on its 'base' member.  InstrumentedSequence\n// already contains implementations of all the Sequence\n// requirements.\n\nvar _nonCustomizableOperation = TypeIndexed(0)\nextension SequenceLog {\n  static var nonCustomizableOperation : TypeIndexed<Int> {\n    get {return _nonCustomizableOperation}\n    set {_nonCustomizableOperation = newValue}\n  }\n}\n\nextension LoggingSequence {\n  func nonCustomizableOperation() {\n    Log.nonCustomizableOperation[type(of: self)] += 1\n    return base.nonCustomizableOperation()\n  }\n}\n\n// Add a non-customizable operation to sequence\nextension Sequence {\n  func nonCustomizableOperation() {}\n}\n\nvar SequenceTypeTests = TestSuite(\"Sequence\")\n\n//===----------------------------------------------------------------------===//\n\n// FIXME: add tests for:\n//\n// - Array, ContiguousArray and ArraySlice as inputs.  These types special-case\n// a lot of collection behavior for performance reasons.  Not to even mention\n// that these are important types to test in any case.\n//\n// - NaN behavior of floating point types, combined with these generic\n// algorithms, should make sense if possible.  For example,\n// [1.0, Double.nan].starts(with: [1.0, 2.0]) should be false.\n\n//===----------------------------------------------------------------------===//\n// min(), max()\n//===----------------------------------------------------------------------===//\n\n% for algorithmKind in ['min', 'max']:\n%   AlgorithmKind = algorithmKind.capitalize()\n\nSequenceTypeTests.test(\"${algorithmKind}/WhereElementIsComparable\") {\n  for test in minMaxTests {\n    let s = MinimalSequence<MinimalComparableValue>(\n      elements: test.sequence.enumerated().map {\n        MinimalComparableValue($1, identity: $0)\n      })\n    var maybeResult = s.${algorithmKind}()\n    expectType(Optional<MinimalComparableValue>.self, &maybeResult)\n    if let result = maybeResult {\n      expectEqual(\n        test.expected${AlgorithmKind}Value!, result.value,\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expected${AlgorithmKind}Index!, result.identity,\n        stackTrace: SourceLocStack().with(test.loc))\n    } else {\n      expectNil(\n        test.expected${AlgorithmKind}Value,\n        stackTrace: SourceLocStack().with(test.loc))\n      expectNil(\n        test.expected${AlgorithmKind}Index,\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n    expectEqual([], s.map { $0.value }, \"sequence should be consumed\")\n  }\n}\n\nSequenceTypeTests.test(\"${algorithmKind}/Predicate\") {\n  for test in minMaxTests {\n    let s = MinimalSequence<OpaqueValue<Int>>(\n      elements: test.sequence.enumerated().map {\n        OpaqueValue($1, identity: $0)\n      })\n    var timesClosureWasCalled = 0\n    var maybeResult = s.${algorithmKind} {\n      (lhs, rhs) -> Bool in\n      timesClosureWasCalled += 1\n      return lhs.value < rhs.value\n    }\n    expectType(Optional<OpaqueValue<Int>>.self, &maybeResult)\n    if let result = maybeResult {\n      expectEqual(\n        test.expected${AlgorithmKind}Value!, result.value,\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expected${AlgorithmKind}Index!, result.identity,\n        stackTrace: SourceLocStack().with(test.loc))\n    } else {\n      expectNil(\n        test.expected${AlgorithmKind}Value,\n        stackTrace: SourceLocStack().with(test.loc))\n      expectNil(\n        test.expected${AlgorithmKind}Index,\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n    expectEqual([], s.map { $0.value }, \"sequence should be consumed\")\n    expectEqual(\n      max(0, test.sequence.count - 1), timesClosureWasCalled,\n      \"max() should be eager and should only call its predicate\"\n      + \" once per element\")\n  }\n}\n\n% end\n\n\n//===----------------------------------------------------------------------===//\n// IteratorSequence\n//===----------------------------------------------------------------------===//\n\n// Check that the generic parameter is called 'Base'.\nprotocol TestProtocol1 {}\n\nextension IteratorSequence where Base : TestProtocol1 {\n  var _baseIsTestProtocol1: Bool {\n    fatalError(\"not implemented\")\n  }\n}\n\n//===--- Demonstrate technique for testing generic dispatching ------------===//\n// A counter we can use to record calls to our non-customizable operation\n\nSequenceTypeTests.test(\"Demonstration/NotCustomizable\") {\n  let tester = SequenceLog.dispatchTester([OpaqueValue(1)])\n\n  tester.nonCustomizableOperation()\n\n  expectNotCustomizable(tester, tester.log.nonCustomizableOperation)\n}\n\nSequenceTypeTests.test(\"IteratorSequence/IteratorProtocol/empty\") {\n  do {\n    let data: [OpaqueValue<Int>] = []\n    let base = MinimalIterator(data)\n    var iter = IteratorSequence(base)\n    expectType(\n      IteratorSequence<MinimalIterator<OpaqueValue<Int>>>.self,\n      &iter)\n    checkIterator(data, iter, resiliencyChecks: .none) { $0.value == $1.value }\n  }\n  do {\n    let data: [OpaqueValue<Int>] = []\n    let base = data.makeIterator()\n    var iter = IteratorSequence(base)\n    expectType(\n      IteratorSequence<IndexingIterator<Array<OpaqueValue<Int>>>>.self,\n      &iter)\n    checkIterator(data, iter) { $0.value == $1.value }\n  }\n}\n\nSequenceTypeTests.test(\"IteratorSequence/IteratorProtocol\") {\n  do {\n    let data: [OpaqueValue<Int>] = []\n    let base = MinimalIterator(data)\n    var iter = IteratorSequence(base)\n    expectType(\n      IteratorSequence<MinimalIterator<OpaqueValue<Int>>>.self,\n      &iter)\n    checkIterator(data, iter, resiliencyChecks: .none) { $0.value == $1.value }\n  }\n  do {\n    let data: [OpaqueValue<Int>] = []\n    let base = data.makeIterator()\n    var iter = IteratorSequence(base)\n    expectType(\n      IteratorSequence<IndexingIterator<Array<OpaqueValue<Int>>>>.self,\n      &iter)\n    checkIterator(data, iter) { $0.value == $1.value }\n  }\n}\n\nSequenceTypeTests.test(\"IteratorSequence/Sequence/empty\") {\n  do {\n    let data = [ 10, 20, 30 ].map(OpaqueValue.init)\n    let base = MinimalIterator(data)\n    var iter = IteratorSequence(base)\n    expectType(\n      IteratorSequence<MinimalIterator<OpaqueValue<Int>>>.self,\n      &iter)\n    checkSequence(data, iter, resiliencyChecks: .none) { $0.value == $1.value }\n  }\n  do {\n    let data = [ 10, 20, 30 ].map(OpaqueValue.init)\n    let base = data.makeIterator()\n    var iter = IteratorSequence(base)\n    expectType(\n      IteratorSequence<IndexingIterator<Array<OpaqueValue<Int>>>>.self,\n      &iter)\n    checkSequence(data, iter) { $0.value == $1.value }\n  }\n}\n\nSequenceTypeTests.test(\"IteratorSequence/Sequence\") {\n  do {\n    let data = [ 10, 20, 30 ].map(OpaqueValue.init)\n    let base = MinimalIterator(data)\n    var iter = IteratorSequence(base)\n    expectType(\n      IteratorSequence<MinimalIterator<OpaqueValue<Int>>>.self,\n      &iter)\n    checkSequence(data, iter, resiliencyChecks: .none) { $0.value == $1.value }\n  }\n  do {\n    let data = [ 10, 20, 30 ].map(OpaqueValue.init)\n    let base = data.makeIterator()\n    var iter = IteratorSequence(base)\n    expectType(\n      IteratorSequence<IndexingIterator<Array<OpaqueValue<Int>>>>.self,\n      &iter)\n    checkSequence(data, iter) { $0.value == $1.value }\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// enumerated()\n//===----------------------------------------------------------------------===//\n\n// Check that the generic parameter is called 'Base'.\nextension EnumeratedIterator where Base : TestProtocol1 {\n  var _elementIsTestProtocol1: Bool {\n    fatalError(\"not implemented\")\n  }\n}\n\nextension EnumeratedSequence where Base : TestProtocol1 {\n  var _elementIsTestProtocol1: Bool {\n    fatalError(\"not implemented\")\n  }\n}\n\nSequenceTypeTests.test(\"enumerated()\") {\n  typealias Element = (offset: Int, element: OpaqueValue<Int>)\n  func compareElements(_ lhs: Element, rhs: Element) -> Bool {\n    return lhs.0 == rhs.0 && lhs.1.value == rhs.1.value\n  }\n\n  for test in enumerateTests {\n    let s = MinimalSequence<OpaqueValue<Int>>(\n      elements: test.sequence.map(OpaqueValue.init))\n    var result = s.enumerated()\n    expectType(\n      EnumeratedSequence<MinimalSequence<OpaqueValue<Int>>>.self,\n      &result)\n\n    checkSequence(\n      test.expected.map {\n        (offset: $0.0, element: OpaqueValue($0.1))\n      } as [Element],\n      result,\n      resiliencyChecks: .none, sameValue: compareElements)\n    expectEqual([], s.map { $0.value }, \"sequence should be consumed\")\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// starts(with:)\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"starts(with:)/WhereElementIsEquatable\") {\n  for test in startsWithTests {\n    do {\n      let s = MinimalSequence<MinimalEquatableValue>(\n        elements: test.sequence.map(MinimalEquatableValue.init))\n      let prefix = MinimalSequence<MinimalEquatableValue>(\n        elements: test.prefix.map(MinimalEquatableValue.init))\n      expectEqual(\n        test.expected,\n        s.starts(with: prefix),\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverSequence, s.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverPrefix, prefix.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n\n    // Use different types for the sequence and prefix.\n    do {\n      let s = MinimalCollection<MinimalEquatableValue>(\n        elements: test.sequence.map(MinimalEquatableValue.init))\n      let prefix = MinimalSequence<MinimalEquatableValue>(\n        elements: test.prefix.map(MinimalEquatableValue.init))\n      expectEqual(\n        test.expected,\n        s.starts(with: prefix),\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.sequence, s.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverPrefix, prefix.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n  }\n}\n\nSequenceTypeTests.test(\"starts(with:)/Predicate\") {\n  for test in startsWithTests {\n    do {\n      let s = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.sequence.map(OpaqueValue.init))\n      let prefix = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.prefix.map(OpaqueValue.init))\n      expectEqual(\n        test.expected,\n        s.starts(with: prefix) { $0.value == $1.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverSequence, s.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverPrefix, prefix.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n    do {\n      let s = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.sequence.map { OpaqueValue($0 * 2) })\n      let prefix = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.prefix.map(OpaqueValue.init))\n      expectEqual(\n        test.expected,\n        s.starts(with: prefix) { $0.value / 2 == $1.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverSequence, s.map { $0.value / 2 },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverPrefix, prefix.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n\n    // Use different types for the sequence and prefix.\n    do {\n      let s = MinimalCollection<OpaqueValue<Int>>(\n        elements: test.sequence.map(OpaqueValue.init))\n      let prefix = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.prefix.map(OpaqueValue.init))\n      expectEqual(\n        test.expected,\n        s.starts(with: prefix) { $0.value == $1.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.sequence, s.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverPrefix, prefix.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// equal()\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"elementsEqual/WhereElementIsEquatable\") {\n  for test in elementsEqualTests {\n    do {\n      let s = MinimalSequence<MinimalEquatableValue>(\n        elements: test.sequence.map(MinimalEquatableValue.init))\n      let other = MinimalSequence<MinimalEquatableValue>(\n        elements: test.other.map(MinimalEquatableValue.init))\n      expectEqual(\n        test.expected,\n        s.elementsEqual(other),\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverSequence, s.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverOther, other.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n\n    // Use different types for the sequence and other.\n    do {\n      let s = MinimalCollection<MinimalEquatableValue>(\n        elements: test.sequence.map(MinimalEquatableValue.init))\n      let other = MinimalSequence<MinimalEquatableValue>(\n        elements: test.other.map(MinimalEquatableValue.init))\n      expectEqual(\n        test.expected,\n        s.elementsEqual(other),\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.sequence, s.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverOther, other.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n  }\n}\n\nSequenceTypeTests.test(\"elementsEqual/Predicate\") {\n  for test in elementsEqualTests {\n    do {\n      let s = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.sequence.map(OpaqueValue.init))\n      let other = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.other.map(OpaqueValue.init))\n      expectEqual(\n        test.expected,\n        s.elementsEqual(other) { $0.value == $1.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverSequence, s.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverOther, other.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n\n    // Use different types for the sequence and other.\n    do {\n      let s = MinimalCollection<OpaqueValue<Int>>(\n        elements: test.sequence.map(OpaqueValue.init))\n      let other = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.other.map(OpaqueValue.init))\n      expectEqual(\n        test.expected,\n        s.elementsEqual(other) { $0.value == $1.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.sequence, s.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverOther, other.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// lexicographicallyPrecedes()\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"lexicographicallyPrecedes(_:)/WhereElementIsComparable\") {\n  for test in lexicographicallyPrecedesTests {\n    do {\n      let s = MinimalSequence<MinimalComparableValue>(\n        elements: test.sequence.map(MinimalComparableValue.init))\n      let other = MinimalSequence<MinimalComparableValue>(\n        elements: test.other.map(MinimalComparableValue.init))\n      expectEqual(\n        test.expected.isLT(),\n        s.lexicographicallyPrecedes(other),\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverSequence, s.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverOther, other.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n\n    // Use different types for the sequence and other.\n    do {\n      let s = MinimalCollection<MinimalComparableValue>(\n        elements: test.sequence.map(MinimalComparableValue.init))\n      let other = MinimalSequence<MinimalComparableValue>(\n        elements: test.other.map(MinimalComparableValue.init))\n      expectEqual(\n        test.expected.isLT(),\n        s.lexicographicallyPrecedes(other),\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.sequence, s.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverOther, other.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n  }\n}\n\nSequenceTypeTests.test(\"lexicographicallyPrecedes(_:)/Predicate\") {\n  for test in lexicographicallyPrecedesTests {\n    do {\n      let s = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.sequence.map(OpaqueValue.init))\n      let other = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.other.map(OpaqueValue.init))\n      expectEqual(\n        test.expected.isLT(),\n        s.lexicographicallyPrecedes(other) { $0.value < $1.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverSequence, s.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverOther, other.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n\n    // Use different types for the sequence and other.\n    do {\n      let s = MinimalCollection<OpaqueValue<Int>>(\n        elements: test.sequence.map(OpaqueValue.init))\n      let other = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.other.map(OpaqueValue.init))\n      expectEqual(\n        test.expected.isLT(),\n        s.lexicographicallyPrecedes(other) { $0.value < $1.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.sequence, s.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        test.expectedLeftoverOther, other.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// contains()\n//===----------------------------------------------------------------------===//\n\ntypealias StrictSequenceOfEquatable = MinimalSequence<MinimalEquatableValue>\n\n// FIXME: rename in StdlibUnittest\n// typealias StrictSequence = MinimalSequence\n\nSequenceTypeTests.test(\"contains/WhereElementIsEquatable/dispatch\") {\n  let tester = SequenceLog.dispatchTester([MinimalEquatableValue(1)])\n  _ = tester.contains(MinimalEquatableValue(1))\n  expectCustomizable(tester, tester.log._customContainsEquatableElement)\n}\n\nfunc callStaticContains(\n  _ set: Set<MinimalHashableValue>,\n  _ element: MinimalHashableValue\n) -> Bool {\n  return set.contains(element)\n}\n\nfunc callGenericContains<\n  S : Sequence\n>(_ sequence: S, _ element: S.Iterator.Element) -> Bool\nwhere S.Iterator.Element : Equatable {\n  return sequence.contains(element)\n}\n\n% for dispatch in ['Static', 'Generic']:\n\n// FIXME: implement the same optimization for Dictionary.\n// FIXME: move to the file where other Set tests live.\nSequenceTypeTests.test(\"Set<T>.contains/CustomImplementation/${dispatch}\") {\n  for test in findTests {\n    let s = Set<MinimalHashableValue>(\n      test.sequence.map { MinimalHashableValue($0.value) })\n    MinimalHashableValue.timesEqualEqualWasCalled = 0\n    MinimalHashableValue.timesHashValueWasCalled = 0\n    expectEqual(\n      test.expected != nil,\n      call${dispatch}Contains(s, MinimalHashableValue(test.element.value)),\n      stackTrace: SourceLocStack().with(test.loc))\n    if test.sequence.isEmpty {\n      expectEqual(\n        0, MinimalHashableValue.timesEqualEqualWasCalled,\n        stackTrace: SourceLocStack().with(test.loc))\n      expectEqual(\n        0, MinimalHashableValue.timesHashValueWasCalled,\n        stackTrace: SourceLocStack().with(test.loc))\n    } else {\n      expectNotEqual(\n        0, MinimalHashableValue.timesHashValueWasCalled,\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n    if test.expected != nil {\n      expectNotEqual(\n        0, MinimalHashableValue.timesEqualEqualWasCalled,\n        stackTrace: SourceLocStack().with(test.loc))\n    }\n  }\n}\n\n% end\n\nSequenceTypeTests.test(\"contains/Predicate\") {\n  for test in findTests {\n    let s = MinimalSequence<OpaqueValue<Int>>(\n      elements: test.sequence.map { OpaqueValue($0.value) })\n    expectEqual(\n      test.expected != nil,\n      s.contains { $0.value == test.element.value },\n      stackTrace: SourceLocStack().with(test.loc))\n    expectEqual(\n      test.expectedLeftoverSequence.map { $0.value }, s.map { $0.value },\n      stackTrace: SourceLocStack().with(test.loc))\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// reduce()\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"reduce\") {\n  for test in reduceTests {\n    let s = MinimalSequence<OpaqueValue<Int>>(\n      elements: test.sequence.map(OpaqueValue.init))\n    var timesClosureWasCalled = 0\n    let result = s.reduce(OpaqueValue<[Int]>([])) {\n      (partialResult: OpaqueValue<[Int]>, element: OpaqueValue<Int>)\n        -> OpaqueValue<[Int]> in\n      timesClosureWasCalled += 1\n      return OpaqueValue<[Int]>(partialResult.value + [element.value])\n    }\n    expectEqual(test.sequence, result.value)\n    expectEqual([], s.map { $0.value }, \"sequence should be consumed\")\n    expectEqual(\n      test.sequence.count, timesClosureWasCalled,\n      \"reduce() should be eager and should only call its predicate\"\n      + \"once per element\")\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// reversed()\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"reverse/Sequence\") {\n  for test in reverseTests {\n    let s = MinimalSequence<OpaqueValue<Int>>(\n      elements: test.sequence.map(OpaqueValue.init))\n    var result = s.reversed()\n    expectType([OpaqueValue<Int>].self, &result)\n    expectEqual(\n      test.expected, result.map { $0.value },\n      stackTrace: SourceLocStack().with(test.loc))\n    expectEqual([], s.map { $0.value }, \"sequence should be consumed\")\n  }\n}\n\nSequenceTypeTests.test(\"reverse/WhereIndexIsBidirectional,BidirectionalReverseView\") {\n  for test in reverseTests {\n    let s = MinimalBidirectionalCollection<OpaqueValue<Int>>(\n      elements: test.sequence.map(OpaqueValue.init))\n    var result = s.reversed()\n    expectType(\n      ReversedCollection<MinimalBidirectionalCollection<OpaqueValue<Int>>>.self,\n      &result)\n    expectEqual(\n      test.expected, result.map { $0.value },\n      stackTrace: SourceLocStack().with(test.loc))\n\n    // Check ReversedCollection's Collection conformance.\n    checkBidirectionalCollection(\n      test.expected.map(OpaqueValue.init) as [OpaqueValue<Int>],\n      result) { $0.value == $1.value }\n  }\n}\n\nSequenceTypeTests.test(\"reverse/WhereIndexIsRandomAccess,RandomAccessReverseView\") {\n  for test in reverseTests {\n    let s = MinimalRandomAccessCollection<OpaqueValue<Int>>(\n      elements: test.sequence.map(OpaqueValue.init))\n    var result = s.reversed()\n    expectType(\n      ReversedRandomAccessCollection<MinimalRandomAccessCollection<OpaqueValue<Int>>>.self,\n      &result)\n    expectEqual(\n      test.expected, result.map { $0.value },\n      stackTrace: SourceLocStack().with(test.loc))\n\n    // Check ReversedRandomAccessCollection Collection conformance.\n    let expected = test.expected.map(OpaqueValue.init) as [OpaqueValue<Int>]\n    checkRandomAccessCollection(expected, result) { $0.value == $1.value }\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// filter()\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"filter/Sequence/Dispatch\") {\n  let tester = SequenceLog.dispatchTester([OpaqueValue(1)])\n  _ = tester.filter { _ in false }\n  expectCustomizable(tester, tester.log.filter)\n}\n\nSequenceTypeTests.test(\"filter/Sequence/Semantics\") {\n  for test in filterTests {\n    for underestimatedCountBehavior in [\n      UnderestimatedCountBehavior.precise,\n      UnderestimatedCountBehavior.half,\n      UnderestimatedCountBehavior.value(0)\n    ] {\n\n      let s = DefaultedSequence<OpaqueValue<Int>>(\n        elements: test.sequence.map(OpaqueValue.init),\n        underestimatedCount: underestimatedCountBehavior)\n\n      let closureLifetimeTracker = LifetimeTracked(0)\n      expectEqual(1, LifetimeTracked.instances)\n      var timesClosureWasCalled = 0\n      var result = s.filter {\n        (element) in\n        _blackHole(closureLifetimeTracker)\n        timesClosureWasCalled += 1\n        return test.includeElement(element.value)\n      }\n      expectType([OpaqueValue<Int>].self, &result)\n      expectEqual(test.expected, result.map { $0.value })\n      expectEqual([], s.map { $0.value }, \"sequence should be consumed\")\n      expectEqual(\n        test.sequence.count, timesClosureWasCalled,\n        \"filter() should be eager and should only call its predicate\"\n        + \"once per element\")\n      expectGE(\n        2 * result.count, result.capacity,\n        \"filter() should not reserve capacity (it does not know how much the\"\n        + \"predicate will filter out)\")\n    }\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// map()\n//===----------------------------------------------------------------------===//\n\nvar MinimalSequenceWithCustomMap_timesMapWasCalled: Int = 0\n\n% for Implementation in ['Default', 'Custom']:\n\nstruct MinimalSequenceWith${Implementation}Map<Element> : Sequence {\n  init(_ data: [Element], underestimatedCount: UnderestimatedCountBehavior) {\n    self._data = MinimalSequence(\n      elements: data, underestimatedCount: underestimatedCount)\n  }\n\n  func makeIterator() -> MinimalIterator<Element> {\n    return _data.makeIterator()\n  }\n\n  var _data: MinimalSequence<Element>\n\n\n%   if Implementation == 'Custom':\n\n  static var timesMapWasCalled: Int {\n    get {\n      return MinimalSequenceWithCustomMap_timesMapWasCalled\n    }\n    set {\n      MinimalSequenceWithCustomMap_timesMapWasCalled = newValue\n    }\n  }\n\n  func map<T>(\n    _ transform: (Element) throws -> T\n  ) rethrows -> [T] {\n    MinimalSequenceWithCustomMap.timesMapWasCalled += 1\n    return try _data.map(transform)\n  }\n\n%   end\n}\n\n% end\n\nfunc callStaticSequenceMap<T>(\n  _ sequence: MinimalSequenceWithDefaultMap<OpaqueValue<Int>>,\n  transform: (OpaqueValue<Int>) -> T\n) -> [T] {\n  var result = sequence.map(transform)\n  expectType([T].self, &result)\n  return result\n}\n\nfunc callStaticSequenceMap<T>(\n  _ sequence: MinimalSequenceWithCustomMap<OpaqueValue<Int>>,\n  transform: (OpaqueValue<Int>) -> T\n) -> [T] {\n  var result = sequence.map(transform)\n  expectType([T].self, &result)\n  return result\n}\n\nfunc callGenericSequenceMap<S : Sequence, T>(\n  _ sequence: S,\n  transform: (S.Iterator.Element) -> T\n) -> [T] {\n  var result = sequence.map(transform)\n  expectType([T].self, &result)\n  return result\n}\n\n% for Implementation in ['Default', 'Custom']:\n\n%   for dispatch in ['Static', 'Generic']:\n\nSequenceTypeTests.test(\n  \"map/Sequence/${Implementation}Implementation/${dispatch}\"\n) {\n  for test in mapTests {\n    for underestimatedCountBehavior in [\n      UnderestimatedCountBehavior.precise,\n      UnderestimatedCountBehavior.half,\n      UnderestimatedCountBehavior.value(0)\n    ] {\n      let s = MinimalSequenceWith${Implementation}Map<OpaqueValue<Int>>(\n        test.sequence.map(OpaqueValue.init),\n        underestimatedCount: underestimatedCountBehavior)\n      let closureLifetimeTracker = LifetimeTracked(0)\n      expectEqual(1, LifetimeTracked.instances)\n      var timesClosureWasCalled = 0\n%     if Implementation == 'Custom':\n      MinimalSequenceWithCustomMap<OpaqueValue<Int>>.timesMapWasCalled = 0\n%     end\n      var result = call${dispatch}SequenceMap(s) {\n        (element: OpaqueValue<Int>) -> OpaqueValue<Int32> in\n        _blackHole(closureLifetimeTracker)\n        timesClosureWasCalled += 1\n        return OpaqueValue(Int32(test.transform(element.value)))\n      }\n      expectType([OpaqueValue<Int32>].self, &result)\n      expectEqual(test.expected, result.map { $0.value })\n%     if Implementation == 'Custom':\n      expectEqual(\n        1, MinimalSequenceWithCustomMap<OpaqueValue<Int>>.timesMapWasCalled)\n%     end\n      expectEqual([], s.map { $0.value }, \"sequence should be consumed\")\n      expectEqual(\n        test.sequence.count, timesClosureWasCalled,\n        \"map() should be eager and should only call its predicate\"\n        + \"once per element\")\n    }\n  }\n}\n\n%   end\n\n% end\n\n//===----------------------------------------------------------------------===//\n// flatMap()\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"flatMap/Sequence\") {\n  for test in flatMapTests {\n    for underestimatedCountBehavior in [\n      UnderestimatedCountBehavior.precise,\n      UnderestimatedCountBehavior.value(0)\n    ] {\n      let s = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.sequence.map(OpaqueValue.init),\n        underestimatedCount: underestimatedCountBehavior)\n      let closureLifetimeTracker = LifetimeTracked(0)\n      var timesClosureWasCalled = 0\n      var result = s.flatMap {\n        (element: OpaqueValue<Int>) -> MinimalSequence<OpaqueValue<Int32>> in\n        _blackHole(closureLifetimeTracker)\n        timesClosureWasCalled += 1\n        return MinimalSequence<OpaqueValue<Int32>>(\n          elements: test.transform(element.value).map { OpaqueValue(Int32($0)) })\n      }\n      expectType([OpaqueValue<Int32>].self, &result)\n      expectEqual(\n        test.expected, result.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectTrue(Array(s).isEmpty, \"sequence should be consumed\")\n      expectEqual(\n        test.sequence.count, timesClosureWasCalled,\n        \"flatMap() should be eager and should only call its predicate\"\n        + \"once per element\")\n      expectGE(\n        2 * result.count, result.capacity,\n        \"flatMap() should not reserve capacity\")\n    }\n  }\n}\n\n% TLazyFlatMapTest = gyb.parse_template(os.path.join(os.path.dirname(__file__), \"Inputs/flatMap.gyb\"))\n% LazyFlatMapTest = gyb.execute_template(TLazyFlatMapTest, Test='SequenceTypeTests', Kinds=['Sequence'])\n${LazyFlatMapTest}\n\nSequenceTypeTests.test(\"flatMap/Sequence/TransformProducesOptional\") {\n  for test in flatMapToOptionalTests {\n    for underestimatedCountBehavior in [\n      UnderestimatedCountBehavior.precise,\n      UnderestimatedCountBehavior.value(0)\n    ] {\n      let s = MinimalSequence<OpaqueValue<Int>>(\n        elements: test.sequence.map(OpaqueValue.init),\n        underestimatedCount: underestimatedCountBehavior)\n      let closureLifetimeTracker = LifetimeTracked(0)\n      expectEqual(1, LifetimeTracked.instances)\n      var timesClosureWasCalled = 0\n      var result = s.flatMap {\n        (element: OpaqueValue<Int>) -> OpaqueValue<Int32>? in\n        _blackHole(closureLifetimeTracker)\n        timesClosureWasCalled += 1\n        return test.transform(element.value).map { OpaqueValue(Int32($0)) }\n      }\n      expectType([OpaqueValue<Int32>].self, &result)\n      expectEqual(\n        test.expected, result.map { $0.value },\n        stackTrace: SourceLocStack().with(test.loc))\n      expectTrue(Array(s).isEmpty, \"sequence should be consumed\")\n      expectEqual(\n        test.sequence.count, timesClosureWasCalled,\n        \"flatMap() should be eager and should only call its predicate\"\n        + \"once per element\")\n      expectGE(\n        2 * result.count, result.capacity,\n        \"flatMap() should not reserve capacity\")\n    }\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// forEach()\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"forEach/dispatch\") {\n  let tester = SequenceLog.dispatchTester([OpaqueValue(1)])\n  tester.forEach { print($0) }\n  expectCustomizable(tester, tester.log.forEach)\n}\n\n//===----------------------------------------------------------------------===//\n// dropFirst()\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"dropFirst/dispatch\") {\n  var tester = SequenceLog.dispatchTester([OpaqueValue(1)])\n  _ = tester.dropFirst(1)\n  expectCustomizable(tester, tester.log.dropFirst)\n}\n\n//===----------------------------------------------------------------------===//\n// dropLast()\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"dropLast/dispatch\") {\n  var tester = SequenceLog.dispatchTester([OpaqueValue(1)])\n  _ = tester.dropLast(1)\n  expectCustomizable(tester, tester.log.dropLast)\n}\n\n//===----------------------------------------------------------------------===//\n// drop(while:)\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"drop(while:)/dispatch\") {\n  var tester = SequenceLog.dispatchTester([OpaqueValue(1)])\n  tester.drop { _ in return false }\n  expectCustomizable(tester, tester.log.dropWhile)\n}\n\n//===----------------------------------------------------------------------===//\n// prefix()\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"prefix/dispatch\") {\n  var tester = SequenceLog.dispatchTester([OpaqueValue(1)])\n  _ = tester.prefix(1)\n  expectCustomizable(tester, tester.log.prefixMaxLength)\n}\n\nSequenceTypeTests.test(\"prefix/drop/dispatch\") {\n  let xs = sequence(first: 1, next: {$0 < 10 ? $0 + 1 : nil})\n  expectEqualSequence([], Array(xs.prefix(3).drop(while: {$0 < 7})))\n}\n\n//===----------------------------------------------------------------------===//\n// prefix(while:)\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"prefix(while:)/dispatch\") {\n  var tester = SequenceLog.dispatchTester([OpaqueValue(1)])\n  tester.prefix { _ in return false }\n  expectCustomizable(tester, tester.log.prefixWhile)\n}\n\n//===----------------------------------------------------------------------===//\n// suffix()\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"suffix/dispatch\") {\n  var tester = SequenceLog.dispatchTester([OpaqueValue(1)])\n  _ = tester.suffix(1)\n  expectCustomizable(tester, tester.log.suffixMaxLength)\n}\n\n//===----------------------------------------------------------------------===//\n// split()\n//===----------------------------------------------------------------------===//\n\nSequenceTypeTests.test(\"Sequence/split/dispatch\") {\n  var tester = SequenceLog.dispatchTester([OpaqueValue(1)])\n  _ = tester.split { $0.value == 1 }\n  expectCustomizable(tester, tester.log.split)\n}\n\n//===----------------------------------------------------------------------===//\n// zip()\n//===----------------------------------------------------------------------===//\n\n// Check generic parameter names.\nextension Zip2Sequence.Iterator\n  where Sequence1 : TestProtocol1, Sequence2 : TestProtocol1 {\n\n  var _generator1IsTestProtocol1: Bool {\n    fatalError(\"not implemented\")\n  }\n}\n\n// Check generic parameter names.\nextension Zip2Sequence\n  where Sequence1 : TestProtocol1, Sequence2 : TestProtocol1 {\n\n  var _sequence1IsTestProtocol1: Bool {\n    fatalError(\"not implemented\")\n  }\n}\n\nSequenceTypeTests.test(\"zip\") {\n  typealias Element = (OpaqueValue<Int>, OpaqueValue<Int32>)\n  func compareElements(_ lhs: Element, rhs: Element) -> Bool {\n    return lhs.0.value == rhs.0.value && lhs.1.value == rhs.1.value\n  }\n\n  for test in zipTests {\n    let s = MinimalSequence<OpaqueValue<Int>>(\n      elements: test.sequence.map(OpaqueValue.init))\n    let other = MinimalSequence<OpaqueValue<Int32>>(\n      elements: test.other.map(OpaqueValue.init))\n    var result = zip(s, other)\n    expectType(\n      Zip2Sequence<MinimalSequence<OpaqueValue<Int>>, MinimalSequence<OpaqueValue<Int32>>>.self,\n      &result)\n\n    // Check for expected result and check the Zip2Sequence's Sequence\n    // conformance.\n    checkSequence(\n      test.expected.map { (OpaqueValue($0), OpaqueValue($1)) }, result,\n      stackTrace: SourceLocStack().with(test.loc), sameValue: compareElements)\n\n    // Check leftovers *after* doing checkSequence(), not before, to ensure\n    // that checkSequence() didn't force us to consume more elements than\n    // needed.\n    expectEqual(\n      test.expectedLeftoverSequence, s.map { $0.value },\n      stackTrace: SourceLocStack().with(test.loc))\n    expectEqual(\n      test.expectedLeftoverOther, other.map { $0.value },\n      stackTrace: SourceLocStack().with(test.loc))\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// underestimatedCount\n//===----------------------------------------------------------------------===//\n\nstruct SequenceWithDefaultUnderestimatedCount : Sequence {\n  init() {}\n\n  func makeIterator() -> MinimalSequence<OpaqueValue<Int>>.Iterator {\n    expectUnreachable()\n    return MinimalSequence(\n      elements: [ 1, 2, 3 ].map(OpaqueValue.init)\n    ).makeIterator()\n  }\n}\n\nSequenceTypeTests.test(\"underestimatedCount/Sequence/DefaultImplementation\") {\n  let s = SequenceWithDefaultUnderestimatedCount()\n  expectEqual(0, callGenericUnderestimatedCount(s))\n}\n\nstruct SequenceWithCustomUnderestimatedCount : Sequence {\n  init(underestimatedCount: Int) {\n    self._underestimatedCount = underestimatedCount\n  }\n\n  func makeIterator() -> MinimalSequence<OpaqueValue<Int>>.Iterator {\n    expectUnreachable()\n    return MinimalSequence(\n      elements: [ 0xffff, 0xffff, 0xffff ].map(OpaqueValue.init)\n    ).makeIterator()\n  }\n\n  var underestimatedCount: Int {\n    return _underestimatedCount\n  }\n\n  let _underestimatedCount: Int\n}\n\nSequenceTypeTests.test(\"underestimatedCount/Sequence/CustomImplementation\") {\n  do {\n    let s = SequenceWithCustomUnderestimatedCount(underestimatedCount: 5)\n    expectEqual(5, callGenericUnderestimatedCount(s))\n  }\n  do {\n    let s = SequenceWithCustomUnderestimatedCount(underestimatedCount: 42)\n    expectEqual(42, callGenericUnderestimatedCount(s))\n  }\n}\n\n//===----------------------------------------------------------------------===//\n// _copyToContiguousArray()\n//===----------------------------------------------------------------------===//\nSequenceTypeTests.test(\"_copyToContiguousArray/OverestimatedCount\")\n  .skip(.custom(\n    { _isFastAssertConfiguration() },\n    reason: \"this trap is not guaranteed to happen in -Ounchecked\"))\n  .code {\n  let s = MinimalSequence<OpaqueValue<Int>>(\n    elements: [ 1, 2, 3 ].map(OpaqueValue.init),\n    underestimatedCount: .value(4))\n  expectCrashLater()\n  let array = s._copyToContiguousArray()\n  _blackHole(array)\n}\n\n//===----------------------------------------------------------------------===//\n// Standard sequence tests\n//===----------------------------------------------------------------------===//\n\n% for Base in ['DefaultedSequence', 'MinimalSequence']:\ndo {\n  let resiliencyChecks = CollectionMisuseResiliencyChecks.all\n\n  SequenceTypeTests.addSequenceTests(\n    makeSequence: { (elements: [OpaqueValue<Int>]) in\n      return ${Base}(elements: elements)\n    },\n    wrapValue: identity,\n    extractValue: identity,\n    makeSequenceOfEquatable: { (elements: [MinimalEquatableValue]) in\n      return ${Base}(elements: elements)\n    },\n    wrapValueIntoEquatable: identityEq,\n    extractValueFromEquatable: identityEq,\n    resiliencyChecks: resiliencyChecks)\n}\n% end\n\nrunAllTests()\n", "subject": "warning suppression", "message": "warning suppression\n" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "tensorflow/tensorflow", "file": "install_bazel.sh", "language": "sh", "commit_date": "2016-06-22 22:16:40.000 UTC", "content": "#!/usr/bin/env bash\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# Select bazel version.\nBAZEL_VERSION=\"0.11.0\"\n\nset +e\nlocal_bazel_ver=$(bazel version 2>&1 | grep -i label | awk '{print $3}')\n\nif [[ \"$local_bazel_ver\" == \"$BAZEL_VERSION\" ]]; then\n  exit 0\nfi\n\nset -e\n\n# Install bazel.\nmkdir -p /bazel\ncd /bazel\nif [[ ! -f \"bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\" ]]; then\n  curl -fSsL -O https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\nfi\nchmod +x /bazel/bazel-*.sh\n/bazel/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\nrm -f /bazel/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\n\n# Enable bazel auto completion.\necho \"source /usr/local/lib/bazel/bin/bazel-complete.bash\" >> ~/.bashrc\n", "subject": "Merge pull request #2997 from maciekcc/branch_125575345", "message": "Merge pull request #2997 from maciekcc/branch_125575345\n\nBranch 125575345" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "tensorflow/tensorflow", "file": "ops.pbtxt", "language": "pbtxt", "commit_date": "2016-06-22 22:16:40.000 UTC", "content": "op {\n  name: \"Abort\"\n  attr {\n    name: \"error_msg\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"exit_without_error\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"Abs\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"AccumulateNV2\"\n  input_arg {\n    name: \"inputs\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"sum\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"shape\"\n    type: \"shape\"\n  }\n  is_aggregate: true\n  is_commutative: true\n}\nop {\n  name: \"AccumulatorApplyGradient\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"local_step\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"gradient\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"AccumulatorNumAccumulated\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"num_accumulated\"\n    type: DT_INT32\n  }\n}\nop {\n  name: \"AccumulatorSetGlobalStep\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"new_global_step\"\n    type: DT_INT64\n  }\n}\nop {\n  name: \"AccumulatorTakeGradient\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"num_required\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"average\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"Acos\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Acosh\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Add\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n        type: DT_STRING\n      }\n    }\n  }\n}\nop {\n  name: \"AddManySparseToTensorsMap\"\n  input_arg {\n    name: \"sparse_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"sparse_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"sparse_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sparse_handles\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"AddN\"\n  input_arg {\n    name: \"inputs\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"sum\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n        type: DT_VARIANT\n      }\n    }\n  }\n  is_aggregate: true\n  is_commutative: true\n}\nop {\n  name: \"AddSparseToTensorsMap\"\n  input_arg {\n    name: \"sparse_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"sparse_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"sparse_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sparse_handle\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"AddV2\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  is_aggregate: true\n  is_commutative: true\n}\nop {\n  name: \"AdjustContrast\"\n  input_arg {\n    name: \"images\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"contrast_factor\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min_value\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_value\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  deprecation {\n    version: 2\n    explanation: \"Use AdjustContrastv2 instead\"\n  }\n}\nop {\n  name: \"AdjustContrastv2\"\n  input_arg {\n    name: \"images\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"contrast_factor\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_FLOAT\n  }\n}\nop {\n  name: \"AdjustHue\"\n  input_arg {\n    name: \"images\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"delta\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_FLOAT\n  }\n}\nop {\n  name: \"AdjustSaturation\"\n  input_arg {\n    name: \"images\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"scale\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_FLOAT\n  }\n}\nop {\n  name: \"All\"\n  input_arg {\n    name: \"input\"\n    type: DT_BOOL\n  }\n  input_arg {\n    name: \"reduction_indices\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"keep_dims\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"AllCandidateSampler\"\n  input_arg {\n    name: \"true_classes\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sampled_candidates\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"true_expected_count\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"sampled_expected_count\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"num_true\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"num_sampled\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"unique\"\n    type: \"bool\"\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Angle\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"Tout\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_COMPLEX64\n    }\n    allowed_values {\n      list {\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  attr {\n    name: \"Tout\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"Any\"\n  input_arg {\n    name: \"input\"\n    type: DT_BOOL\n  }\n  input_arg {\n    name: \"reduction_indices\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"keep_dims\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"ApplyAdadelta\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum_update\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rho\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyAdagrad\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyAdagradDA\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"gradient_accumulator\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"gradient_squared_accumulator\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"global_step\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyAdam\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"m\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"v\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"beta1_power\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"beta2_power\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"beta1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"beta2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"use_nesterov\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyAddSign\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"m\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"alpha\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"sign_decay\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"beta\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyCenteredRMSProp\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"mg\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"ms\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"mom\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rho\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"momentum\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyFtrl\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"linear\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr_power\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyFtrlV2\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"linear\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2_shrinkage\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr_power\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyGradientDescent\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"alpha\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"delta\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyMomentum\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"momentum\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"use_nesterov\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyPowerSign\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"m\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"logbase\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"sign_decay\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"beta\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyProximalAdagrad\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyProximalGradientDescent\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"alpha\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"delta\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApplyRMSProp\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"ms\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"mom\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rho\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"momentum\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ApproximateEqual\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"tolerance\"\n    type: \"float\"\n    default_value {\n      f: 1e-05\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"ArgMax\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"dimension\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"output_type\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"output_type\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"ArgMin\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"dimension\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"output_type\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"output_type\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"AsString\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_BOOL\n        type: DT_INT8\n      }\n    }\n  }\n  attr {\n    name: \"precision\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"scientific\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"shortest\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"width\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"fill\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n}\nop {\n  name: \"Asin\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Asinh\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Assert\"\n  input_arg {\n    name: \"condition\"\n    type: DT_BOOL\n  }\n  input_arg {\n    name: \"data\"\n    type_list_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"summarize\"\n    type: \"int\"\n    default_value {\n      i: 3\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Assign\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"validate_shape\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  allows_uninitialized_input: true\n}\nop {\n  name: \"AssignAdd\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"AssignAddVariableOp\"\n  input_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"AssignSub\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"AssignSubVariableOp\"\n  input_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"AssignVariableOp\"\n  input_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"Atan\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Atan2\"\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"Atanh\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"AudioSpectrogram\"\n  input_arg {\n    name: \"input\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"spectrogram\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"window_size\"\n    type: \"int\"\n  }\n  attr {\n    name: \"stride\"\n    type: \"int\"\n  }\n  attr {\n    name: \"magnitude_squared\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"AudioSummary\"\n  input_arg {\n    name: \"tag\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"tensor\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"summary\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"sample_rate\"\n    type: \"float\"\n  }\n  attr {\n    name: \"max_outputs\"\n    type: \"int\"\n    default_value {\n      i: 3\n    }\n    has_minimum: true\n    minimum: 1\n  }\n  deprecation {\n    version: 15\n    explanation: \"Use AudioSummaryV2.\"\n  }\n}\nop {\n  name: \"AudioSummaryV2\"\n  input_arg {\n    name: \"tag\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"tensor\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"sample_rate\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"summary\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"max_outputs\"\n    type: \"int\"\n    default_value {\n      i: 3\n    }\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"AvgPool\"\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"AvgPool3D\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NDHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NDHWC\"\n        s: \"NCDHW\"\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"AvgPool3DGrad\"\n  input_arg {\n    name: \"orig_input_shape\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NDHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NDHWC\"\n        s: \"NCDHW\"\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"AvgPoolGrad\"\n  input_arg {\n    name: \"orig_input_shape\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"Barrier\"\n  output_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"shapes\"\n    type: \"list(shape)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"BarrierClose\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"cancel_pending_enqueues\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"BarrierIncompleteSize\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n}\nop {\n  name: \"BarrierInsertMany\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"keys\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"component_index\"\n    type: \"int\"\n  }\n}\nop {\n  name: \"BarrierReadySize\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n}\nop {\n  name: \"BarrierTakeMany\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"num_elements\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"keys\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"values\"\n    type_list_attr: \"component_types\"\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"allow_small_batch\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"wait_for_incomplete\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"timeout_ms\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n}\nop {\n  name: \"Batch\"\n  input_arg {\n    name: \"in_tensors\"\n    type_list_attr: \"T\"\n  }\n  output_arg {\n    name: \"batched_tensors\"\n    type_list_attr: \"T\"\n  }\n  output_arg {\n    name: \"batch_index\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"id\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"num_batch_threads\"\n    type: \"int\"\n  }\n  attr {\n    name: \"max_batch_size\"\n    type: \"int\"\n  }\n  attr {\n    name: \"max_enqueued_batches\"\n    type: \"int\"\n    default_value {\n      i: 10\n    }\n  }\n  attr {\n    name: \"batch_timeout_micros\"\n    type: \"int\"\n  }\n  attr {\n    name: \"allowed_batch_sizes\"\n    type: \"list(int)\"\n    default_value {\n      list {\n      }\n    }\n  }\n  attr {\n    name: \"grad_timeout_micros\"\n    type: \"int\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"batching_queue\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"BatchCholesky\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n      }\n    }\n  }\n  deprecation {\n    version: 13\n    explanation: \"Use Cholesky instead.\"\n  }\n}\nop {\n  name: \"BatchCholeskyGrad\"\n  input_arg {\n    name: \"l\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  deprecation {\n    version: 13\n    explanation: \"Use CholeskyGrad instead.\"\n  }\n}\nop {\n  name: \"BatchDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"batch_size\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"BatchFFT\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n  deprecation {\n    version: 15\n    explanation: \"Use FFT\"\n  }\n}\nop {\n  name: \"BatchFFT2D\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n  deprecation {\n    version: 15\n    explanation: \"Use FFT2D\"\n  }\n}\nop {\n  name: \"BatchFFT3D\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n  deprecation {\n    version: 15\n    explanation: \"Use FFT3D\"\n  }\n}\nop {\n  name: \"BatchIFFT\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n  deprecation {\n    version: 15\n    explanation: \"Use IFFT\"\n  }\n}\nop {\n  name: \"BatchIFFT2D\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n  deprecation {\n    version: 15\n    explanation: \"Use IFFT2D\"\n  }\n}\nop {\n  name: \"BatchIFFT3D\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n  deprecation {\n    version: 15\n    explanation: \"Use IFFT3D\"\n  }\n}\nop {\n  name: \"BatchMatMul\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  attr {\n    name: \"adj_x\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"adj_y\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"BatchMatrixBandPart\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"num_lower\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"num_upper\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"band\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 14\n    explanation: \"Use MatrixBandPart\"\n  }\n}\nop {\n  name: \"BatchMatrixDeterminant\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  deprecation {\n    version: 13\n    explanation: \"Use MatrixDeterminant instead.\"\n  }\n}\nop {\n  name: \"BatchMatrixDiag\"\n  input_arg {\n    name: \"diagonal\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 14\n    explanation: \"Use MatrixDiag\"\n  }\n}\nop {\n  name: \"BatchMatrixDiagPart\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"diagonal\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 14\n    explanation: \"Use MatrixDiagPart\"\n  }\n}\nop {\n  name: \"BatchMatrixInverse\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"adjoint\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n      }\n    }\n  }\n  deprecation {\n    version: 13\n    explanation: \"Use MatrixInverse instead.\"\n  }\n}\nop {\n  name: \"BatchMatrixSetDiag\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"diagonal\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 14\n    explanation: \"Use MatrixSetDiag\"\n  }\n}\nop {\n  name: \"BatchMatrixSolve\"\n  input_arg {\n    name: \"matrix\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rhs\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"adjoint\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n      }\n    }\n  }\n  deprecation {\n    version: 13\n    explanation: \"Use MatrixSolve instead.\"\n  }\n}\nop {\n  name: \"BatchMatrixSolveLs\"\n  input_arg {\n    name: \"matrix\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rhs\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2_regularizer\"\n    type: DT_DOUBLE\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"fast\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  deprecation {\n    version: 13\n    explanation: \"Use MatrixSolveLs instead.\"\n  }\n}\nop {\n  name: \"BatchMatrixTriangularSolve\"\n  input_arg {\n    name: \"matrix\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rhs\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"lower\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"adjoint\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n      }\n    }\n  }\n  deprecation {\n    version: 13\n    explanation: \"Use MatrixTriangularSolve instead.\"\n  }\n}\nop {\n  name: \"BatchNormWithGlobalNormalization\"\n  input_arg {\n    name: \"t\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"m\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"v\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"beta\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"gamma\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"result\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"variance_epsilon\"\n    type: \"float\"\n  }\n  attr {\n    name: \"scale_after_normalization\"\n    type: \"bool\"\n  }\n  deprecation {\n    version: 9\n    explanation: \"Use tf.nn.batch_normalization()\"\n  }\n}\nop {\n  name: \"BatchNormWithGlobalNormalizationGrad\"\n  input_arg {\n    name: \"t\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"m\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"v\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"gamma\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"dx\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"dm\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"dv\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"db\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"dg\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"variance_epsilon\"\n    type: \"float\"\n  }\n  attr {\n    name: \"scale_after_normalization\"\n    type: \"bool\"\n  }\n  deprecation {\n    version: 9\n    explanation: \"Use tf.nn.batch_normalization()\"\n  }\n}\nop {\n  name: \"BatchSelfAdjointEig\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n      }\n    }\n  }\n  deprecation {\n    version: 11\n    explanation: \"Use SelfAdjointEigV2 instead.\"\n  }\n}\nop {\n  name: \"BatchSelfAdjointEigV2\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"e\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"v\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"compute_v\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n      }\n    }\n  }\n  deprecation {\n    version: 13\n    explanation: \"Use SelfAdjointEigV2 instead.\"\n  }\n}\nop {\n  name: \"BatchSvd\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"s\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"u\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"v\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"compute_uv\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"full_matrices\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  deprecation {\n    version: 13\n    explanation: \"Use Svd instead.\"\n  }\n}\nop {\n  name: \"BatchToSpace\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"crops\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"block_size\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 2\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"BatchToSpaceND\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"block_shape\"\n    type_attr: \"Tblock_shape\"\n  }\n  input_arg {\n    name: \"crops\"\n    type_attr: \"Tcrops\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tblock_shape\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"Tcrops\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Betainc\"\n  input_arg {\n    name: \"a\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"b\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"BiasAdd\"\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"bias\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n}\nop {\n  name: \"BiasAddGrad\"\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n}\nop {\n  name: \"BiasAddV1\"\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"bias\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"Bincount\"\n  input_arg {\n    name: \"arr\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"weights\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"bins\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"Bitcast\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"type\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT64\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT16\n        type: DT_QUINT16\n        type: DT_QINT32\n        type: DT_HALF\n      }\n    }\n  }\n  attr {\n    name: \"type\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT64\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT16\n        type: DT_QUINT16\n        type: DT_QINT32\n        type: DT_HALF\n      }\n    }\n  }\n}\nop {\n  name: \"BitwiseAnd\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"BitwiseOr\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"BitwiseXor\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"BroadcastArgs\"\n  input_arg {\n    name: \"s0\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"s1\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"r0\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"BroadcastGradientArgs\"\n  input_arg {\n    name: \"s0\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"s1\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"r0\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"r1\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Bucketize\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"boundaries\"\n    type: \"list(float)\"\n  }\n}\nop {\n  name: \"BytesProducedStatsDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"tag\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"CTCBeamSearchDecoder\"\n  input_arg {\n    name: \"inputs\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"sequence_length\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"decoded_indices\"\n    type: DT_INT64\n    number_attr: \"top_paths\"\n  }\n  output_arg {\n    name: \"decoded_values\"\n    type: DT_INT64\n    number_attr: \"top_paths\"\n  }\n  output_arg {\n    name: \"decoded_shape\"\n    type: DT_INT64\n    number_attr: \"top_paths\"\n  }\n  output_arg {\n    name: \"log_probability\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"beam_width\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"top_paths\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"merge_repeated\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n}\nop {\n  name: \"CTCGreedyDecoder\"\n  input_arg {\n    name: \"inputs\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"sequence_length\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"decoded_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"decoded_values\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"decoded_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"log_probability\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"merge_repeated\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"CTCLoss\"\n  input_arg {\n    name: \"inputs\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"labels_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"labels_values\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"sequence_length\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"loss\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"gradient\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"preprocess_collapse_repeated\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"ctc_merge_repeated\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"ignore_longer_outputs_than_inputs\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"CacheDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"filename\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"Cast\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"SrcT\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"DstT\"\n  }\n  attr {\n    name: \"SrcT\"\n    type: \"type\"\n  }\n  attr {\n    name: \"DstT\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"Ceil\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"CheckNumerics\"\n  input_arg {\n    name: \"tensor\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"message\"\n    type: \"string\"\n  }\n}\nop {\n  name: \"Cholesky\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"CholeskyGrad\"\n  input_arg {\n    name: \"l\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"CompareAndBitpack\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"threshold\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_UINT8\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BOOL\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Complex\"\n  input_arg {\n    name: \"real\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"imag\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"Tout\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"Tout\"\n    type: \"type\"\n    default_value {\n      type: DT_COMPLEX64\n    }\n    allowed_values {\n      list {\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"ComplexAbs\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"Tout\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_COMPLEX64\n    }\n    allowed_values {\n      list {\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  attr {\n    name: \"Tout\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"ComputeAccidentalHits\"\n  input_arg {\n    name: \"true_classes\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"sampled_candidates\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"ids\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"weights\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"num_true\"\n    type: \"int\"\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n}\nop {\n  name: \"Concat\"\n  input_arg {\n    name: \"concat_dim\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 2\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"ConcatOffset\"\n  input_arg {\n    name: \"concat_dim\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"shape\"\n    type: DT_INT32\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"offset\"\n    type: DT_INT32\n    number_attr: \"N\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 2\n  }\n}\nop {\n  name: \"ConcatV2\"\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n  }\n  input_arg {\n    name: \"axis\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 2\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"ConcatenateDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"another_dataset\"\n    type: DT_VARIANT\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"ConditionalAccumulator\"\n  output_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"shape\"\n    type: \"shape\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Conj\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_COMPLEX64\n    }\n    allowed_values {\n      list {\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n        type: DT_VARIANT\n      }\n    }\n  }\n}\nop {\n  name: \"ConjugateTranspose\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"perm\"\n    type_attr: \"Tperm\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tperm\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Const\"\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"value\"\n    type: \"tensor\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"ConsumeMutexLock\"\n  input_arg {\n    name: \"mutex_lock\"\n    type: DT_VARIANT\n  }\n  is_stateful: true\n}\nop {\n  name: \"ControlTrigger\"\n}\nop {\n  name: \"Conv2D\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"use_cudnn_on_gpu\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n  attr {\n    name: \"dilations\"\n    type: \"list(int)\"\n    default_value {\n      list {\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n      }\n    }\n  }\n}\nop {\n  name: \"Conv2DBackpropFilter\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"filter_sizes\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"use_cudnn_on_gpu\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n  attr {\n    name: \"dilations\"\n    type: \"list(int)\"\n    default_value {\n      list {\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n      }\n    }\n  }\n}\nop {\n  name: \"Conv2DBackpropInput\"\n  input_arg {\n    name: \"input_sizes\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"use_cudnn_on_gpu\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n  attr {\n    name: \"dilations\"\n    type: \"list(int)\"\n    default_value {\n      list {\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n      }\n    }\n  }\n}\nop {\n  name: \"Conv3D\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NDHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NDHWC\"\n        s: \"NCDHW\"\n      }\n    }\n  }\n  attr {\n    name: \"dilations\"\n    type: \"list(int)\"\n    default_value {\n      list {\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n      }\n    }\n  }\n}\nop {\n  name: \"Conv3DBackpropFilter\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  deprecation {\n    version: 10\n    explanation: \"Use Conv3DBackpropFilterV2\"\n  }\n}\nop {\n  name: \"Conv3DBackpropFilterV2\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"filter_sizes\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NDHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NDHWC\"\n        s: \"NCDHW\"\n      }\n    }\n  }\n  attr {\n    name: \"dilations\"\n    type: \"list(int)\"\n    default_value {\n      list {\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n      }\n    }\n  }\n}\nop {\n  name: \"Conv3DBackpropInput\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  deprecation {\n    version: 10\n    explanation: \"Use Conv3DBackpropInputV2\"\n  }\n}\nop {\n  name: \"Conv3DBackpropInputV2\"\n  input_arg {\n    name: \"input_sizes\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NDHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NDHWC\"\n        s: \"NCDHW\"\n      }\n    }\n  }\n  attr {\n    name: \"dilations\"\n    type: \"list(int)\"\n    default_value {\n      list {\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n      }\n    }\n  }\n}\nop {\n  name: \"Copy\"\n  input_arg {\n    name: \"input\"\n    description: \"Input tensor.\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    description: \"Output tensor, deep-copied from input.\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"tensor_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n    description: \"The name of the input tensor.\"\n  }\n  attr {\n    name: \"debug_ops_spec\"\n    type: \"list(string)\"\n    default_value {\n      list {\n      }\n    }\n    description: \"A list of debug op spec (op, url, gated_grpc) for attached debug\\nops. Each element of the list has the format\\n<debug_op>;<grpc_url>;<gated_grpc>, wherein gated_grpc is boolean represented\\nas 0/1. E.g., \\\"DebugIdentity;grpc://foo:3333;1\\\",\\n\\\"DebugIdentity;file:///tmp/tfdbg_1;0\\\".\"\n  }\n  summary: \"Copy Op.\"\n  description: \"Performs CPU-to-CPU or GPU-to-GPU deep-copying of tensor, depending on the\\ndevice on which the tensor is allocated.\\nN.B.: If the all downstream attached debug ops are disabled given the current\\ngRPC gating status, the output will simply forward the input tensor without\\ndeep-copying. See the documentation of Debug* ops for more details.\\n\\nUnlike the CopyHost Op, this op does not have HostMemory constraint on its\\ninput or output.\"\n  allows_uninitialized_input: true\n}\nop {\n  name: \"CopyHost\"\n  input_arg {\n    name: \"input\"\n    description: \"Input tensor.\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    description: \"Output tensor, deep-copied from input.\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"tensor_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n    description: \"The name of the input tensor.\"\n  }\n  attr {\n    name: \"debug_ops_spec\"\n    type: \"list(string)\"\n    default_value {\n      list {\n      }\n    }\n    description: \"A list of debug op spec (op, url, gated_grpc) for attached debug\\nops. Each element of the list has the format\\n<debug_op>;<grpc_url>;<gated_grpc>, wherein gated_grpc is boolean represented\\nas 0/1. E.g., \\\"DebugIdentity;grpc://foo:3333;1\\\",\\n\\\"DebugIdentity;file:///tmp/tfdbg_1;0\\\".\"\n  }\n  summary: \"Copy Host Op.\"\n  description: \"Performs CPU-to-CPU deep-copying of tensor.\\nN.B.: If the all downstream attached debug ops are disabled given the current\\ngRPC gating status, the output will simply forward the input tensor without\\ndeep-copying. See the documentation of Debug* ops for more details.\\n\\nUnlike the Copy Op, this op has HostMemory constraint on its input or output.\"\n  allows_uninitialized_input: true\n}\nop {\n  name: \"Cos\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Cosh\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"CountUpTo\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"limit\"\n    type: \"int\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"CropAndResize\"\n  input_arg {\n    name: \"image\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"boxes\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"box_ind\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"crop_size\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"crops\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"method\"\n    type: \"string\"\n    default_value {\n      s: \"bilinear\"\n    }\n    allowed_values {\n      list {\n        s: \"bilinear\"\n      }\n    }\n  }\n  attr {\n    name: \"extrapolation_value\"\n    type: \"float\"\n    default_value {\n      f: 0\n    }\n  }\n}\nop {\n  name: \"CropAndResizeGradBoxes\"\n  input_arg {\n    name: \"grads\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"image\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"boxes\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"box_ind\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"method\"\n    type: \"string\"\n    default_value {\n      s: \"bilinear\"\n    }\n    allowed_values {\n      list {\n        s: \"bilinear\"\n      }\n    }\n  }\n}\nop {\n  name: \"CropAndResizeGradImage\"\n  input_arg {\n    name: \"grads\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"boxes\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"box_ind\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"image_size\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_HALF\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"method\"\n    type: \"string\"\n    default_value {\n      s: \"bilinear\"\n    }\n    allowed_values {\n      list {\n        s: \"bilinear\"\n      }\n    }\n  }\n}\nop {\n  name: \"Cross\"\n  input_arg {\n    name: \"a\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"b\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"product\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"Cumprod\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"axis\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"exclusive\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"reverse\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Cumsum\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"axis\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"exclusive\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"reverse\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"DataFormatDimMap\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"src_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n  }\n  attr {\n    name: \"dst_format\"\n    type: \"string\"\n    default_value {\n      s: \"NCHW\"\n    }\n  }\n}\nop {\n  name: \"DataFormatVecPermute\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"src_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n  }\n  attr {\n    name: \"dst_format\"\n    type: \"string\"\n    default_value {\n      s: \"NCHW\"\n    }\n  }\n}\nop {\n  name: \"DatasetToSingleElement\"\n  input_arg {\n    name: \"dataset\"\n    type: DT_VARIANT\n  }\n  output_arg {\n    name: \"components\"\n    type_list_attr: \"output_types\"\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"DebugGradientIdentity\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  allows_uninitialized_input: true\n}\nop {\n  name: \"DebugGradientRefIdentity\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  allows_uninitialized_input: true\n}\nop {\n  name: \"DebugIdentity\"\n  input_arg {\n    name: \"input\"\n    description: \"Input tensor, non-Reference type.\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    description: \"Output tensor that equals the input tensor.\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"device_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"tensor_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n    description: \"Name of the input tensor.\"\n  }\n  attr {\n    name: \"debug_urls\"\n    type: \"list(string)\"\n    default_value {\n      list {\n      }\n    }\n    description: \"List of URLs to debug targets, e.g.,\\nfile:///foo/tfdbg_dump, grpc:://localhost:11011\"\n  }\n  attr {\n    name: \"gated_grpc\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n    description: \"Whether this op will be gated. If any of the debug_urls of this\\ndebug node is of the grpc:// scheme, when the value of this attribute is set\\nto True, the data will not actually be sent via the grpc stream unless this\\ndebug op has been enabled at the debug_url. If all of the debug_urls of this\\ndebug node are of the grpc:// scheme and the debug op is enabled at none of\\nthem, the output will be an empty Tensor.\"\n  }\n  summary: \"Debug Identity Op.\"\n  description: \"Provides an identity mapping of the non-Ref type input tensor for debugging.\"\n  allows_uninitialized_input: true\n}\nop {\n  name: \"DebugNanCount\"\n  input_arg {\n    name: \"input\"\n    description: \"Input tensor, non-Reference type.\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    description: \"An integer output tensor that is the number of NaNs in the input.\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"device_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"tensor_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n    description: \"Name of the input tensor.\"\n  }\n  attr {\n    name: \"debug_urls\"\n    type: \"list(string)\"\n    default_value {\n      list {\n      }\n    }\n    description: \"List of URLs to debug targets, e.g.,\\nfile:///foo/tfdbg_dump, grpc:://localhost:11011.\"\n  }\n  attr {\n    name: \"gated_grpc\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n    description: \"Whether this op will be gated. If any of the debug_urls of this\\ndebug node is of the grpc:// scheme, when the value of this attribute is set\\nto True, the data will not actually be sent via the grpc stream unless this\\ndebug op has been enabled at the debug_url. If all of the debug_urls of this\\ndebug node are of the grpc:// scheme and the debug op is enabled at none of\\nthem, the output will be an empty Tensor.\"\n  }\n  summary: \"Debug NaN Value Counter Op\"\n  description: \"Counts number of NaNs in the input tensor, for debugging.\"\n  allows_uninitialized_input: true\n}\nop {\n  name: \"DebugNumericSummary\"\n  input_arg {\n    name: \"input\"\n    description: \"Input tensor, non-Reference type, float or double.\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    description: \"A double tensor of shape [14 + nDimensions], where nDimensions is the\\n  the number of dimensions of the tensor\\'s shape. The elements of output are:\\n  [0]: is initialized (1.0) or not (0.0).\\n  [1]: total number of elements\\n  [2]: NaN element count\\n  [3]: generalized -inf count: elements <= lower_bound. lower_bound is -inf by\\n    default.\\n  [4]: negative element count (excluding -inf), if lower_bound is the default\\n    -inf. Otherwise, this is the count of elements > lower_bound and < 0.\\n  [5]: zero element count\\n  [6]: positive element count (excluding +inf), if upper_bound is the default\\n    -inf. Otherwise, this is the count of elements < upper_bound and > 0.\\n  [7]: generalized +inf count, elements >= upper_bound. upper_bound is +inf by\\n    default.\\nOutput elements [1:8] are all zero, if the tensor is uninitialized.\\n  [8]: minimum of all non-inf and non-NaN elements.\\n       If uninitialized or no such element exists: +inf.\\n  [9]: maximum of all non-inf and non-NaN elements.\\n       If uninitialized or no such element exists: -inf.\\n  [10]: mean of all non-inf and non-NaN elements.\\n        If uninitialized or no such element exists: NaN.\\n  [11]: variance of all non-inf and non-NaN elements.\\n        If uninitialized or no such element exists: NaN.\\n  [12]: Data type of the tensor encoded as an enum integer. See the DataType\\n        proto for more details.\\n  [13]: Number of dimensions of the tensor (ndims).\\n  [14+]: Sizes of the dimensions.\"\n    type: DT_DOUBLE\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"device_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"tensor_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n    description: \"Name of the input tensor.\"\n  }\n  attr {\n    name: \"debug_urls\"\n    type: \"list(string)\"\n    default_value {\n      list {\n      }\n    }\n    description: \"List of URLs to debug targets, e.g.,\\nfile:///foo/tfdbg_dump, grpc:://localhost:11011\"\n  }\n  attr {\n    name: \"lower_bound\"\n    type: \"float\"\n    default_value {\n      f: -inf\n    }\n    description: \"(float) The lower bound <= which values will be included in the\\ngeneralized -inf count. Default: -inf.\"\n  }\n  attr {\n    name: \"upper_bound\"\n    type: \"float\"\n    default_value {\n      f: inf\n    }\n    description: \"(float) The upper bound >= which values will be included in the\\ngeneralized +inf count. Default: +inf.\"\n  }\n  attr {\n    name: \"mute_if_healthy\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n    description: \"(bool) Do not send data to the debug URLs unless at least one\\nof elements [2], [3] and [7] (i.e., the nan count and the generalized -inf and\\ninf counts) is non-zero.\"\n  }\n  attr {\n    name: \"gated_grpc\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n    description: \"Whether this op will be gated. If any of the debug_urls of this\\ndebug node is of the grpc:// scheme, when the value of this attribute is set\\nto True, the data will not actually be sent via the grpc stream unless this\\ndebug op has been enabled at the debug_url. If all of the debug_urls of this\\ndebug node are of the grpc:// scheme and the debug op is enabled at none of\\nthem, the output will be an empty Tensor.\"\n  }\n  summary: \"Debug Numeric Summary Op.\"\n  description: \"Provide a basic summary of numeric value types, range and distribution.\"\n  allows_uninitialized_input: true\n}\nop {\n  name: \"DecodeAndCropJpeg\"\n  input_arg {\n    name: \"contents\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"crop_window\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"image\"\n    type: DT_UINT8\n  }\n  attr {\n    name: \"channels\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"ratio\"\n    type: \"int\"\n    default_value {\n      i: 1\n    }\n  }\n  attr {\n    name: \"fancy_upscaling\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"try_recover_truncated\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"acceptable_fraction\"\n    type: \"float\"\n    default_value {\n      f: 1\n    }\n  }\n  attr {\n    name: \"dct_method\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n}\nop {\n  name: \"DecodeBase64\"\n  input_arg {\n    name: \"input\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"DecodeBmp\"\n  input_arg {\n    name: \"contents\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"image\"\n    type: DT_UINT8\n  }\n  attr {\n    name: \"channels\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n}\nop {\n  name: \"DecodeCSV\"\n  input_arg {\n    name: \"records\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"record_defaults\"\n    type_list_attr: \"OUT_TYPE\"\n  }\n  output_arg {\n    name: \"output\"\n    type_list_attr: \"OUT_TYPE\"\n  }\n  attr {\n    name: \"OUT_TYPE\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n  attr {\n    name: \"field_delim\"\n    type: \"string\"\n    default_value {\n      s: \",\"\n    }\n  }\n  attr {\n    name: \"use_quote_delim\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"na_value\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n}\nop {\n  name: \"DecodeCompressed\"\n  input_arg {\n    name: \"bytes\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"compression_type\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n}\nop {\n  name: \"DecodeGif\"\n  input_arg {\n    name: \"contents\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"image\"\n    type: DT_UINT8\n  }\n}\nop {\n  name: \"DecodeJSONExample\"\n  input_arg {\n    name: \"json_examples\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"binary_examples\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"DecodeJpeg\"\n  input_arg {\n    name: \"contents\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"image\"\n    type: DT_UINT8\n  }\n  attr {\n    name: \"channels\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"ratio\"\n    type: \"int\"\n    default_value {\n      i: 1\n    }\n  }\n  attr {\n    name: \"fancy_upscaling\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"try_recover_truncated\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"acceptable_fraction\"\n    type: \"float\"\n    default_value {\n      f: 1\n    }\n  }\n  attr {\n    name: \"dct_method\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n}\nop {\n  name: \"DecodePng\"\n  input_arg {\n    name: \"contents\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"image\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"channels\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    default_value {\n      type: DT_UINT8\n    }\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_UINT16\n      }\n    }\n  }\n}\nop {\n  name: \"DecodeRaw\"\n  input_arg {\n    name: \"bytes\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"out_type\"\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT16\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"little_endian\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n}\nop {\n  name: \"DecodeWav\"\n  input_arg {\n    name: \"contents\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"audio\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"sample_rate\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"desired_channels\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"desired_samples\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n}\nop {\n  name: \"DeleteSessionTensor\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  is_stateful: true\n}\nop {\n  name: \"DenseToDenseSetOperation\"\n  input_arg {\n    name: \"set1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"set2\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"result_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"result_values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"result_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"set_operation\"\n    type: \"string\"\n  }\n  attr {\n    name: \"validate_indices\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_STRING\n      }\n    }\n  }\n}\nop {\n  name: \"DenseToSparseBatchDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"batch_size\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"row_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"DenseToSparseSetOperation\"\n  input_arg {\n    name: \"set1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"set2_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"set2_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"set2_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"result_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"result_values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"result_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"set_operation\"\n    type: \"string\"\n  }\n  attr {\n    name: \"validate_indices\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_STRING\n      }\n    }\n  }\n}\nop {\n  name: \"DepthToSpace\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"block_size\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 2\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n        s: \"NCHW_VECT_C\"\n      }\n    }\n  }\n}\nop {\n  name: \"DepthwiseConv2dNative\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n  attr {\n    name: \"dilations\"\n    type: \"list(int)\"\n    default_value {\n      list {\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n      }\n    }\n  }\n}\nop {\n  name: \"DepthwiseConv2dNativeBackpropFilter\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"filter_sizes\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n  attr {\n    name: \"dilations\"\n    type: \"list(int)\"\n    default_value {\n      list {\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n      }\n    }\n  }\n}\nop {\n  name: \"DepthwiseConv2dNativeBackpropInput\"\n  input_arg {\n    name: \"input_sizes\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n  attr {\n    name: \"dilations\"\n    type: \"list(int)\"\n    default_value {\n      list {\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n      }\n    }\n  }\n}\nop {\n  name: \"Dequantize\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"min_range\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_range\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"mode\"\n    type: \"string\"\n    default_value {\n      s: \"MIN_COMBINED\"\n    }\n    allowed_values {\n      list {\n        s: \"MIN_COMBINED\"\n        s: \"MIN_FIRST\"\n        s: \"SCALED\"\n      }\n    }\n  }\n}\nop {\n  name: \"DeserializeIterator\"\n  input_arg {\n    name: \"resource_handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"serialized\"\n    type: DT_VARIANT\n  }\n  is_stateful: true\n}\nop {\n  name: \"DeserializeManySparse\"\n  input_arg {\n    name: \"serialized_sparse\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"sparse_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sparse_values\"\n    type_attr: \"dtype\"\n  }\n  output_arg {\n    name: \"sparse_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"DeserializeSparse\"\n  input_arg {\n    name: \"serialized_sparse\"\n    type_attr: \"Tserialized\"\n  }\n  output_arg {\n    name: \"sparse_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sparse_values\"\n    type_attr: \"dtype\"\n  }\n  output_arg {\n    name: \"sparse_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tserialized\"\n    type: \"type\"\n    default_value {\n      type: DT_STRING\n    }\n    allowed_values {\n      list {\n        type: DT_STRING\n        type: DT_VARIANT\n      }\n    }\n  }\n}\nop {\n  name: \"DestroyResourceOp\"\n  input_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"ignore_lookup_error\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"DestroyTemporaryVariable\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"var_name\"\n    type: \"string\"\n  }\n}\nop {\n  name: \"Diag\"\n  input_arg {\n    name: \"diagonal\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"DiagPart\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"diagonal\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Digamma\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"Dilation2D\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"rates\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n}\nop {\n  name: \"Dilation2DBackpropFilter\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"filter_backprop\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"rates\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n}\nop {\n  name: \"Dilation2DBackpropInput\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"in_backprop\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"rates\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n}\nop {\n  name: \"Div\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_UINT16\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"DrawBoundingBoxes\"\n  input_arg {\n    name: \"images\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"boxes\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_HALF\n      }\n    }\n  }\n}\nop {\n  name: \"DynamicPartition\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"partitions\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"outputs\"\n    type_attr: \"T\"\n    number_attr: \"num_partitions\"\n  }\n  attr {\n    name: \"num_partitions\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"DynamicStitch\"\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n    number_attr: \"N\"\n  }\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"merged\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"EagerPyFunc\"\n  input_arg {\n    name: \"input\"\n    type_list_attr: \"Tin\"\n  }\n  output_arg {\n    name: \"output\"\n    type_list_attr: \"Tout\"\n  }\n  attr {\n    name: \"token\"\n    type: \"string\"\n  }\n  attr {\n    name: \"Tin\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"Tout\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  is_stateful: true\n}\nop {\n  name: \"EditDistance\"\n  input_arg {\n    name: \"hypothesis_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"hypothesis_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"hypothesis_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"truth_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"truth_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"truth_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"normalize\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"Elu\"\n  input_arg {\n    name: \"features\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"activations\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"EluGrad\"\n  input_arg {\n    name: \"gradients\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"outputs\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"backprops\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"EmptyTensorList\"\n  input_arg {\n    name: \"element_shape\"\n    type_attr: \"shape_type\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"element_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"shape_type\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"EncodeBase64\"\n  input_arg {\n    name: \"input\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"pad\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"EncodeJpeg\"\n  input_arg {\n    name: \"image\"\n    type: DT_UINT8\n  }\n  output_arg {\n    name: \"contents\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"format\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n    allowed_values {\n      list {\n        s: \"\"\n        s: \"grayscale\"\n        s: \"rgb\"\n      }\n    }\n  }\n  attr {\n    name: \"quality\"\n    type: \"int\"\n    default_value {\n      i: 95\n    }\n  }\n  attr {\n    name: \"progressive\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"optimize_size\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"chroma_downsampling\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"density_unit\"\n    type: \"string\"\n    default_value {\n      s: \"in\"\n    }\n    allowed_values {\n      list {\n        s: \"in\"\n        s: \"cm\"\n      }\n    }\n  }\n  attr {\n    name: \"x_density\"\n    type: \"int\"\n    default_value {\n      i: 300\n    }\n  }\n  attr {\n    name: \"y_density\"\n    type: \"int\"\n    default_value {\n      i: 300\n    }\n  }\n  attr {\n    name: \"xmp_metadata\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n}\nop {\n  name: \"EncodePng\"\n  input_arg {\n    name: \"image\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"contents\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"compression\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_UINT8\n    }\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_UINT16\n      }\n    }\n  }\n}\nop {\n  name: \"EncodeWav\"\n  input_arg {\n    name: \"audio\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"sample_rate\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"contents\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"EnqueueInQueueDataset\"\n  input_arg {\n    name: \"queue\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"components\"\n    type_list_attr: \"Tcomponents\"\n  }\n  attr {\n    name: \"Tcomponents\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"Enter\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"frame_name\"\n    type: \"string\"\n  }\n  attr {\n    name: \"is_constant\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"parallel_iterations\"\n    type: \"int\"\n    default_value {\n      i: 10\n    }\n  }\n}\nop {\n  name: \"Equal\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_QUINT8\n        type: DT_QINT8\n        type: DT_QINT32\n        type: DT_STRING\n        type: DT_BOOL\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"Erf\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"Erfc\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"Exit\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"Exp\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"ExpandDims\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"dim\"\n    type_attr: \"Tdim\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tdim\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Expm1\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"ExtractGlimpse\"\n  input_arg {\n    name: \"input\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"offsets\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"glimpse\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"centered\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"normalized\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"uniform_noise\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n}\nop {\n  name: \"ExtractImagePatches\"\n  input_arg {\n    name: \"images\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"patches\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"ksizes\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"rates\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n}\nop {\n  name: \"ExtractJpegShape\"\n  input_arg {\n    name: \"contents\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"image_shape\"\n    type_attr: \"output_type\"\n  }\n  attr {\n    name: \"output_type\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"FFT\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n}\nop {\n  name: \"FFT2D\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n}\nop {\n  name: \"FFT3D\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n}\nop {\n  name: \"FIFOQueue\"\n  output_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"shapes\"\n    type: \"list(shape)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"FIFOQueueV2\"\n  output_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"shapes\"\n    type: \"list(shape)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Fact\"\n  output_arg {\n    name: \"fact\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"FakeQuantWithMinMaxArgs\"\n  input_arg {\n    name: \"inputs\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"outputs\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"min\"\n    type: \"float\"\n    default_value {\n      f: -6\n    }\n  }\n  attr {\n    name: \"max\"\n    type: \"float\"\n    default_value {\n      f: 6\n    }\n  }\n  attr {\n    name: \"num_bits\"\n    type: \"int\"\n    default_value {\n      i: 8\n    }\n  }\n  attr {\n    name: \"narrow_range\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"FakeQuantWithMinMaxArgsGradient\"\n  input_arg {\n    name: \"gradients\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"inputs\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"backprops\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"min\"\n    type: \"float\"\n    default_value {\n      f: -6\n    }\n  }\n  attr {\n    name: \"max\"\n    type: \"float\"\n    default_value {\n      f: 6\n    }\n  }\n  attr {\n    name: \"num_bits\"\n    type: \"int\"\n    default_value {\n      i: 8\n    }\n  }\n  attr {\n    name: \"narrow_range\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"FakeQuantWithMinMaxVars\"\n  input_arg {\n    name: \"inputs\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"outputs\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"num_bits\"\n    type: \"int\"\n    default_value {\n      i: 8\n    }\n  }\n  attr {\n    name: \"narrow_range\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"FakeQuantWithMinMaxVarsGradient\"\n  input_arg {\n    name: \"gradients\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"inputs\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"backprops_wrt_input\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"backprop_wrt_min\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"backprop_wrt_max\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"num_bits\"\n    type: \"int\"\n    default_value {\n      i: 8\n    }\n  }\n  attr {\n    name: \"narrow_range\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"FakeQuantWithMinMaxVarsPerChannel\"\n  input_arg {\n    name: \"inputs\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"outputs\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"num_bits\"\n    type: \"int\"\n    default_value {\n      i: 8\n    }\n  }\n  attr {\n    name: \"narrow_range\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"FakeQuantWithMinMaxVarsPerChannelGradient\"\n  input_arg {\n    name: \"gradients\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"inputs\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"backprops_wrt_input\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"backprop_wrt_min\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"backprop_wrt_max\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"num_bits\"\n    type: \"int\"\n    default_value {\n      i: 8\n    }\n  }\n  attr {\n    name: \"narrow_range\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"FakeQueue\"\n  input_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  is_stateful: true\n}\nop {\n  name: \"Fill\"\n  input_arg {\n    name: \"dims\"\n    type_attr: \"index_type\"\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"index_type\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"FilterDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"other_arguments\"\n    type_list_attr: \"Targuments\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"predicate\"\n    type: \"func\"\n  }\n  attr {\n    name: \"Targuments\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"FixedLengthRecordDataset\"\n  input_arg {\n    name: \"filenames\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"header_bytes\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"record_bytes\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"footer_bytes\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"buffer_size\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  is_stateful: true\n}\nop {\n  name: \"FixedLengthRecordReader\"\n  output_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"header_bytes\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"record_bytes\"\n    type: \"int\"\n  }\n  attr {\n    name: \"footer_bytes\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"hop_bytes\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use FixedLengthRecordReaderV2\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"FixedLengthRecordReaderV2\"\n  output_arg {\n    name: \"reader_handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"header_bytes\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"record_bytes\"\n    type: \"int\"\n  }\n  attr {\n    name: \"footer_bytes\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"hop_bytes\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"encoding\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"FixedUnigramCandidateSampler\"\n  input_arg {\n    name: \"true_classes\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sampled_candidates\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"true_expected_count\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"sampled_expected_count\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"num_true\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"num_sampled\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"unique\"\n    type: \"bool\"\n  }\n  attr {\n    name: \"range_max\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"vocab_file\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"distortion\"\n    type: \"float\"\n    default_value {\n      f: 1\n    }\n  }\n  attr {\n    name: \"num_reserved_ids\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"num_shards\"\n    type: \"int\"\n    default_value {\n      i: 1\n    }\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"shard\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"unigrams\"\n    type: \"list(float)\"\n    default_value {\n      list {\n      }\n    }\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"FlatMapDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"other_arguments\"\n    type_list_attr: \"Targuments\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"f\"\n    type: \"func\"\n  }\n  attr {\n    name: \"Targuments\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"Floor\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"FloorDiv\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_UINT16\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"FloorMod\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"FractionalAvgPool\"\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"row_pooling_sequence\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"col_pooling_sequence\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"pooling_ratio\"\n    type: \"list(float)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"pseudo_random\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"overlapping\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"deterministic\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"FractionalAvgPoolGrad\"\n  input_arg {\n    name: \"orig_input_tensor_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"row_pooling_sequence\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"col_pooling_sequence\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"overlapping\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"FractionalMaxPool\"\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"row_pooling_sequence\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"col_pooling_sequence\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"pooling_ratio\"\n    type: \"list(float)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"pseudo_random\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"overlapping\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"deterministic\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"FractionalMaxPoolGrad\"\n  input_arg {\n    name: \"orig_input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"orig_output\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"out_backprop\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"row_pooling_sequence\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"col_pooling_sequence\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"overlapping\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"FusedBatchNorm\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"scale\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"offset\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"mean\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"variance\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"batch_mean\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"batch_variance\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"reserve_space_1\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"reserve_space_2\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"epsilon\"\n    type: \"float\"\n    default_value {\n      f: 0.0001\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n  }\n  attr {\n    name: \"is_training\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n}\nop {\n  name: \"FusedBatchNormGrad\"\n  input_arg {\n    name: \"y_backprop\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"scale\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"reserve_space_1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"reserve_space_2\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"x_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"scale_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"offset_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"reserve_space_3\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"reserve_space_4\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"epsilon\"\n    type: \"float\"\n    default_value {\n      f: 0.0001\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n  }\n  attr {\n    name: \"is_training\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n}\nop {\n  name: \"FusedBatchNormGradV2\"\n  input_arg {\n    name: \"y_backprop\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"scale\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"reserve_space_1\"\n    type_attr: \"U\"\n  }\n  input_arg {\n    name: \"reserve_space_2\"\n    type_attr: \"U\"\n  }\n  output_arg {\n    name: \"x_backprop\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"scale_backprop\"\n    type_attr: \"U\"\n  }\n  output_arg {\n    name: \"offset_backprop\"\n    type_attr: \"U\"\n  }\n  output_arg {\n    name: \"reserve_space_3\"\n    type_attr: \"U\"\n  }\n  output_arg {\n    name: \"reserve_space_4\"\n    type_attr: \"U\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"U\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"epsilon\"\n    type: \"float\"\n    default_value {\n      f: 0.0001\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n  }\n  attr {\n    name: \"is_training\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n}\nop {\n  name: \"FusedBatchNormV2\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"scale\"\n    type_attr: \"U\"\n  }\n  input_arg {\n    name: \"offset\"\n    type_attr: \"U\"\n  }\n  input_arg {\n    name: \"mean\"\n    type_attr: \"U\"\n  }\n  input_arg {\n    name: \"variance\"\n    type_attr: \"U\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"batch_mean\"\n    type_attr: \"U\"\n  }\n  output_arg {\n    name: \"batch_variance\"\n    type_attr: \"U\"\n  }\n  output_arg {\n    name: \"reserve_space_1\"\n    type_attr: \"U\"\n  }\n  output_arg {\n    name: \"reserve_space_2\"\n    type_attr: \"U\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"U\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"epsilon\"\n    type: \"float\"\n    default_value {\n      f: 0.0001\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n  }\n  attr {\n    name: \"is_training\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n}\nop {\n  name: \"FusedPadConv2D\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"paddings\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"mode\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"REFLECT\"\n        s: \"SYMMETRIC\"\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n}\nop {\n  name: \"FusedResizeAndPadConv2D\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"paddings\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"resize_align_corners\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"mode\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"REFLECT\"\n        s: \"SYMMETRIC\"\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n}\nop {\n  name: \"Gather\"\n  input_arg {\n    name: \"params\"\n    type_attr: \"Tparams\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"Tparams\"\n  }\n  attr {\n    name: \"validate_indices\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"Tparams\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"GatherNd\"\n  input_arg {\n    name: \"params\"\n    type_attr: \"Tparams\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"Tparams\"\n  }\n  attr {\n    name: \"Tparams\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"GatherV2\"\n  input_arg {\n    name: \"params\"\n    type_attr: \"Tparams\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"axis\"\n    type_attr: \"Taxis\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"Tparams\"\n  }\n  attr {\n    name: \"Tparams\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"Taxis\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"GenerateVocabRemapping\"\n  input_arg {\n    name: \"new_vocab_file\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"old_vocab_file\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"remapping\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"num_present\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"new_vocab_offset\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"num_new_vocab\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"old_vocab_size\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n    has_minimum: true\n    minimum: -1\n  }\n}\nop {\n  name: \"GeneratorDataset\"\n  input_arg {\n    name: \"init_func_other_args\"\n    type_list_attr: \"Tinit_func_args\"\n  }\n  input_arg {\n    name: \"next_func_other_args\"\n    type_list_attr: \"Tnext_func_args\"\n  }\n  input_arg {\n    name: \"finalize_func_other_args\"\n    type_list_attr: \"Tfinalize_func_args\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"init_func\"\n    type: \"func\"\n  }\n  attr {\n    name: \"next_func\"\n    type: \"func\"\n  }\n  attr {\n    name: \"finalize_func\"\n    type: \"func\"\n  }\n  attr {\n    name: \"Tinit_func_args\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"Tnext_func_args\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"Tfinalize_func_args\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"GetSessionHandle\"\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"GetSessionHandleV2\"\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"GetSessionTensor\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"Greater\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"GreaterEqual\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"GroupByWindowDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"key_func_other_arguments\"\n    type_list_attr: \"Tkey_func_other_arguments\"\n  }\n  input_arg {\n    name: \"reduce_func_other_arguments\"\n    type_list_attr: \"Treduce_func_other_arguments\"\n  }\n  input_arg {\n    name: \"window_size_func_other_arguments\"\n    type_list_attr: \"Twindow_size_func_other_arguments\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"key_func\"\n    type: \"func\"\n  }\n  attr {\n    name: \"reduce_func\"\n    type: \"func\"\n  }\n  attr {\n    name: \"window_size_func\"\n    type: \"func\"\n  }\n  attr {\n    name: \"Tkey_func_other_arguments\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"Treduce_func_other_arguments\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"Twindow_size_func_other_arguments\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"GuaranteeConst\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"HSVToRGB\"\n  input_arg {\n    name: \"images\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"HashTable\"\n  output_arg {\n    name: \"table_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"use_node_name_sharing\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"key_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"value_dtype\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"HashTableV2\"\n  output_arg {\n    name: \"table_handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"use_node_name_sharing\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"key_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"value_dtype\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"HistogramFixedWidth\"\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"value_range\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"nbins\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"HistogramSummary\"\n  input_arg {\n    name: \"tag\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"summary\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"IFFT\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n}\nop {\n  name: \"IFFT2D\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n}\nop {\n  name: \"IFFT3D\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n}\nop {\n  name: \"IRFFT\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  input_arg {\n    name: \"fft_length\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_FLOAT\n  }\n}\nop {\n  name: \"IRFFT2D\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  input_arg {\n    name: \"fft_length\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_FLOAT\n  }\n}\nop {\n  name: \"IRFFT3D\"\n  input_arg {\n    name: \"input\"\n    type: DT_COMPLEX64\n  }\n  input_arg {\n    name: \"fft_length\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_FLOAT\n  }\n}\nop {\n  name: \"Identity\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"IdentityN\"\n  input_arg {\n    name: \"input\"\n    type_list_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_list_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"IdentityReader\"\n  output_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use IdentityReaderV2\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"IdentityReaderV2\"\n  output_arg {\n    name: \"reader_handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Igamma\"\n  input_arg {\n    name: \"a\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"Igammac\"\n  input_arg {\n    name: \"a\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"Imag\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"Tout\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_COMPLEX64\n    }\n    allowed_values {\n      list {\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  attr {\n    name: \"Tout\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"ImageSummary\"\n  input_arg {\n    name: \"tag\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"tensor\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"summary\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"max_images\"\n    type: \"int\"\n    default_value {\n      i: 3\n    }\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_FLOAT\n        type: DT_HALF\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"bad_color\"\n    type: \"tensor\"\n    default_value {\n      tensor {\n        dtype: DT_UINT8\n        tensor_shape {\n          dim {\n            size: 4\n          }\n        }\n        int_val: 255\n        int_val: 0\n        int_val: 0\n        int_val: 255\n      }\n    }\n  }\n}\nop {\n  name: \"ImmutableConst\"\n  output_arg {\n    name: \"tensor\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"shape\"\n    type: \"shape\"\n  }\n  attr {\n    name: \"memory_region_name\"\n    type: \"string\"\n  }\n}\nop {\n  name: \"InTopK\"\n  input_arg {\n    name: \"predictions\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"targets\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"precision\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"k\"\n    type: \"int\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"InTopKV2\"\n  input_arg {\n    name: \"predictions\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"targets\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"k\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"precision\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"InitializeTable\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"keys\"\n    type_attr: \"Tkey\"\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"Tval\"\n  }\n  attr {\n    name: \"Tkey\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tval\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"InitializeTableFromTextFile\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"filename\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"key_index\"\n    type: \"int\"\n    has_minimum: true\n    minimum: -2\n  }\n  attr {\n    name: \"value_index\"\n    type: \"int\"\n    has_minimum: true\n    minimum: -2\n  }\n  attr {\n    name: \"vocab_size\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n    has_minimum: true\n    minimum: -1\n  }\n  attr {\n    name: \"delimiter\"\n    type: \"string\"\n    default_value {\n      s: \"\\t\"\n    }\n  }\n}\nop {\n  name: \"InitializeTableFromTextFileV2\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"filename\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"key_index\"\n    type: \"int\"\n    has_minimum: true\n    minimum: -2\n  }\n  attr {\n    name: \"value_index\"\n    type: \"int\"\n    has_minimum: true\n    minimum: -2\n  }\n  attr {\n    name: \"vocab_size\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n    has_minimum: true\n    minimum: -1\n  }\n  attr {\n    name: \"delimiter\"\n    type: \"string\"\n    default_value {\n      s: \"\\t\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"InitializeTableV2\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"keys\"\n    type_attr: \"Tkey\"\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"Tval\"\n  }\n  attr {\n    name: \"Tkey\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tval\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"InterleaveDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"other_arguments\"\n    type_list_attr: \"Targuments\"\n  }\n  input_arg {\n    name: \"cycle_length\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"block_length\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"f\"\n    type: \"func\"\n  }\n  attr {\n    name: \"Targuments\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"Inv\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"InvGrad\"\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"dy\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Invert\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"InvertPermutation\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"IsFinite\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"IsInf\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"IsNan\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"IsVariableInitialized\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"dtype\"\n    is_ref: true\n  }\n  output_arg {\n    name: \"is_initialized\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  allows_uninitialized_input: true\n}\nop {\n  name: \"Iterator\"\n  output_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"IteratorFromStringHandle\"\n  input_arg {\n    name: \"string_handle\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"resource_handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n  is_stateful: true\n}\nop {\n  name: \"IteratorGetNext\"\n  input_arg {\n    name: \"iterator\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"components\"\n    type_list_attr: \"output_types\"\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"IteratorGetNextSync\"\n  input_arg {\n    name: \"iterator\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"components\"\n    type_list_attr: \"output_types\"\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"IteratorSetStatsAggregator\"\n  input_arg {\n    name: \"iterator_handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"stats_aggregator_handle\"\n    type: DT_RESOURCE\n  }\n  is_stateful: true\n}\nop {\n  name: \"IteratorToStringHandle\"\n  input_arg {\n    name: \"resource_handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"string_handle\"\n    type: DT_STRING\n  }\n  is_stateful: true\n}\nop {\n  name: \"L2Loss\"\n  input_arg {\n    name: \"t\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"LMDBReader\"\n  output_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"LRN\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"depth_radius\"\n    type: \"int\"\n    default_value {\n      i: 5\n    }\n  }\n  attr {\n    name: \"bias\"\n    type: \"float\"\n    default_value {\n      f: 1\n    }\n  }\n  attr {\n    name: \"alpha\"\n    type: \"float\"\n    default_value {\n      f: 1\n    }\n  }\n  attr {\n    name: \"beta\"\n    type: \"float\"\n    default_value {\n      f: 0.5\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n      }\n    }\n  }\n}\nop {\n  name: \"LRNGrad\"\n  input_arg {\n    name: \"input_grads\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"input_image\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"output_image\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"depth_radius\"\n    type: \"int\"\n    default_value {\n      i: 5\n    }\n  }\n  attr {\n    name: \"bias\"\n    type: \"float\"\n    default_value {\n      f: 1\n    }\n  }\n  attr {\n    name: \"alpha\"\n    type: \"float\"\n    default_value {\n      f: 1\n    }\n  }\n  attr {\n    name: \"beta\"\n    type: \"float\"\n    default_value {\n      f: 0.5\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n      }\n    }\n  }\n}\nop {\n  name: \"LatencyStatsDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"tag\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"LearnedUnigramCandidateSampler\"\n  input_arg {\n    name: \"true_classes\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sampled_candidates\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"true_expected_count\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"sampled_expected_count\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"num_true\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"num_sampled\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"unique\"\n    type: \"bool\"\n  }\n  attr {\n    name: \"range_max\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"LeftShift\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"Less\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"LessEqual\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"Lgamma\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"LinSpace\"\n  input_arg {\n    name: \"start\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"stop\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"num\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"ListDiff\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"idx\"\n    type_attr: \"out_idx\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"out_idx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"LoadAndRemapMatrix\"\n  input_arg {\n    name: \"ckpt_path\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"old_tensor_name\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"row_remapping\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"col_remapping\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"initializing_values\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output_matrix\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"num_rows\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"num_cols\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"max_rows_in_memory\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Log\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Log1p\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"LogMatrixDeterminant\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"sign\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"log_abs_determinant\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"LogSoftmax\"\n  input_arg {\n    name: \"logits\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"logsoftmax\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"LogUniformCandidateSampler\"\n  input_arg {\n    name: \"true_classes\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sampled_candidates\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"true_expected_count\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"sampled_expected_count\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"num_true\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"num_sampled\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"unique\"\n    type: \"bool\"\n  }\n  attr {\n    name: \"range_max\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"LogicalAnd\"\n  input_arg {\n    name: \"x\"\n    type: DT_BOOL\n  }\n  input_arg {\n    name: \"y\"\n    type: DT_BOOL\n  }\n  output_arg {\n    name: \"z\"\n    type: DT_BOOL\n  }\n  is_commutative: true\n}\nop {\n  name: \"LogicalNot\"\n  input_arg {\n    name: \"x\"\n    type: DT_BOOL\n  }\n  output_arg {\n    name: \"y\"\n    type: DT_BOOL\n  }\n}\nop {\n  name: \"LogicalOr\"\n  input_arg {\n    name: \"x\"\n    type: DT_BOOL\n  }\n  input_arg {\n    name: \"y\"\n    type: DT_BOOL\n  }\n  output_arg {\n    name: \"z\"\n    type: DT_BOOL\n  }\n  is_commutative: true\n}\nop {\n  name: \"LookupTableExport\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"keys\"\n    type_attr: \"Tkeys\"\n  }\n  output_arg {\n    name: \"values\"\n    type_attr: \"Tvalues\"\n  }\n  attr {\n    name: \"Tkeys\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tvalues\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"LookupTableExportV2\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"keys\"\n    type_attr: \"Tkeys\"\n  }\n  output_arg {\n    name: \"values\"\n    type_attr: \"Tvalues\"\n  }\n  attr {\n    name: \"Tkeys\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tvalues\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"LookupTableFind\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"keys\"\n    type_attr: \"Tin\"\n  }\n  input_arg {\n    name: \"default_value\"\n    type_attr: \"Tout\"\n  }\n  output_arg {\n    name: \"values\"\n    type_attr: \"Tout\"\n  }\n  attr {\n    name: \"Tin\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tout\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"LookupTableFindV2\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"keys\"\n    type_attr: \"Tin\"\n  }\n  input_arg {\n    name: \"default_value\"\n    type_attr: \"Tout\"\n  }\n  output_arg {\n    name: \"values\"\n    type_attr: \"Tout\"\n  }\n  attr {\n    name: \"Tin\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tout\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"LookupTableImport\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"keys\"\n    type_attr: \"Tin\"\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"Tout\"\n  }\n  attr {\n    name: \"Tin\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tout\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"LookupTableImportV2\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"keys\"\n    type_attr: \"Tin\"\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"Tout\"\n  }\n  attr {\n    name: \"Tin\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tout\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"LookupTableInsert\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"keys\"\n    type_attr: \"Tin\"\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"Tout\"\n  }\n  attr {\n    name: \"Tin\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tout\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"LookupTableInsertV2\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"keys\"\n    type_attr: \"Tin\"\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"Tout\"\n  }\n  attr {\n    name: \"Tin\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tout\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"LookupTableSize\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"size\"\n    type: DT_INT64\n  }\n}\nop {\n  name: \"LookupTableSizeV2\"\n  input_arg {\n    name: \"table_handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"size\"\n    type: DT_INT64\n  }\n  is_stateful: true\n}\nop {\n  name: \"LoopCond\"\n  input_arg {\n    name: \"input\"\n    type: DT_BOOL\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_BOOL\n  }\n}\nop {\n  name: \"MakeIterator\"\n  input_arg {\n    name: \"dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"iterator\"\n    type: DT_RESOURCE\n  }\n  is_stateful: true\n}\nop {\n  name: \"MapAndBatchDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"other_arguments\"\n    type_list_attr: \"Targuments\"\n  }\n  input_arg {\n    name: \"batch_size\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"num_parallel_batches\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"f\"\n    type: \"func\"\n  }\n  attr {\n    name: \"Targuments\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"MapClear\"\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"MapDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"other_arguments\"\n    type_list_attr: \"Targuments\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"f\"\n    type: \"func\"\n  }\n  attr {\n    name: \"Targuments\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"MapIncompleteSize\"\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"MapPeek\"\n  input_arg {\n    name: \"key\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"values\"\n    type_list_attr: \"dtypes\"\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"MapSize\"\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"MapStage\"\n  input_arg {\n    name: \"key\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"values\"\n    type_list_attr: \"fake_dtypes\"\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n  }\n  attr {\n    name: \"fake_dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"MapUnstage\"\n  input_arg {\n    name: \"key\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"values\"\n    type_list_attr: \"dtypes\"\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"MapUnstageNoKey\"\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"key\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"values\"\n    type_list_attr: \"dtypes\"\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"MatMul\"\n  input_arg {\n    name: \"a\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"b\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"product\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"transpose_a\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"transpose_b\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"MatchingFiles\"\n  input_arg {\n    name: \"pattern\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"filenames\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"MatrixBandPart\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"num_lower\"\n    type_attr: \"Tindex\"\n  }\n  input_arg {\n    name: \"num_upper\"\n    type_attr: \"Tindex\"\n  }\n  output_arg {\n    name: \"band\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tindex\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"MatrixDeterminant\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"MatrixDiag\"\n  input_arg {\n    name: \"diagonal\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"MatrixDiagPart\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"diagonal\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"MatrixExponential\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"MatrixInverse\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"adjoint\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"MatrixLogarithm\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"MatrixSetDiag\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"diagonal\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"MatrixSolve\"\n  input_arg {\n    name: \"matrix\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rhs\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"adjoint\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"MatrixSolveLs\"\n  input_arg {\n    name: \"matrix\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rhs\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2_regularizer\"\n    type: DT_DOUBLE\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  attr {\n    name: \"fast\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n}\nop {\n  name: \"MatrixTriangularSolve\"\n  input_arg {\n    name: \"matrix\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rhs\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"lower\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"adjoint\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Max\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"reduction_indices\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"keep_dims\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"MaxPool\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_UINT16\n        type: DT_QINT8\n      }\n    }\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n        s: \"NCHW_VECT_C\"\n      }\n    }\n  }\n}\nop {\n  name: \"MaxPool3D\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NDHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NDHWC\"\n        s: \"NCDHW\"\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n      }\n    }\n  }\n}\nop {\n  name: \"MaxPool3DGrad\"\n  input_arg {\n    name: \"orig_input\"\n    type_attr: \"TInput\"\n  }\n  input_arg {\n    name: \"orig_output\"\n    type_attr: \"TInput\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NDHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NDHWC\"\n        s: \"NCDHW\"\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"TInput\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n      }\n    }\n  }\n}\nop {\n  name: \"MaxPool3DGradGrad\"\n  input_arg {\n    name: \"orig_input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"orig_output\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 5\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NDHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NDHWC\"\n        s: \"NCDHW\"\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n      }\n    }\n  }\n}\nop {\n  name: \"MaxPoolGrad\"\n  input_arg {\n    name: \"orig_input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"orig_output\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"MaxPoolGradGrad\"\n  input_arg {\n    name: \"orig_input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"orig_output\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"MaxPoolGradGradV2\"\n  input_arg {\n    name: \"orig_input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"orig_output\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"ksize\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"strides\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"MaxPoolGradGradWithArgmax\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"argmax\"\n    type_attr: \"Targmax\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"Targmax\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"MaxPoolGradV2\"\n  input_arg {\n    name: \"orig_input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"orig_output\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"ksize\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"strides\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"MaxPoolGradWithArgmax\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"argmax\"\n    type_attr: \"Targmax\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"Targmax\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"MaxPoolV2\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"ksize\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"strides\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_UINT16\n        type: DT_QINT8\n      }\n    }\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n        s: \"NCHW_VECT_C\"\n      }\n    }\n  }\n}\nop {\n  name: \"MaxPoolWithArgmax\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"argmax\"\n    type_attr: \"Targmax\"\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n    has_minimum: true\n    minimum: 4\n  }\n  attr {\n    name: \"Targmax\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"Maximum\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"Mean\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"reduction_indices\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"keep_dims\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Merge\"\n  input_arg {\n    name: \"inputs\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"value_index\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"MergeSummary\"\n  input_arg {\n    name: \"inputs\"\n    type: DT_STRING\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"summary\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"MergeV2Checkpoints\"\n  input_arg {\n    name: \"checkpoint_prefixes\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"destination_prefix\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"delete_old_dirs\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Mfcc\"\n  input_arg {\n    name: \"spectrogram\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"sample_rate\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"upper_frequency_limit\"\n    type: \"float\"\n    default_value {\n      f: 4000\n    }\n  }\n  attr {\n    name: \"lower_frequency_limit\"\n    type: \"float\"\n    default_value {\n      f: 20\n    }\n  }\n  attr {\n    name: \"filterbank_channel_count\"\n    type: \"int\"\n    default_value {\n      i: 40\n    }\n  }\n  attr {\n    name: \"dct_coefficient_count\"\n    type: \"int\"\n    default_value {\n      i: 13\n    }\n  }\n}\nop {\n  name: \"Min\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"reduction_indices\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"keep_dims\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Minimum\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"MirrorPad\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"paddings\"\n    type_attr: \"Tpaddings\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tpaddings\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"mode\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"REFLECT\"\n        s: \"SYMMETRIC\"\n      }\n    }\n  }\n}\nop {\n  name: \"MirrorPadGrad\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"paddings\"\n    type_attr: \"Tpaddings\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tpaddings\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"mode\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"REFLECT\"\n        s: \"SYMMETRIC\"\n      }\n    }\n  }\n}\nop {\n  name: \"Mod\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"Mul\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_UINT16\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"Multinomial\"\n  input_arg {\n    name: \"logits\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"num_samples\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"output_dtype\"\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"output_dtype\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"MutableDenseHashTable\"\n  input_arg {\n    name: \"empty_key\"\n    type_attr: \"key_dtype\"\n  }\n  output_arg {\n    name: \"table_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"use_node_name_sharing\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"key_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"value_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"value_shape\"\n    type: \"shape\"\n    default_value {\n      shape {\n      }\n    }\n  }\n  attr {\n    name: \"initial_num_buckets\"\n    type: \"int\"\n    default_value {\n      i: 131072\n    }\n  }\n  attr {\n    name: \"max_load_factor\"\n    type: \"float\"\n    default_value {\n      f: 0.8\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"MutableDenseHashTableV2\"\n  input_arg {\n    name: \"empty_key\"\n    type_attr: \"key_dtype\"\n  }\n  output_arg {\n    name: \"table_handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"use_node_name_sharing\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"key_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"value_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"value_shape\"\n    type: \"shape\"\n    default_value {\n      shape {\n      }\n    }\n  }\n  attr {\n    name: \"initial_num_buckets\"\n    type: \"int\"\n    default_value {\n      i: 131072\n    }\n  }\n  attr {\n    name: \"max_load_factor\"\n    type: \"float\"\n    default_value {\n      f: 0.8\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"MutableHashTable\"\n  output_arg {\n    name: \"table_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"use_node_name_sharing\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"key_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"value_dtype\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"MutableHashTableOfTensors\"\n  output_arg {\n    name: \"table_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"use_node_name_sharing\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"key_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"value_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"value_shape\"\n    type: \"shape\"\n    default_value {\n      shape {\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"MutableHashTableOfTensorsV2\"\n  output_arg {\n    name: \"table_handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"use_node_name_sharing\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"key_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"value_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"value_shape\"\n    type: \"shape\"\n    default_value {\n      shape {\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"MutableHashTableV2\"\n  output_arg {\n    name: \"table_handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"use_node_name_sharing\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"key_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"value_dtype\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"MutexLock\"\n  input_arg {\n    name: \"mutex\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"mutex_lock\"\n    type: DT_VARIANT\n  }\n  is_stateful: true\n}\nop {\n  name: \"MutexV2\"\n  output_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Neg\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"NegTrain\"\n  input_arg {\n    name: \"w_in\"\n    type: DT_FLOAT\n    is_ref: true\n  }\n  input_arg {\n    name: \"w_out\"\n    type: DT_FLOAT\n    is_ref: true\n  }\n  input_arg {\n    name: \"examples\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"labels\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"lr\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"vocab_count\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"num_negative_samples\"\n    type: \"int\"\n  }\n  deprecation {\n    version: 19\n    explanation: \"Moving word2vec into tensorflow_models/tutorials and deprecating its ops here as a result\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"NextIteration\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"NoOp\"\n}\nop {\n  name: \"NonMaxSuppression\"\n  input_arg {\n    name: \"boxes\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"scores\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_output_size\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"selected_indices\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"iou_threshold\"\n    type: \"float\"\n    default_value {\n      f: 0.5\n    }\n  }\n}\nop {\n  name: \"NonMaxSuppressionV2\"\n  input_arg {\n    name: \"boxes\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"scores\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_output_size\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"iou_threshold\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"selected_indices\"\n    type: DT_INT32\n  }\n}\nop {\n  name: \"NotEqual\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type: DT_BOOL\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_QUINT8\n        type: DT_QINT8\n        type: DT_QINT32\n        type: DT_STRING\n        type: DT_BOOL\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"NthElement\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"n\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"values\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"reverse\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"OneHot\"\n  input_arg {\n    name: \"indices\"\n    type_attr: \"TI\"\n  }\n  input_arg {\n    name: \"depth\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"on_value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"off_value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"axis\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"TI\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"OneShotIterator\"\n  output_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"dataset_factory\"\n    type: \"func\"\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"OnesLike\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT8\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_UINT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n        type: DT_BOOL\n      }\n    }\n  }\n}\nop {\n  name: \"OrderedMapClear\"\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"OrderedMapIncompleteSize\"\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"OrderedMapPeek\"\n  input_arg {\n    name: \"key\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"values\"\n    type_list_attr: \"dtypes\"\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"OrderedMapSize\"\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"OrderedMapStage\"\n  input_arg {\n    name: \"key\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"values\"\n    type_list_attr: \"fake_dtypes\"\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n  }\n  attr {\n    name: \"fake_dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"OrderedMapUnstage\"\n  input_arg {\n    name: \"key\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"values\"\n    type_list_attr: \"dtypes\"\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"OrderedMapUnstageNoKey\"\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"key\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"values\"\n    type_list_attr: \"dtypes\"\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Pack\"\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"axis\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n}\nop {\n  name: \"Pad\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"paddings\"\n    type_attr: \"Tpaddings\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tpaddings\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"PadV2\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"paddings\"\n    type_attr: \"Tpaddings\"\n  }\n  input_arg {\n    name: \"constant_values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tpaddings\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"PaddedBatchDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"batch_size\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"padded_shapes\"\n    type: DT_INT64\n    number_attr: \"N\"\n  }\n  input_arg {\n    name: \"padding_values\"\n    type_list_attr: \"Toutput_types\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"Toutput_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"PaddingFIFOQueue\"\n  output_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"shapes\"\n    type: \"list(shape)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"PaddingFIFOQueueV2\"\n  output_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"shapes\"\n    type: \"list(shape)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ParallelConcat\"\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"shape\"\n    type: \"shape\"\n  }\n}\nop {\n  name: \"ParallelDynamicStitch\"\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n    number_attr: \"N\"\n  }\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"merged\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"ParallelInterleaveDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"other_arguments\"\n    type_list_attr: \"Targuments\"\n  }\n  input_arg {\n    name: \"cycle_length\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"block_length\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"sloppy\"\n    type: DT_BOOL\n  }\n  input_arg {\n    name: \"buffer_output_elements\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"prefetch_input_elements\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"f\"\n    type: \"func\"\n  }\n  attr {\n    name: \"Targuments\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"ParallelMapDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"other_arguments\"\n    type_list_attr: \"Targuments\"\n  }\n  input_arg {\n    name: \"num_parallel_calls\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"f\"\n    type: \"func\"\n  }\n  attr {\n    name: \"Targuments\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"ParameterizedTruncatedNormal\"\n  input_arg {\n    name: \"shape\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"means\"\n    type_attr: \"dtype\"\n  }\n  input_arg {\n    name: \"stdevs\"\n    type_attr: \"dtype\"\n  }\n  input_arg {\n    name: \"minvals\"\n    type_attr: \"dtype\"\n  }\n  input_arg {\n    name: \"maxvals\"\n    type_attr: \"dtype\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ParseExample\"\n  input_arg {\n    name: \"serialized\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"names\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"sparse_keys\"\n    type: DT_STRING\n    number_attr: \"Nsparse\"\n  }\n  input_arg {\n    name: \"dense_keys\"\n    type: DT_STRING\n    number_attr: \"Ndense\"\n  }\n  input_arg {\n    name: \"dense_defaults\"\n    type_list_attr: \"Tdense\"\n  }\n  output_arg {\n    name: \"sparse_indices\"\n    type: DT_INT64\n    number_attr: \"Nsparse\"\n  }\n  output_arg {\n    name: \"sparse_values\"\n    type_list_attr: \"sparse_types\"\n  }\n  output_arg {\n    name: \"sparse_shapes\"\n    type: DT_INT64\n    number_attr: \"Nsparse\"\n  }\n  output_arg {\n    name: \"dense_values\"\n    type_list_attr: \"Tdense\"\n  }\n  attr {\n    name: \"Nsparse\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"Ndense\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"sparse_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n  attr {\n    name: \"Tdense\"\n    type: \"list(type)\"\n    has_minimum: true\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n  attr {\n    name: \"dense_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n  }\n}\nop {\n  name: \"ParseSingleExample\"\n  input_arg {\n    name: \"serialized\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"dense_defaults\"\n    type_list_attr: \"Tdense\"\n  }\n  output_arg {\n    name: \"sparse_indices\"\n    type: DT_INT64\n    number_attr: \"num_sparse\"\n  }\n  output_arg {\n    name: \"sparse_values\"\n    type_list_attr: \"sparse_types\"\n  }\n  output_arg {\n    name: \"sparse_shapes\"\n    type: DT_INT64\n    number_attr: \"num_sparse\"\n  }\n  output_arg {\n    name: \"dense_values\"\n    type_list_attr: \"Tdense\"\n  }\n  attr {\n    name: \"num_sparse\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"sparse_keys\"\n    type: \"list(string)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"dense_keys\"\n    type: \"list(string)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"sparse_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n  attr {\n    name: \"Tdense\"\n    type: \"list(type)\"\n    has_minimum: true\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n  attr {\n    name: \"dense_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n  }\n}\nop {\n  name: \"ParseSingleSequenceExample\"\n  input_arg {\n    name: \"serialized\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"feature_list_dense_missing_assumed_empty\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"context_sparse_keys\"\n    type: DT_STRING\n    number_attr: \"Ncontext_sparse\"\n  }\n  input_arg {\n    name: \"context_dense_keys\"\n    type: DT_STRING\n    number_attr: \"Ncontext_dense\"\n  }\n  input_arg {\n    name: \"feature_list_sparse_keys\"\n    type: DT_STRING\n    number_attr: \"Nfeature_list_sparse\"\n  }\n  input_arg {\n    name: \"feature_list_dense_keys\"\n    type: DT_STRING\n    number_attr: \"Nfeature_list_dense\"\n  }\n  input_arg {\n    name: \"context_dense_defaults\"\n    type_list_attr: \"Tcontext_dense\"\n  }\n  input_arg {\n    name: \"debug_name\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"context_sparse_indices\"\n    type: DT_INT64\n    number_attr: \"Ncontext_sparse\"\n  }\n  output_arg {\n    name: \"context_sparse_values\"\n    type_list_attr: \"context_sparse_types\"\n  }\n  output_arg {\n    name: \"context_sparse_shapes\"\n    type: DT_INT64\n    number_attr: \"Ncontext_sparse\"\n  }\n  output_arg {\n    name: \"context_dense_values\"\n    type_list_attr: \"Tcontext_dense\"\n  }\n  output_arg {\n    name: \"feature_list_sparse_indices\"\n    type: DT_INT64\n    number_attr: \"Nfeature_list_sparse\"\n  }\n  output_arg {\n    name: \"feature_list_sparse_values\"\n    type_list_attr: \"feature_list_sparse_types\"\n  }\n  output_arg {\n    name: \"feature_list_sparse_shapes\"\n    type: DT_INT64\n    number_attr: \"Nfeature_list_sparse\"\n  }\n  output_arg {\n    name: \"feature_list_dense_values\"\n    type_list_attr: \"feature_list_dense_types\"\n  }\n  attr {\n    name: \"Ncontext_sparse\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"Ncontext_dense\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"Nfeature_list_sparse\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"Nfeature_list_dense\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"context_sparse_types\"\n    type: \"list(type)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n  attr {\n    name: \"Tcontext_dense\"\n    type: \"list(type)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n  attr {\n    name: \"feature_list_dense_types\"\n    type: \"list(type)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n  attr {\n    name: \"context_dense_shapes\"\n    type: \"list(shape)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"feature_list_sparse_types\"\n    type: \"list(type)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n  attr {\n    name: \"feature_list_dense_shapes\"\n    type: \"list(shape)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n}\nop {\n  name: \"ParseTensor\"\n  input_arg {\n    name: \"serialized\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"out_type\"\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"Placeholder\"\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"shape\"\n    type: \"shape\"\n    default_value {\n      shape {\n        unknown_rank: true\n      }\n    }\n  }\n}\nop {\n  name: \"PlaceholderV2\"\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"shape\"\n    type: \"shape\"\n  }\n  deprecation {\n    version: 23\n    explanation: \"Placeholder now behaves the same as PlaceholderV2.\"\n  }\n}\nop {\n  name: \"PlaceholderWithDefault\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"dtype\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"shape\"\n    type: \"shape\"\n  }\n}\nop {\n  name: \"Polygamma\"\n  input_arg {\n    name: \"a\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"PopulationCount\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type: DT_UINT8\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"Pow\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"PrefetchDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"buffer_size\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"PrependFromQueueAndPaddedBatchDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"batch_size\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"padded_shapes\"\n    type: DT_INT64\n    number_attr: \"N\"\n  }\n  input_arg {\n    name: \"padding_values\"\n    type_list_attr: \"Toutput_types\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"Toutput_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"PreventGradient\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"message\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n}\nop {\n  name: \"Print\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"data\"\n    type_list_attr: \"U\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"U\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"message\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"first_n\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"summarize\"\n    type: \"int\"\n    default_value {\n      i: 3\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"PriorityQueue\"\n  output_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"PriorityQueueV2\"\n  output_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Prod\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"reduction_indices\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"keep_dims\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"PyFunc\"\n  input_arg {\n    name: \"input\"\n    type_list_attr: \"Tin\"\n  }\n  output_arg {\n    name: \"output\"\n    type_list_attr: \"Tout\"\n  }\n  attr {\n    name: \"token\"\n    type: \"string\"\n  }\n  attr {\n    name: \"Tin\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"Tout\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  is_stateful: true\n}\nop {\n  name: \"PyFuncStateless\"\n  input_arg {\n    name: \"input\"\n    type_list_attr: \"Tin\"\n  }\n  output_arg {\n    name: \"output\"\n    type_list_attr: \"Tout\"\n  }\n  attr {\n    name: \"token\"\n    type: \"string\"\n  }\n  attr {\n    name: \"Tin\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"Tout\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n}\nop {\n  name: \"Qr\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"q\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"r\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"full_matrices\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizeAndDequantize\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"signed_input\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"num_bits\"\n    type: \"int\"\n    default_value {\n      i: 8\n    }\n  }\n  attr {\n    name: \"range_given\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"input_min\"\n    type: \"float\"\n    default_value {\n      f: 0\n    }\n  }\n  attr {\n    name: \"input_max\"\n    type: \"float\"\n    default_value {\n      f: 0\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  deprecation {\n    version: 22\n    explanation: \"Replaced by QuantizeAndDequantizeV2\"\n  }\n}\nop {\n  name: \"QuantizeAndDequantizeV2\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"input_min\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"input_max\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"signed_input\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"num_bits\"\n    type: \"int\"\n    default_value {\n      i: 8\n    }\n  }\n  attr {\n    name: \"range_given\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizeAndDequantizeV3\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"input_min\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"input_max\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"num_bits\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"signed_input\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"range_given\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizeDownAndShrinkRange\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"Tinput\"\n  }\n  input_arg {\n    name: \"input_min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"input_max\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"out_type\"\n  }\n  output_arg {\n    name: \"output_min\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output_max\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"Tinput\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizeV2\"\n  input_arg {\n    name: \"input\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min_range\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_range\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_min\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output_max\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"mode\"\n    type: \"string\"\n    default_value {\n      s: \"MIN_COMBINED\"\n    }\n    allowed_values {\n      list {\n        s: \"MIN_COMBINED\"\n        s: \"MIN_FIRST\"\n        s: \"SCALED\"\n      }\n    }\n  }\n  attr {\n    name: \"round_mode\"\n    type: \"string\"\n    default_value {\n      s: \"HALF_AWAY_FROM_ZERO\"\n    }\n    allowed_values {\n      list {\n        s: \"HALF_AWAY_FROM_ZERO\"\n        s: \"HALF_TO_EVEN\"\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizedAdd\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T1\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T2\"\n  }\n  input_arg {\n    name: \"min_x\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_x\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min_y\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_y\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"Toutput\"\n  }\n  output_arg {\n    name: \"min_z\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"max_z\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T1\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"T2\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"Toutput\"\n    type: \"type\"\n    default_value {\n      type: DT_QINT32\n    }\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"QuantizedAvgPool\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"min_input\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_input\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"min_output\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"max_output\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizedBatchNormWithGlobalNormalization\"\n  input_arg {\n    name: \"t\"\n    type_attr: \"Tinput\"\n  }\n  input_arg {\n    name: \"t_min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"t_max\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"m\"\n    type_attr: \"Tinput\"\n  }\n  input_arg {\n    name: \"m_min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"m_max\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"v\"\n    type_attr: \"Tinput\"\n  }\n  input_arg {\n    name: \"v_min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"v_max\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"beta\"\n    type_attr: \"Tinput\"\n  }\n  input_arg {\n    name: \"beta_min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"beta_max\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"gamma\"\n    type_attr: \"Tinput\"\n  }\n  input_arg {\n    name: \"gamma_min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"gamma_max\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"result\"\n    type_attr: \"out_type\"\n  }\n  output_arg {\n    name: \"result_min\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"result_max\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"Tinput\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"variance_epsilon\"\n    type: \"float\"\n  }\n  attr {\n    name: \"scale_after_normalization\"\n    type: \"bool\"\n  }\n}\nop {\n  name: \"QuantizedBiasAdd\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T1\"\n  }\n  input_arg {\n    name: \"bias\"\n    type_attr: \"T2\"\n  }\n  input_arg {\n    name: \"min_input\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_input\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min_bias\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_bias\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"out_type\"\n  }\n  output_arg {\n    name: \"min_out\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"max_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T1\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"T2\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizedConcat\"\n  input_arg {\n    name: \"concat_dim\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n  }\n  input_arg {\n    name: \"input_mins\"\n    type: DT_FLOAT\n    number_attr: \"N\"\n  }\n  input_arg {\n    name: \"input_maxes\"\n    type: DT_FLOAT\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_min\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output_max\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 2\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"QuantizedConv2D\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"Tinput\"\n  }\n  input_arg {\n    name: \"filter\"\n    type_attr: \"Tfilter\"\n  }\n  input_arg {\n    name: \"min_input\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_input\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min_filter\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_filter\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"out_type\"\n  }\n  output_arg {\n    name: \"min_output\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"max_output\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"Tinput\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"Tfilter\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    default_value {\n      type: DT_QINT32\n    }\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n  attr {\n    name: \"dilations\"\n    type: \"list(int)\"\n    default_value {\n      list {\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizedInstanceNorm\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"x_min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"x_max\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y_min\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"y_max\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"output_range_given\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"given_y_min\"\n    type: \"float\"\n    default_value {\n      f: 0\n    }\n  }\n  attr {\n    name: \"given_y_max\"\n    type: \"float\"\n    default_value {\n      f: 0\n    }\n  }\n  attr {\n    name: \"variance_epsilon\"\n    type: \"float\"\n    default_value {\n      f: 1e-05\n    }\n  }\n  attr {\n    name: \"min_separation\"\n    type: \"float\"\n    default_value {\n      f: 0.001\n    }\n  }\n}\nop {\n  name: \"QuantizedMatMul\"\n  input_arg {\n    name: \"a\"\n    type_attr: \"T1\"\n  }\n  input_arg {\n    name: \"b\"\n    type_attr: \"T2\"\n  }\n  input_arg {\n    name: \"min_a\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_a\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min_b\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_b\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"Toutput\"\n  }\n  output_arg {\n    name: \"min_out\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"max_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T1\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"T2\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"Toutput\"\n    type: \"type\"\n    default_value {\n      type: DT_QINT32\n    }\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"transpose_a\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"transpose_b\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"Tactivation\"\n    type: \"type\"\n    default_value {\n      type: DT_QUINT8\n    }\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizedMaxPool\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"min_input\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_input\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"min_output\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"max_output\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"ksize\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"strides\"\n    type: \"list(int)\"\n  }\n  attr {\n    name: \"padding\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"SAME\"\n        s: \"VALID\"\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizedMul\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T1\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T2\"\n  }\n  input_arg {\n    name: \"min_x\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_x\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min_y\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_y\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"Toutput\"\n  }\n  output_arg {\n    name: \"min_z\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"max_z\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T1\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"T2\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"Toutput\"\n    type: \"type\"\n    default_value {\n      type: DT_QINT32\n    }\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"QuantizedRelu\"\n  input_arg {\n    name: \"features\"\n    type_attr: \"Tinput\"\n  }\n  input_arg {\n    name: \"min_features\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_features\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"activations\"\n    type_attr: \"out_type\"\n  }\n  output_arg {\n    name: \"min_activations\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"max_activations\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"Tinput\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    default_value {\n      type: DT_QUINT8\n    }\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizedRelu6\"\n  input_arg {\n    name: \"features\"\n    type_attr: \"Tinput\"\n  }\n  input_arg {\n    name: \"min_features\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_features\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"activations\"\n    type_attr: \"out_type\"\n  }\n  output_arg {\n    name: \"min_activations\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"max_activations\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"Tinput\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    default_value {\n      type: DT_QUINT8\n    }\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizedReluX\"\n  input_arg {\n    name: \"features\"\n    type_attr: \"Tinput\"\n  }\n  input_arg {\n    name: \"max_value\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min_features\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max_features\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"activations\"\n    type_attr: \"out_type\"\n  }\n  output_arg {\n    name: \"min_activations\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"max_activations\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"Tinput\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    default_value {\n      type: DT_QUINT8\n    }\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizedReshape\"\n  input_arg {\n    name: \"tensor\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"shape\"\n    type_attr: \"Tshape\"\n  }\n  input_arg {\n    name: \"input_min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"input_max\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_min\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output_max\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tshape\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"QuantizedResizeBilinear\"\n  input_arg {\n    name: \"images\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"max\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"resized_images\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out_min\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"out_max\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_FLOAT\n      }\n    }\n  }\n  attr {\n    name: \"align_corners\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"QueueClose\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"cancel_pending_enqueues\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"QueueCloseV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"cancel_pending_enqueues\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"QueueDequeue\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"components\"\n    type_list_attr: \"component_types\"\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"timeout_ms\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n}\nop {\n  name: \"QueueDequeueMany\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"n\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"components\"\n    type_list_attr: \"component_types\"\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"timeout_ms\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n}\nop {\n  name: \"QueueDequeueManyV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"n\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"components\"\n    type_list_attr: \"component_types\"\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"timeout_ms\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"QueueDequeueUpTo\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"n\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"components\"\n    type_list_attr: \"component_types\"\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"timeout_ms\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n}\nop {\n  name: \"QueueDequeueUpToV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"n\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"components\"\n    type_list_attr: \"component_types\"\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"timeout_ms\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"QueueDequeueV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"components\"\n    type_list_attr: \"component_types\"\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"timeout_ms\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"QueueEnqueue\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"components\"\n    type_list_attr: \"Tcomponents\"\n  }\n  attr {\n    name: \"Tcomponents\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"timeout_ms\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n}\nop {\n  name: \"QueueEnqueueMany\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"components\"\n    type_list_attr: \"Tcomponents\"\n  }\n  attr {\n    name: \"Tcomponents\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"timeout_ms\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n}\nop {\n  name: \"QueueEnqueueManyV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"components\"\n    type_list_attr: \"Tcomponents\"\n  }\n  attr {\n    name: \"Tcomponents\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"timeout_ms\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"QueueEnqueueV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"components\"\n    type_list_attr: \"Tcomponents\"\n  }\n  attr {\n    name: \"Tcomponents\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"timeout_ms\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"QueueIsClosed\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"is_closed\"\n    type: DT_BOOL\n  }\n}\nop {\n  name: \"QueueIsClosedV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"is_closed\"\n    type: DT_BOOL\n  }\n  is_stateful: true\n}\nop {\n  name: \"QueueSize\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n}\nop {\n  name: \"QueueSizeV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  is_stateful: true\n}\nop {\n  name: \"RFFT\"\n  input_arg {\n    name: \"input\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"fft_length\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n}\nop {\n  name: \"RFFT2D\"\n  input_arg {\n    name: \"input\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"fft_length\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n}\nop {\n  name: \"RFFT3D\"\n  input_arg {\n    name: \"input\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"fft_length\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_COMPLEX64\n  }\n}\nop {\n  name: \"RGBToHSV\"\n  input_arg {\n    name: \"images\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"RandomCrop\"\n  input_arg {\n    name: \"image\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"size\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  deprecation {\n    version: 8\n    explanation: \"Random crop is now pure Python\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"RandomDataset\"\n  input_arg {\n    name: \"seed\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"seed2\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"RandomGamma\"\n  input_arg {\n    name: \"shape\"\n    type_attr: \"S\"\n  }\n  input_arg {\n    name: \"alpha\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"S\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"RandomPoisson\"\n  input_arg {\n    name: \"shape\"\n    type_attr: \"S\"\n  }\n  input_arg {\n    name: \"rate\"\n    type_attr: \"dtype\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"S\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  deprecation {\n    version: 25\n    explanation: \"Replaced by RandomPoissonV2\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"RandomPoissonV2\"\n  input_arg {\n    name: \"shape\"\n    type_attr: \"S\"\n  }\n  input_arg {\n    name: \"rate\"\n    type_attr: \"R\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"S\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"R\"\n    type: \"type\"\n    default_value {\n      type: DT_DOUBLE\n    }\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"RandomShuffle\"\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"RandomShuffleQueue\"\n  output_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"shapes\"\n    type: \"list(shape)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"min_after_dequeue\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"RandomShuffleQueueV2\"\n  output_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"component_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"shapes\"\n    type: \"list(shape)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  attr {\n    name: \"min_after_dequeue\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"RandomStandardNormal\"\n  input_arg {\n    name: \"shape\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"RandomUniform\"\n  input_arg {\n    name: \"shape\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"RandomUniformInt\"\n  input_arg {\n    name: \"shape\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"minval\"\n    type_attr: \"Tout\"\n  }\n  input_arg {\n    name: \"maxval\"\n    type_attr: \"Tout\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"Tout\"\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"Tout\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Range\"\n  input_arg {\n    name: \"start\"\n    type_attr: \"Tidx\"\n  }\n  input_arg {\n    name: \"limit\"\n    type_attr: \"Tidx\"\n  }\n  input_arg {\n    name: \"delta\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"Tidx\"\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"RangeDataset\"\n  input_arg {\n    name: \"start\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"stop\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"step\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"Rank\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"ReadFile\"\n  input_arg {\n    name: \"filename\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"contents\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"ReadVariableOp\"\n  input_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"ReaderNumRecordsProduced\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"records_produced\"\n    type: DT_INT64\n  }\n}\nop {\n  name: \"ReaderNumRecordsProducedV2\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"records_produced\"\n    type: DT_INT64\n  }\n  is_stateful: true\n}\nop {\n  name: \"ReaderNumWorkUnitsCompleted\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"units_completed\"\n    type: DT_INT64\n  }\n}\nop {\n  name: \"ReaderNumWorkUnitsCompletedV2\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"units_completed\"\n    type: DT_INT64\n  }\n  is_stateful: true\n}\nop {\n  name: \"ReaderRead\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"queue_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"key\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"value\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"ReaderReadUpTo\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"queue_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"num_records\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"keys\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"values\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"ReaderReadUpToV2\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"queue_handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"num_records\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"keys\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"values\"\n    type: DT_STRING\n  }\n  is_stateful: true\n}\nop {\n  name: \"ReaderReadV2\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"queue_handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"key\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"value\"\n    type: DT_STRING\n  }\n  is_stateful: true\n}\nop {\n  name: \"ReaderReset\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n}\nop {\n  name: \"ReaderResetV2\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_RESOURCE\n  }\n  is_stateful: true\n}\nop {\n  name: \"ReaderRestoreState\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"state\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"ReaderRestoreStateV2\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"state\"\n    type: DT_STRING\n  }\n  is_stateful: true\n}\nop {\n  name: \"ReaderSerializeState\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"state\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"ReaderSerializeStateV2\"\n  input_arg {\n    name: \"reader_handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"state\"\n    type: DT_STRING\n  }\n  is_stateful: true\n}\nop {\n  name: \"Real\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"Tout\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_COMPLEX64\n    }\n    allowed_values {\n      list {\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  attr {\n    name: \"Tout\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"RealDiv\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_UINT16\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Reciprocal\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"ReciprocalGrad\"\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"dy\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"RecordInput\"\n  output_arg {\n    name: \"records\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"file_pattern\"\n    type: \"string\"\n  }\n  attr {\n    name: \"file_random_seed\"\n    type: \"int\"\n    default_value {\n      i: 301\n    }\n  }\n  attr {\n    name: \"file_shuffle_shift_ratio\"\n    type: \"float\"\n    default_value {\n      f: 0\n    }\n  }\n  attr {\n    name: \"file_buffer_size\"\n    type: \"int\"\n    default_value {\n      i: 10000\n    }\n  }\n  attr {\n    name: \"file_parallelism\"\n    type: \"int\"\n    default_value {\n      i: 16\n    }\n  }\n  attr {\n    name: \"batch_size\"\n    type: \"int\"\n    default_value {\n      i: 32\n    }\n  }\n  attr {\n    name: \"compression_type\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ReduceJoin\"\n  input_arg {\n    name: \"inputs\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"reduction_indices\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"keep_dims\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"separator\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n}\nop {\n  name: \"RefEnter\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"frame_name\"\n    type: \"string\"\n  }\n  attr {\n    name: \"is_constant\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"parallel_iterations\"\n    type: \"int\"\n    default_value {\n      i: 10\n    }\n  }\n}\nop {\n  name: \"RefExit\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"RefIdentity\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  allows_uninitialized_input: true\n}\nop {\n  name: \"RefMerge\"\n  input_arg {\n    name: \"inputs\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n    is_ref: true\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  output_arg {\n    name: \"value_index\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"RefNextIteration\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"RefSelect\"\n  input_arg {\n    name: \"index\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"inputs\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n    is_ref: true\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"RefSwitch\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"pred\"\n    type: DT_BOOL\n  }\n  output_arg {\n    name: \"output_false\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  output_arg {\n    name: \"output_true\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  allows_uninitialized_input: true\n}\nop {\n  name: \"Relu\"\n  input_arg {\n    name: \"features\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"activations\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"Relu6\"\n  input_arg {\n    name: \"features\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"activations\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"Relu6Grad\"\n  input_arg {\n    name: \"gradients\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"features\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"backprops\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"ReluGrad\"\n  input_arg {\n    name: \"gradients\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"features\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"backprops\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"RemoteCall\"\n  input_arg {\n    name: \"target\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"args\"\n    type_list_attr: \"Tin\"\n  }\n  output_arg {\n    name: \"output\"\n    type_list_attr: \"Tout\"\n  }\n  attr {\n    name: \"Tin\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"Tout\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"f\"\n    type: \"func\"\n  }\n}\nop {\n  name: \"RemoteFusedGraphExecute\"\n  input_arg {\n    name: \"inputs\"\n    type_list_attr: \"Tinputs\"\n  }\n  output_arg {\n    name: \"outputs\"\n    type_list_attr: \"Toutputs\"\n  }\n  attr {\n    name: \"Tinputs\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"Toutputs\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"serialized_remote_fused_graph_execute_info\"\n    type: \"string\"\n  }\n}\nop {\n  name: \"RepeatDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"count\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"RequantizationRange\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"Tinput\"\n  }\n  input_arg {\n    name: \"input_min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"input_max\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output_min\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output_max\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"Tinput\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n}\nop {\n  name: \"Requantize\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"Tinput\"\n  }\n  input_arg {\n    name: \"input_min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"input_max\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"requested_output_min\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"requested_output_max\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"out_type\"\n  }\n  output_arg {\n    name: \"output_min\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"output_max\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"Tinput\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_QINT16\n        type: DT_QUINT16\n      }\n    }\n  }\n}\nop {\n  name: \"Reshape\"\n  input_arg {\n    name: \"tensor\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"shape\"\n    type_attr: \"Tshape\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tshape\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"ResizeArea\"\n  input_arg {\n    name: \"images\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"resized_images\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_UINT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"align_corners\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ResizeBicubic\"\n  input_arg {\n    name: \"images\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"resized_images\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_UINT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"align_corners\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ResizeBicubicGrad\"\n  input_arg {\n    name: \"grads\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"original_image\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"align_corners\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ResizeBilinear\"\n  input_arg {\n    name: \"images\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"resized_images\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_UINT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"align_corners\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ResizeBilinearGrad\"\n  input_arg {\n    name: \"grads\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"original_image\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_BFLOAT16\n        type: DT_HALF\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"align_corners\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ResizeNearestNeighbor\"\n  input_arg {\n    name: \"images\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"resized_images\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_UINT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"align_corners\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ResizeNearestNeighborGrad\"\n  input_arg {\n    name: \"grads\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_INT32\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"align_corners\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ResourceApplyAdadelta\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum_update\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rho\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyAdagrad\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyAdagradDA\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"gradient_accumulator\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"gradient_squared_accumulator\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"global_step\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyAdam\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"m\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"v\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"beta1_power\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"beta2_power\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"beta1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"beta2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"use_nesterov\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyAddSign\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"m\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"alpha\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"sign_decay\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"beta\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyCenteredRMSProp\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"mg\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"ms\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"mom\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rho\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"momentum\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyFtrl\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"linear\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr_power\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyFtrlV2\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"linear\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2_shrinkage\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr_power\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyGradientDescent\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"alpha\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"delta\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyMomentum\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"momentum\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"use_nesterov\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyPowerSign\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"m\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"logbase\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"sign_decay\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"beta\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyProximalAdagrad\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyProximalGradientDescent\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"alpha\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"delta\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceApplyRMSProp\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"ms\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"mom\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rho\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"momentum\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceCountUpTo\"\n  input_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"limit\"\n    type: \"int\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceGather\"\n  input_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"validate_indices\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceScatterAdd\"\n  input_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceScatterNdUpdate\"\n  input_arg {\n    name: \"ref\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceScatterUpdate\"\n  input_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceSparseApplyAdadelta\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum_update\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rho\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceSparseApplyAdagrad\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceSparseApplyAdagradDA\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"gradient_accumulator\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"gradient_squared_accumulator\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"global_step\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceSparseApplyCenteredRMSProp\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"mg\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"ms\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"mom\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rho\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"momentum\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceSparseApplyFtrl\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"linear\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr_power\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceSparseApplyFtrlV2\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"linear\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2_shrinkage\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr_power\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceSparseApplyMomentum\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"momentum\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"use_nesterov\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceSparseApplyProximalAdagrad\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"accum\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceSparseApplyProximalGradientDescent\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"alpha\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceSparseApplyRMSProp\"\n  input_arg {\n    name: \"var\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"ms\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"mom\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rho\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"momentum\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ResourceStridedSliceAssign\"\n  input_arg {\n    name: \"ref\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"begin\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"end\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"strides\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Index\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"begin_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"end_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"ellipsis_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"new_axis_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"shrink_axis_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Restore\"\n  input_arg {\n    name: \"file_pattern\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"tensor_name\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"tensor\"\n    type_attr: \"dt\"\n  }\n  attr {\n    name: \"dt\"\n    type: \"type\"\n  }\n  attr {\n    name: \"preferred_shard\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"RestoreSlice\"\n  input_arg {\n    name: \"file_pattern\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"tensor_name\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"shape_and_slice\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"tensor\"\n    type_attr: \"dt\"\n  }\n  attr {\n    name: \"dt\"\n    type: \"type\"\n  }\n  attr {\n    name: \"preferred_shard\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"RestoreV2\"\n  input_arg {\n    name: \"prefix\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"tensor_names\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"shape_and_slices\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"tensors\"\n    type_list_attr: \"dtypes\"\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"Reverse\"\n  input_arg {\n    name: \"tensor\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"dims\"\n    type: DT_BOOL\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_UINT16\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_BOOL\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n        type: DT_STRING\n      }\n    }\n  }\n}\nop {\n  name: \"ReverseSequence\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"seq_lengths\"\n    type_attr: \"Tlen\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"seq_dim\"\n    type: \"int\"\n  }\n  attr {\n    name: \"batch_dim\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tlen\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"ReverseV2\"\n  input_arg {\n    name: \"tensor\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"axis\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_UINT16\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_BOOL\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n        type: DT_STRING\n      }\n    }\n  }\n}\nop {\n  name: \"RightShift\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"Rint\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"Roll\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"shift\"\n    type_attr: \"Tshift\"\n  }\n  input_arg {\n    name: \"axis\"\n    type_attr: \"Taxis\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tshift\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"Taxis\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Round\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Rsqrt\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"RsqrtGrad\"\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"dy\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"SampleDistortedBoundingBox\"\n  input_arg {\n    name: \"image_size\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"bounding_boxes\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"begin\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"size\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"bboxes\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"min_object_covered\"\n    type: \"float\"\n    default_value {\n      f: 0.1\n    }\n  }\n  attr {\n    name: \"aspect_ratio_range\"\n    type: \"list(float)\"\n    default_value {\n      list {\n        f: 0.75\n        f: 1.33\n      }\n    }\n  }\n  attr {\n    name: \"area_range\"\n    type: \"list(float)\"\n    default_value {\n      list {\n        f: 0.05\n        f: 1\n      }\n    }\n  }\n  attr {\n    name: \"max_attempts\"\n    type: \"int\"\n    default_value {\n      i: 100\n    }\n  }\n  attr {\n    name: \"use_image_if_no_bounding_boxes\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"SampleDistortedBoundingBoxV2\"\n  input_arg {\n    name: \"image_size\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"bounding_boxes\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"min_object_covered\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"begin\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"size\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"bboxes\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"aspect_ratio_range\"\n    type: \"list(float)\"\n    default_value {\n      list {\n        f: 0.75\n        f: 1.33\n      }\n    }\n  }\n  attr {\n    name: \"area_range\"\n    type: \"list(float)\"\n    default_value {\n      list {\n        f: 0.05\n        f: 1\n      }\n    }\n  }\n  attr {\n    name: \"max_attempts\"\n    type: \"int\"\n    default_value {\n      i: 100\n    }\n  }\n  attr {\n    name: \"use_image_if_no_bounding_boxes\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Save\"\n  input_arg {\n    name: \"filename\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"tensor_names\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"data\"\n    type_list_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"SaveSlices\"\n  input_arg {\n    name: \"filename\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"tensor_names\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"shapes_and_slices\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"data\"\n    type_list_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"SaveV2\"\n  input_arg {\n    name: \"prefix\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"tensor_names\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"shape_and_slices\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"tensors\"\n    type_list_attr: \"dtypes\"\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"ScalarSummary\"\n  input_arg {\n    name: \"tags\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"summary\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"ScanDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"initial_state\"\n    type_list_attr: \"Tstate\"\n  }\n  input_arg {\n    name: \"other_arguments\"\n    type_list_attr: \"Targuments\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"f\"\n    type: \"func\"\n  }\n  attr {\n    name: \"Tstate\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"Targuments\"\n    type: \"list(type)\"\n    has_minimum: true\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"ScatterAdd\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ScatterDiv\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ScatterMul\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ScatterNd\"\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"shape\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"ScatterNdAdd\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ScatterNdNonAliasingAdd\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"ScatterNdSub\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ScatterNdUpdate\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n}\nop {\n  name: \"ScatterSub\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"ScatterUpdate\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"updates\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n}\nop {\n  name: \"SdcaFprint\"\n  input_arg {\n    name: \"input\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_INT64\n  }\n}\nop {\n  name: \"SdcaOptimizer\"\n  input_arg {\n    name: \"sparse_example_indices\"\n    type: DT_INT64\n    number_attr: \"num_sparse_features\"\n  }\n  input_arg {\n    name: \"sparse_feature_indices\"\n    type: DT_INT64\n    number_attr: \"num_sparse_features\"\n  }\n  input_arg {\n    name: \"sparse_feature_values\"\n    type: DT_FLOAT\n    number_attr: \"num_sparse_features_with_values\"\n  }\n  input_arg {\n    name: \"dense_features\"\n    type: DT_FLOAT\n    number_attr: \"num_dense_features\"\n  }\n  input_arg {\n    name: \"example_weights\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"example_labels\"\n    type: DT_FLOAT\n  }\n  input_arg {\n    name: \"sparse_indices\"\n    type: DT_INT64\n    number_attr: \"num_sparse_features\"\n  }\n  input_arg {\n    name: \"sparse_weights\"\n    type: DT_FLOAT\n    number_attr: \"num_sparse_features\"\n  }\n  input_arg {\n    name: \"dense_weights\"\n    type: DT_FLOAT\n    number_attr: \"num_dense_features\"\n  }\n  input_arg {\n    name: \"example_state_data\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"out_example_state_data\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"out_delta_sparse_weights\"\n    type: DT_FLOAT\n    number_attr: \"num_sparse_features\"\n  }\n  output_arg {\n    name: \"out_delta_dense_weights\"\n    type: DT_FLOAT\n    number_attr: \"num_dense_features\"\n  }\n  attr {\n    name: \"loss_type\"\n    type: \"string\"\n    allowed_values {\n      list {\n        s: \"logistic_loss\"\n        s: \"squared_loss\"\n        s: \"hinge_loss\"\n        s: \"smooth_hinge_loss\"\n      }\n    }\n  }\n  attr {\n    name: \"adaptative\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"num_sparse_features\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"num_sparse_features_with_values\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"num_dense_features\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"l1\"\n    type: \"float\"\n  }\n  attr {\n    name: \"l2\"\n    type: \"float\"\n  }\n  attr {\n    name: \"num_loss_partitions\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"num_inner_iterations\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"SdcaShrinkL1\"\n  input_arg {\n    name: \"weights\"\n    type: DT_FLOAT\n    number_attr: \"num_features\"\n    is_ref: true\n  }\n  attr {\n    name: \"num_features\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"l1\"\n    type: \"float\"\n  }\n  attr {\n    name: \"l2\"\n    type: \"float\"\n  }\n}\nop {\n  name: \"SegmentMax\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SegmentMean\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SegmentMin\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SegmentProd\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SegmentSum\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Select\"\n  input_arg {\n    name: \"condition\"\n    type: DT_BOOL\n  }\n  input_arg {\n    name: \"t\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"e\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"SelfAdjointEig\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n      }\n    }\n  }\n  deprecation {\n    version: 11\n    explanation: \"Use SelfAdjointEigV2 instead.\"\n  }\n}\nop {\n  name: \"SelfAdjointEigV2\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"e\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"v\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"compute_v\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Selu\"\n  input_arg {\n    name: \"features\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"activations\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"SeluGrad\"\n  input_arg {\n    name: \"gradients\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"outputs\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"backprops\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"SerializeIterator\"\n  input_arg {\n    name: \"resource_handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"serialized\"\n    type: DT_VARIANT\n  }\n  is_stateful: true\n}\nop {\n  name: \"SerializeManySparse\"\n  input_arg {\n    name: \"sparse_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"sparse_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"sparse_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"serialized_sparse\"\n    type_attr: \"out_type\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    default_value {\n      type: DT_STRING\n    }\n    allowed_values {\n      list {\n        type: DT_STRING\n        type: DT_VARIANT\n      }\n    }\n  }\n}\nop {\n  name: \"SerializeSparse\"\n  input_arg {\n    name: \"sparse_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"sparse_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"sparse_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"serialized_sparse\"\n    type_attr: \"out_type\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    default_value {\n      type: DT_STRING\n    }\n    allowed_values {\n      list {\n        type: DT_STRING\n        type: DT_VARIANT\n      }\n    }\n  }\n}\nop {\n  name: \"SerializeTensor\"\n  input_arg {\n    name: \"tensor\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"serialized\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"SetSize\"\n  input_arg {\n    name: \"set_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"set_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"set_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"validate_indices\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_STRING\n      }\n    }\n  }\n}\nop {\n  name: \"Shape\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"out_type\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"ShapeN\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"out_type\"\n    number_attr: \"N\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"ShardedFilename\"\n  input_arg {\n    name: \"basename\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"shard\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"num_shards\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"filename\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"ShardedFilespec\"\n  input_arg {\n    name: \"basename\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"num_shards\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"filename\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"ShuffleAndRepeatDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"buffer_size\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"seed\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"seed2\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"count\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"ShuffleDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"buffer_size\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"seed\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"seed2\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"reshuffle_each_iteration\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"Sigmoid\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"SigmoidGrad\"\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"dy\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Sign\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Sin\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Sinh\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Size\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"out_type\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SkipDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"count\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"Skipgram\"\n  output_arg {\n    name: \"vocab_word\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"vocab_freq\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"words_per_epoch\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"current_epoch\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"total_words_processed\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"examples\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"labels\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"filename\"\n    type: \"string\"\n  }\n  attr {\n    name: \"batch_size\"\n    type: \"int\"\n  }\n  attr {\n    name: \"window_size\"\n    type: \"int\"\n    default_value {\n      i: 5\n    }\n  }\n  attr {\n    name: \"min_count\"\n    type: \"int\"\n    default_value {\n      i: 5\n    }\n  }\n  attr {\n    name: \"subsample\"\n    type: \"float\"\n    default_value {\n      f: 0.001\n    }\n  }\n  deprecation {\n    version: 19\n    explanation: \"Moving word2vec into tensorflow_models/tutorials and deprecating its ops here as a result\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"Slice\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"begin\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"size\"\n    type_attr: \"Index\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Index\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Snapshot\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"Softmax\"\n  input_arg {\n    name: \"logits\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"softmax\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"SoftmaxCrossEntropyWithLogits\"\n  input_arg {\n    name: \"features\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"labels\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"loss\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"backprop\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"Softplus\"\n  input_arg {\n    name: \"features\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"activations\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SoftplusGrad\"\n  input_arg {\n    name: \"gradients\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"features\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"backprops\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"Softsign\"\n  input_arg {\n    name: \"features\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"activations\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SoftsignGrad\"\n  input_arg {\n    name: \"gradients\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"features\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"backprops\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SpaceToBatch\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"paddings\"\n    type_attr: \"Tpaddings\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tpaddings\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"block_size\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 2\n  }\n}\nop {\n  name: \"SpaceToBatchND\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"block_shape\"\n    type_attr: \"Tblock_shape\"\n  }\n  input_arg {\n    name: \"paddings\"\n    type_attr: \"Tpaddings\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tblock_shape\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"Tpaddings\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SpaceToDepth\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"block_size\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 2\n  }\n  attr {\n    name: \"data_format\"\n    type: \"string\"\n    default_value {\n      s: \"NHWC\"\n    }\n    allowed_values {\n      list {\n        s: \"NHWC\"\n        s: \"NCHW\"\n        s: \"NCHW_VECT_C\"\n      }\n    }\n  }\n}\nop {\n  name: \"SparseAccumulatorApplyGradient\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"local_step\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"gradient_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"gradient_values\"\n    type_attr: \"dtype\"\n  }\n  input_arg {\n    name: \"gradient_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"has_known_shape\"\n    type: \"bool\"\n  }\n}\nop {\n  name: \"SparseAccumulatorTakeGradient\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"num_required\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"values\"\n    type_attr: \"dtype\"\n  }\n  output_arg {\n    name: \"shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseAdd\"\n  input_arg {\n    name: \"a_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"a_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"a_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"b_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"b_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"b_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"thresh\"\n    type_attr: \"Treal\"\n  }\n  output_arg {\n    name: \"sum_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sum_values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"sum_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Treal\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseAddGrad\"\n  input_arg {\n    name: \"backprop_val_grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"a_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"b_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"sum_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"a_val_grad\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"b_val_grad\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseApplyAdadelta\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum_update\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rho\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"SparseApplyAdagrad\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"SparseApplyAdagradDA\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"gradient_accumulator\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"gradient_squared_accumulator\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"global_step\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"SparseApplyCenteredRMSProp\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"mg\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"ms\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"mom\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rho\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"momentum\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"SparseApplyFtrl\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"linear\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr_power\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"SparseApplyFtrlV2\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"linear\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2_shrinkage\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lr_power\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"SparseApplyMomentum\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"momentum\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"use_nesterov\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"SparseApplyProximalAdagrad\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"accum\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"SparseApplyProximalGradientDescent\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"alpha\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l1\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"l2\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"SparseApplyRMSProp\"\n  input_arg {\n    name: \"var\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"ms\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"mom\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"lr\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"rho\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"momentum\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"epsilon\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tindices\"\n  }\n  output_arg {\n    name: \"out\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"use_locking\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"SparseConcat\"\n  input_arg {\n    name: \"indices\"\n    type: DT_INT64\n    number_attr: \"N\"\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n    number_attr: \"N\"\n  }\n  input_arg {\n    name: \"shapes\"\n    type: DT_INT64\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"output_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"concat_dim\"\n    type: \"int\"\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 2\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"SparseConditionalAccumulator\"\n  output_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"shape\"\n    type: \"shape\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"SparseCross\"\n  input_arg {\n    name: \"indices\"\n    type: DT_INT64\n    number_attr: \"N\"\n  }\n  input_arg {\n    name: \"values\"\n    type_list_attr: \"sparse_types\"\n  }\n  input_arg {\n    name: \"shapes\"\n    type: DT_INT64\n    number_attr: \"N\"\n  }\n  input_arg {\n    name: \"dense_inputs\"\n    type_list_attr: \"dense_types\"\n  }\n  output_arg {\n    name: \"output_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_values\"\n    type_attr: \"out_type\"\n  }\n  output_arg {\n    name: \"output_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"hashed_output\"\n    type: \"bool\"\n  }\n  attr {\n    name: \"num_buckets\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"hash_key\"\n    type: \"int\"\n  }\n  attr {\n    name: \"sparse_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    allowed_values {\n      list {\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n  attr {\n    name: \"dense_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    allowed_values {\n      list {\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n  attr {\n    name: \"internal_type\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT64\n        type: DT_STRING\n      }\n    }\n  }\n}\nop {\n  name: \"SparseDenseCwiseAdd\"\n  input_arg {\n    name: \"sp_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"sp_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"sp_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"dense\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseDenseCwiseDiv\"\n  input_arg {\n    name: \"sp_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"sp_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"sp_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"dense\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseDenseCwiseMul\"\n  input_arg {\n    name: \"sp_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"sp_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"sp_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"dense\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseFillEmptyRows\"\n  input_arg {\n    name: \"indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"dense_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"default_value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"empty_row_indicator\"\n    type: DT_BOOL\n  }\n  output_arg {\n    name: \"reverse_index_map\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"SparseFillEmptyRowsGrad\"\n  input_arg {\n    name: \"reverse_index_map\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"grad_values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"d_values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"d_default_value\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"SparseMatMul\"\n  input_arg {\n    name: \"a\"\n    type_attr: \"Ta\"\n  }\n  input_arg {\n    name: \"b\"\n    type_attr: \"Tb\"\n  }\n  output_arg {\n    name: \"product\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"transpose_a\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"transpose_b\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"a_is_sparse\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"b_is_sparse\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"Ta\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_BFLOAT16\n      }\n    }\n  }\n  attr {\n    name: \"Tb\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_BFLOAT16\n      }\n    }\n  }\n}\nop {\n  name: \"SparseReduceMax\"\n  input_arg {\n    name: \"input_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"input_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"input_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"reduction_axes\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"keep_dims\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseReduceMaxSparse\"\n  input_arg {\n    name: \"input_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"input_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"input_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"reduction_axes\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"keep_dims\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseReduceSum\"\n  input_arg {\n    name: \"input_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"input_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"input_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"reduction_axes\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"keep_dims\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseReduceSumSparse\"\n  input_arg {\n    name: \"input_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"input_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"input_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"reduction_axes\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"keep_dims\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseReorder\"\n  input_arg {\n    name: \"input_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"input_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"input_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_values\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"SparseReshape\"\n  input_arg {\n    name: \"input_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"input_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"new_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_shape\"\n    type: DT_INT64\n  }\n}\nop {\n  name: \"SparseSegmentMean\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tidx\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseSegmentMeanGrad\"\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tidx\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"output_dim0\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseSegmentMeanWithNumSegments\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tidx\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"num_segments\"\n    type_attr: \"Tnumsegments\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"Tnumsegments\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseSegmentSqrtN\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tidx\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseSegmentSqrtNGrad\"\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tidx\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"output_dim0\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseSegmentSqrtNWithNumSegments\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tidx\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"num_segments\"\n    type_attr: \"Tnumsegments\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"Tnumsegments\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseSegmentSum\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tidx\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseSegmentSumWithNumSegments\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tidx\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"num_segments\"\n    type_attr: \"Tnumsegments\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"Tnumsegments\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseSlice\"\n  input_arg {\n    name: \"indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"start\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"size\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"SparseSoftmax\"\n  input_arg {\n    name: \"sp_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"sp_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"sp_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"SparseSoftmaxCrossEntropyWithLogits\"\n  input_arg {\n    name: \"features\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"labels\"\n    type_attr: \"Tlabels\"\n  }\n  output_arg {\n    name: \"loss\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"backprop\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"Tlabels\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseSparseMaximum\"\n  input_arg {\n    name: \"a_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"a_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"a_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"b_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"b_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"b_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_values\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseSparseMinimum\"\n  input_arg {\n    name: \"a_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"a_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"a_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"b_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"b_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"b_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_values\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseSplit\"\n  input_arg {\n    name: \"split_dim\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"output_indices\"\n    type: DT_INT64\n    number_attr: \"num_split\"\n  }\n  output_arg {\n    name: \"output_values\"\n    type_attr: \"T\"\n    number_attr: \"num_split\"\n  }\n  output_arg {\n    name: \"output_shape\"\n    type: DT_INT64\n    number_attr: \"num_split\"\n  }\n  attr {\n    name: \"num_split\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"SparseTensorDenseAdd\"\n  input_arg {\n    name: \"a_indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"a_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"a_shape\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"b\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseTensorDenseMatMul\"\n  input_arg {\n    name: \"a_indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"a_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"a_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"b\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"product\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"adjoint_a\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"adjoint_b\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"SparseTensorSliceDataset\"\n  input_arg {\n    name: \"indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"values\"\n    type_attr: \"Tvalues\"\n  }\n  input_arg {\n    name: \"dense_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"Tvalues\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"SparseToDense\"\n  input_arg {\n    name: \"sparse_indices\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"output_shape\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"sparse_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"default_value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"dense\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"validate_indices\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SparseToSparseSetOperation\"\n  input_arg {\n    name: \"set1_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"set1_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"set1_shape\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"set2_indices\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"set2_values\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"set2_shape\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"result_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"result_values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"result_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"set_operation\"\n    type: \"string\"\n  }\n  attr {\n    name: \"validate_indices\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT8\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_UINT8\n        type: DT_UINT16\n        type: DT_STRING\n      }\n    }\n  }\n}\nop {\n  name: \"Split\"\n  input_arg {\n    name: \"split_dim\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n    number_attr: \"num_split\"\n  }\n  attr {\n    name: \"num_split\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"SplitV\"\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"size_splits\"\n    type_attr: \"Tlen\"\n  }\n  input_arg {\n    name: \"split_dim\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n    number_attr: \"num_split\"\n  }\n  attr {\n    name: \"num_split\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tlen\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"SqlDataset\"\n  input_arg {\n    name: \"driver_name\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"data_source_name\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"query\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"Sqrt\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"SqrtGrad\"\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"dy\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Square\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"SquaredDifference\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n  is_commutative: true\n}\nop {\n  name: \"Squeeze\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"squeeze_dims\"\n    type: \"list(int)\"\n    default_value {\n      list {\n      }\n    }\n    has_minimum: true\n  }\n}\nop {\n  name: \"Stack\"\n  output_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"elem_type\"\n    type: \"type\"\n  }\n  attr {\n    name: \"stack_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"StackClose\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n}\nop {\n  name: \"StackCloseV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  is_stateful: true\n}\nop {\n  name: \"StackPop\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  output_arg {\n    name: \"elem\"\n    type_attr: \"elem_type\"\n  }\n  attr {\n    name: \"elem_type\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"StackPopV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"elem\"\n    type_attr: \"elem_type\"\n  }\n  attr {\n    name: \"elem_type\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"StackPush\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"elem\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"swap_memory\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n}\nop {\n  name: \"StackPushV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"elem\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"swap_memory\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"StackV2\"\n  input_arg {\n    name: \"max_size\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"elem_type\"\n    type: \"type\"\n  }\n  attr {\n    name: \"stack_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Stage\"\n  input_arg {\n    name: \"values\"\n    type_list_attr: \"dtypes\"\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"StageClear\"\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"StagePeek\"\n  input_arg {\n    name: \"index\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"values\"\n    type_list_attr: \"dtypes\"\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"StageSize\"\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"StatelessRandomNormal\"\n  input_arg {\n    name: \"shape\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"seed\"\n    type_attr: \"Tseed\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"Tseed\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"StatelessRandomUniform\"\n  input_arg {\n    name: \"shape\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"seed\"\n    type_attr: \"Tseed\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"Tseed\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"StatelessTruncatedNormal\"\n  input_arg {\n    name: \"shape\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"seed\"\n    type_attr: \"Tseed\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"Tseed\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"StatsAggregatorHandle\"\n  output_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"StatsAggregatorSummary\"\n  input_arg {\n    name: \"iterator\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"summary\"\n    type: DT_STRING\n  }\n  is_stateful: true\n}\nop {\n  name: \"StopGradient\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"StridedSlice\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"begin\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"end\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"strides\"\n    type_attr: \"Index\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Index\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"begin_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"end_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"ellipsis_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"new_axis_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"shrink_axis_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n}\nop {\n  name: \"StridedSliceAssign\"\n  input_arg {\n    name: \"ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  input_arg {\n    name: \"begin\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"end\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"strides\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_ref\"\n    type_attr: \"T\"\n    is_ref: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Index\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"begin_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"end_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"ellipsis_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"new_axis_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"shrink_axis_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n}\nop {\n  name: \"StridedSliceGrad\"\n  input_arg {\n    name: \"shape\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"begin\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"end\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"strides\"\n    type_attr: \"Index\"\n  }\n  input_arg {\n    name: \"dy\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Index\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"begin_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"end_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"ellipsis_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"new_axis_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"shrink_axis_mask\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n}\nop {\n  name: \"StringJoin\"\n  input_arg {\n    name: \"inputs\"\n    type: DT_STRING\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"separator\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n}\nop {\n  name: \"StringSplit\"\n  input_arg {\n    name: \"input\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"delimiter\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"values\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"skip_empty\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n}\nop {\n  name: \"StringToHashBucket\"\n  input_arg {\n    name: \"string_tensor\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"num_buckets\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"StringToHashBucketFast\"\n  input_arg {\n    name: \"input\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"num_buckets\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"StringToHashBucketStrong\"\n  input_arg {\n    name: \"input\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"num_buckets\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"key\"\n    type: \"list(int)\"\n  }\n}\nop {\n  name: \"StringToNumber\"\n  input_arg {\n    name: \"string_tensor\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"out_type\"\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    default_value {\n      type: DT_FLOAT\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Sub\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_UINT16\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Substr\"\n  input_arg {\n    name: \"input\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"pos\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"len\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Sum\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"reduction_indices\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"keep_dims\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Svd\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"s\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"u\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"v\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"compute_uv\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"full_matrices\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_DOUBLE\n        type: DT_FLOAT\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Switch\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"pred\"\n    type: DT_BOOL\n  }\n  output_arg {\n    name: \"output_false\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output_true\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"SymbolicGradient\"\n  input_arg {\n    name: \"input\"\n    type_list_attr: \"Tin\"\n  }\n  output_arg {\n    name: \"output\"\n    type_list_attr: \"Tout\"\n  }\n  attr {\n    name: \"Tin\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"Tout\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"f\"\n    type: \"func\"\n  }\n}\nop {\n  name: \"TFRecordDataset\"\n  input_arg {\n    name: \"filenames\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"compression_type\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"buffer_size\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  is_stateful: true\n}\nop {\n  name: \"TFRecordReader\"\n  output_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"compression_type\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use TFRecordReaderV2\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"TFRecordReaderV2\"\n  output_arg {\n    name: \"reader_handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"compression_type\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"TakeDataset\"\n  input_arg {\n    name: \"input_dataset\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"count\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n}\nop {\n  name: \"TakeManySparseFromTensorsMap\"\n  input_arg {\n    name: \"sparse_handles\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sparse_indices\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sparse_values\"\n    type_attr: \"dtype\"\n  }\n  output_arg {\n    name: \"sparse_shape\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Tan\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"Tanh\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"TanhGrad\"\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"dy\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"TemporaryVariable\"\n  output_arg {\n    name: \"ref\"\n    type_attr: \"dtype\"\n    is_ref: true\n  }\n  attr {\n    name: \"shape\"\n    type: \"shape\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"var_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArray\"\n  input_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"dynamic_size\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"clear_after_read\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"tensor_array_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"element_shape\"\n    type: \"shape\"\n    default_value {\n      shape {\n        unknown_rank: true\n      }\n    }\n  }\n  deprecation {\n    version: 16\n    explanation: \"Use TensorArrayV3\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArrayClose\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  deprecation {\n    version: 16\n    explanation: \"Use TensorArrayCloseV3\"\n  }\n}\nop {\n  name: \"TensorArrayCloseV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use TensorArrayCloseV3\"\n  }\n}\nop {\n  name: \"TensorArrayCloseV3\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArrayConcat\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  output_arg {\n    name: \"lengths\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"element_shape_except0\"\n    type: \"shape\"\n    default_value {\n      shape {\n        unknown_rank: true\n      }\n    }\n  }\n  deprecation {\n    version: 16\n    explanation: \"Use TensorArrayGradV3\"\n  }\n}\nop {\n  name: \"TensorArrayConcatV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  output_arg {\n    name: \"lengths\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"element_shape_except0\"\n    type: \"shape\"\n    default_value {\n      shape {\n        unknown_rank: true\n      }\n    }\n  }\n}\nop {\n  name: \"TensorArrayConcatV3\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  output_arg {\n    name: \"lengths\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"element_shape_except0\"\n    type: \"shape\"\n    default_value {\n      shape {\n        unknown_rank: true\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArrayGather\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"element_shape\"\n    type: \"shape\"\n    default_value {\n      shape {\n        unknown_rank: true\n      }\n    }\n  }\n  deprecation {\n    version: 16\n    explanation: \"Use TensorArrayGatherV3\"\n  }\n}\nop {\n  name: \"TensorArrayGatherV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"element_shape\"\n    type: \"shape\"\n    default_value {\n      shape {\n        unknown_rank: true\n      }\n    }\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use TensorArrayGatherV3\"\n  }\n}\nop {\n  name: \"TensorArrayGatherV3\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"element_shape\"\n    type: \"shape\"\n    default_value {\n      shape {\n        unknown_rank: true\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArrayGrad\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"grad_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"source\"\n    type: \"string\"\n  }\n  deprecation {\n    version: 16\n    explanation: \"Use TensorArrayGradV3\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArrayGradV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"grad_handle\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"source\"\n    type: \"string\"\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use TensorArrayGradV3\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArrayGradV3\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"grad_handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"flow_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"source\"\n    type: \"string\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArrayPack\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"element_shape\"\n    type: \"shape\"\n    default_value {\n      shape {\n        unknown_rank: true\n      }\n    }\n  }\n  deprecation {\n    version: 16\n    explanation: \"Use TensorArrayGatherV3 with RangeOp\"\n  }\n}\nop {\n  name: \"TensorArrayRead\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"index\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 16\n    explanation: \"Use TensorArrayReadV3\"\n  }\n}\nop {\n  name: \"TensorArrayReadV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"index\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use TensorArrayReadV3\"\n  }\n}\nop {\n  name: \"TensorArrayReadV3\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"index\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"value\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArrayScatter\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"flow_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 19\n    explanation: \"Use TensorArrayGradV3\"\n  }\n}\nop {\n  name: \"TensorArrayScatterV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"flow_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use TensorArrayScatterV3\"\n  }\n}\nop {\n  name: \"TensorArrayScatterV3\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"flow_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArraySize\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  deprecation {\n    version: 16\n    explanation: \"Use TensorArraySizeV3\"\n  }\n}\nop {\n  name: \"TensorArraySizeV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use TensorArraySizeV3\"\n  }\n}\nop {\n  name: \"TensorArraySizeV3\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArraySplit\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lengths\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"flow_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 16\n    explanation: \"Use TensorArraySplitV3\"\n  }\n}\nop {\n  name: \"TensorArraySplitV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lengths\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"flow_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use TensorArraySplitV3\"\n  }\n}\nop {\n  name: \"TensorArraySplitV3\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"lengths\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"flow_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArrayUnpack\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"flow_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 20\n    explanation: \"Use TensorArrayScatterV3 with RangeOp\"\n  }\n}\nop {\n  name: \"TensorArrayV2\"\n  input_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"element_shape\"\n    type: \"shape\"\n    default_value {\n      shape {\n        unknown_rank: true\n      }\n    }\n  }\n  attr {\n    name: \"dynamic_size\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"clear_after_read\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"tensor_array_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use TensorArrayV3\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArrayV3\"\n  input_arg {\n    name: \"size\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"flow\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"element_shape\"\n    type: \"shape\"\n    default_value {\n      shape {\n        unknown_rank: true\n      }\n    }\n  }\n  attr {\n    name: \"dynamic_size\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"clear_after_read\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"identical_element_shapes\"\n    type: \"bool\"\n    default_value {\n      b: false\n    }\n  }\n  attr {\n    name: \"tensor_array_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorArrayWrite\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  input_arg {\n    name: \"index\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"flow_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 16\n    explanation: \"Use TensorArrayWriteV3\"\n  }\n}\nop {\n  name: \"TensorArrayWriteV2\"\n  input_arg {\n    name: \"handle\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"index\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"flow_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use TensorArrayWriteV3\"\n  }\n}\nop {\n  name: \"TensorArrayWriteV3\"\n  input_arg {\n    name: \"handle\"\n    type: DT_RESOURCE\n  }\n  input_arg {\n    name: \"index\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"flow_in\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"flow_out\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorDataset\"\n  input_arg {\n    name: \"components\"\n    type_list_attr: \"Toutput_types\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"Toutput_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorListElementShape\"\n  input_arg {\n    name: \"input_handle\"\n    type: DT_VARIANT\n  }\n  output_arg {\n    name: \"element_shape\"\n    type_attr: \"shape_type\"\n  }\n  attr {\n    name: \"shape_type\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"TensorListFromTensor\"\n  input_arg {\n    name: \"tensor\"\n    type_attr: \"element_dtype\"\n  }\n  input_arg {\n    name: \"element_shape\"\n    type_attr: \"shape_type\"\n  }\n  output_arg {\n    name: \"output_handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"element_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"shape_type\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"TensorListGetItem\"\n  input_arg {\n    name: \"input_handle\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"index\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"item\"\n    type_attr: \"element_dtype\"\n  }\n  attr {\n    name: \"element_dtype\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"TensorListLength\"\n  input_arg {\n    name: \"input_handle\"\n    type: DT_VARIANT\n  }\n  output_arg {\n    name: \"length\"\n    type: DT_INT32\n  }\n}\nop {\n  name: \"TensorListPopBack\"\n  input_arg {\n    name: \"input_handle\"\n    type: DT_VARIANT\n  }\n  output_arg {\n    name: \"output_handle\"\n    type: DT_VARIANT\n  }\n  output_arg {\n    name: \"tensor\"\n    type_attr: \"element_dtype\"\n  }\n  attr {\n    name: \"element_dtype\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"TensorListPushBack\"\n  input_arg {\n    name: \"input_handle\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"tensor\"\n    type_attr: \"element_dtype\"\n  }\n  output_arg {\n    name: \"output_handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"element_dtype\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"TensorListReserve\"\n  input_arg {\n    name: \"element_shape\"\n    type_attr: \"shape_type\"\n  }\n  input_arg {\n    name: \"num_elements\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"element_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"shape_type\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"TensorListSetItem\"\n  input_arg {\n    name: \"input_handle\"\n    type: DT_VARIANT\n  }\n  input_arg {\n    name: \"index\"\n    type: DT_INT32\n  }\n  input_arg {\n    name: \"item\"\n    type_attr: \"element_dtype\"\n  }\n  output_arg {\n    name: \"output_handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"element_dtype\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"TensorListStack\"\n  input_arg {\n    name: \"input_handle\"\n    type: DT_VARIANT\n  }\n  output_arg {\n    name: \"tensor\"\n    type_attr: \"element_dtype\"\n  }\n  attr {\n    name: \"element_dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"num_elements\"\n    type: \"int\"\n    default_value {\n      i: -1\n    }\n  }\n}\nop {\n  name: \"TensorSliceDataset\"\n  input_arg {\n    name: \"components\"\n    type_list_attr: \"Toutput_types\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"Toutput_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  is_stateful: true\n}\nop {\n  name: \"TensorSummary\"\n  input_arg {\n    name: \"tensor\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"summary\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"description\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"labels\"\n    type: \"list(string)\"\n    default_value {\n      list {\n      }\n    }\n  }\n  attr {\n    name: \"display_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n}\nop {\n  name: \"TensorSummaryV2\"\n  input_arg {\n    name: \"tag\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"tensor\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"serialized_summary_metadata\"\n    type: DT_STRING\n  }\n  output_arg {\n    name: \"summary\"\n    type: DT_STRING\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"TextLineDataset\"\n  input_arg {\n    name: \"filenames\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"compression_type\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"buffer_size\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  is_stateful: true\n}\nop {\n  name: \"TextLineReader\"\n  output_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"skip_header_lines\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  deprecation {\n    version: 26\n    explanation: \"Use TextLineReaderV2\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"TextLineReaderV2\"\n  output_arg {\n    name: \"reader_handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"skip_header_lines\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"ThreadUnsafeUnigramCandidateSampler\"\n  input_arg {\n    name: \"true_classes\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sampled_candidates\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"true_expected_count\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"sampled_expected_count\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"num_true\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"num_sampled\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"unique\"\n    type: \"bool\"\n  }\n  attr {\n    name: \"range_max\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Tile\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"multiples\"\n    type_attr: \"Tmultiples\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tmultiples\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"TileGrad\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"multiples\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  deprecation {\n    version: 3\n    explanation: \"TileGrad has been replaced with reduce_sum\"\n  }\n}\nop {\n  name: \"Timestamp\"\n  output_arg {\n    name: \"ts\"\n    type: DT_DOUBLE\n  }\n  is_stateful: true\n}\nop {\n  name: \"TopK\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"k\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"sorted\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  deprecation {\n    version: 7\n    explanation: \"Use TopKV2 instead\"\n  }\n}\nop {\n  name: \"TopKV2\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"k\"\n    type: DT_INT32\n  }\n  output_arg {\n    name: \"values\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"indices\"\n    type: DT_INT32\n  }\n  attr {\n    name: \"sorted\"\n    type: \"bool\"\n    default_value {\n      b: true\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n}\nop {\n  name: \"Transpose\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"perm\"\n    type_attr: \"Tperm\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Tperm\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"TruncateDiv\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_UINT8\n        type: DT_INT8\n        type: DT_UINT16\n        type: DT_INT16\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_COMPLEX64\n        type: DT_COMPLEX128\n      }\n    }\n  }\n}\nop {\n  name: \"TruncateMod\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"TruncatedNormal\"\n  input_arg {\n    name: \"shape\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"dtype\"\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_HALF\n        type: DT_BFLOAT16\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Unbatch\"\n  input_arg {\n    name: \"batched_tensor\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"batch_index\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"id\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"unbatched_tensor\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"timeout_micros\"\n    type: \"int\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"UnbatchGrad\"\n  input_arg {\n    name: \"original_input\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"batch_index\"\n    type: DT_INT64\n  }\n  input_arg {\n    name: \"grad\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"id\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"batched_grad\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"UniformCandidateSampler\"\n  input_arg {\n    name: \"true_classes\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"sampled_candidates\"\n    type: DT_INT64\n  }\n  output_arg {\n    name: \"true_expected_count\"\n    type: DT_FLOAT\n  }\n  output_arg {\n    name: \"sampled_expected_count\"\n    type: DT_FLOAT\n  }\n  attr {\n    name: \"num_true\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"num_sampled\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"unique\"\n    type: \"bool\"\n  }\n  attr {\n    name: \"range_max\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"seed\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  attr {\n    name: \"seed2\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Unique\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"idx\"\n    type_attr: \"out_idx\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"out_idx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"UniqueV2\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"axis\"\n    type_attr: \"Taxis\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"idx\"\n    type_attr: \"out_idx\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"Taxis\"\n    type: \"type\"\n    default_value {\n      type: DT_INT64\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"out_idx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"UniqueWithCounts\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"idx\"\n    type_attr: \"out_idx\"\n  }\n  output_arg {\n    name: \"count\"\n    type_attr: \"out_idx\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"out_idx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Unpack\"\n  input_arg {\n    name: \"value\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n    number_attr: \"num\"\n  }\n  attr {\n    name: \"num\"\n    type: \"int\"\n    has_minimum: true\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n  attr {\n    name: \"axis\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n  }\n}\nop {\n  name: \"UnravelIndex\"\n  input_arg {\n    name: \"indices\"\n    type_attr: \"Tidx\"\n  }\n  input_arg {\n    name: \"dims\"\n    type_attr: \"Tidx\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"Tidx\"\n  }\n  attr {\n    name: \"Tidx\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"UnsortedSegmentMax\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"num_segments\"\n    type_attr: \"Tnumsegments\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_INT64\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"Tnumsegments\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"UnsortedSegmentSum\"\n  input_arg {\n    name: \"data\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"segment_ids\"\n    type_attr: \"Tindices\"\n  }\n  input_arg {\n    name: \"num_segments\"\n    type_attr: \"Tnumsegments\"\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n      }\n    }\n  }\n  attr {\n    name: \"Tindices\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  attr {\n    name: \"Tnumsegments\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n}\nop {\n  name: \"Unstage\"\n  output_arg {\n    name: \"values\"\n    type_list_attr: \"dtypes\"\n  }\n  attr {\n    name: \"capacity\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"memory_limit\"\n    type: \"int\"\n    default_value {\n      i: 0\n    }\n    has_minimum: true\n  }\n  attr {\n    name: \"dtypes\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"VarHandleOp\"\n  output_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"shape\"\n    type: \"shape\"\n  }\n  is_stateful: true\n}\nop {\n  name: \"VarIsInitializedOp\"\n  input_arg {\n    name: \"resource\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"is_initialized\"\n    type: DT_BOOL\n  }\n  is_stateful: true\n}\nop {\n  name: \"Variable\"\n  output_arg {\n    name: \"ref\"\n    type_attr: \"dtype\"\n    is_ref: true\n  }\n  attr {\n    name: \"shape\"\n    type: \"shape\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"VariableShape\"\n  input_arg {\n    name: \"input\"\n    type: DT_RESOURCE\n  }\n  output_arg {\n    name: \"output\"\n    type_attr: \"out_type\"\n  }\n  attr {\n    name: \"out_type\"\n    type: \"type\"\n    default_value {\n      type: DT_INT32\n    }\n    allowed_values {\n      list {\n        type: DT_INT32\n        type: DT_INT64\n      }\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"VariableV2\"\n  output_arg {\n    name: \"ref\"\n    type_attr: \"dtype\"\n    is_ref: true\n  }\n  attr {\n    name: \"shape\"\n    type: \"shape\"\n  }\n  attr {\n    name: \"dtype\"\n    type: \"type\"\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"Where\"\n  input_arg {\n    name: \"input\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"index\"\n    type: DT_INT64\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    default_value {\n      type: DT_BOOL\n    }\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n        type: DT_INT32\n        type: DT_UINT8\n        type: DT_INT16\n        type: DT_INT8\n        type: DT_COMPLEX64\n        type: DT_INT64\n        type: DT_QINT8\n        type: DT_QUINT8\n        type: DT_QINT32\n        type: DT_BFLOAT16\n        type: DT_UINT16\n        type: DT_COMPLEX128\n        type: DT_HALF\n        type: DT_UINT32\n        type: DT_UINT64\n        type: DT_BOOL\n      }\n    }\n  }\n}\nop {\n  name: \"WholeFileReader\"\n  output_arg {\n    name: \"reader_handle\"\n    type: DT_STRING\n    is_ref: true\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"WholeFileReaderV2\"\n  output_arg {\n    name: \"reader_handle\"\n    type: DT_RESOURCE\n  }\n  attr {\n    name: \"container\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  attr {\n    name: \"shared_name\"\n    type: \"string\"\n    default_value {\n      s: \"\"\n    }\n  }\n  is_stateful: true\n}\nop {\n  name: \"WriteFile\"\n  input_arg {\n    name: \"filename\"\n    type: DT_STRING\n  }\n  input_arg {\n    name: \"contents\"\n    type: DT_STRING\n  }\n}\nop {\n  name: \"ZerosLike\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"y\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n  }\n}\nop {\n  name: \"Zeta\"\n  input_arg {\n    name: \"x\"\n    type_attr: \"T\"\n  }\n  input_arg {\n    name: \"q\"\n    type_attr: \"T\"\n  }\n  output_arg {\n    name: \"z\"\n    type_attr: \"T\"\n  }\n  attr {\n    name: \"T\"\n    type: \"type\"\n    allowed_values {\n      list {\n        type: DT_FLOAT\n        type: DT_DOUBLE\n      }\n    }\n  }\n}\nop {\n  name: \"ZipDataset\"\n  input_arg {\n    name: \"input_datasets\"\n    type: DT_VARIANT\n    number_attr: \"N\"\n  }\n  output_arg {\n    name: \"handle\"\n    type: DT_VARIANT\n  }\n  attr {\n    name: \"output_types\"\n    type: \"list(type)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"output_shapes\"\n    type: \"list(shape)\"\n    has_minimum: true\n    minimum: 1\n  }\n  attr {\n    name: \"N\"\n    type: \"int\"\n    has_minimum: true\n    minimum: 1\n  }\n}\n", "subject": "Merge pull request #2997 from maciekcc/branch_125575345", "message": "Merge pull request #2997 from maciekcc/branch_125575345\n\nBranch 125575345" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "tensorflow/tensorflow", "file": "gradients.py", "language": "py", "commit_date": "2016-06-22 22:16:40.000 UTC", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Implements the graph generation for computation of gradients.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\nfrom tensorflow.python.ops.gradients_impl import AggregationMethod\nfrom tensorflow.python.ops.gradients_impl import gradients\nfrom tensorflow.python.ops.gradients_impl import hessians\n# pylint: enable=unused-import\nfrom tensorflow.python.util.all_util import remove_undocumented\n\n_allowed_symbols = [\n    # TODO(drpng): find a good place to reference this.\n    \"AggregationMethod\",\n    \"gradients\",  # tf.gradients.gradients.\n    \"hessians\",  # tf.gradients.hessians\n]\nremove_undocumented(__name__, _allowed_symbols)\n", "subject": "Merge pull request #2997 from maciekcc/branch_125575345", "message": "Merge pull request #2997 from maciekcc/branch_125575345\n\nBranch 125575345" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "tensorflow/tensorflow", "file": "tf_record_reader_op.cc", "language": "cc", "commit_date": "2016-06-22 22:16:40.000 UTC", "content": "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/io_ops.cc.\n\n#include <memory>\n#include \"tensorflow/core/framework/reader_base.h\"\n#include \"tensorflow/core/framework/reader_op_kernel.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/io/record_reader.h\"\n#include \"tensorflow/core/lib/strings/strcat.h\"\n#include \"tensorflow/core/platform/env.h\"\n\nnamespace tensorflow {\n\nclass TFRecordReader : public ReaderBase {\n public:\n  TFRecordReader(const string& node_name, const string& compression_type,\n                 Env* env)\n      : ReaderBase(strings::StrCat(\"TFRecordReader '\", node_name, \"'\")),\n        env_(env),\n        offset_(0),\n        compression_type_(compression_type) {}\n\n  Status OnWorkStartedLocked() override {\n    offset_ = 0;\n    TF_RETURN_IF_ERROR(env_->NewRandomAccessFile(current_work(), &file_));\n\n    io::RecordReaderOptions options =\n        io::RecordReaderOptions::CreateRecordReaderOptions(compression_type_);\n    reader_.reset(new io::RecordReader(file_.get(), options));\n    return Status::OK();\n  }\n\n  Status OnWorkFinishedLocked() override {\n    reader_.reset(nullptr);\n    file_.reset(nullptr);\n    return Status::OK();\n  }\n\n  Status ReadLocked(string* key, string* value, bool* produced,\n                    bool* at_end) override {\n    *key = strings::StrCat(current_work(), \":\", offset_);\n    Status status = reader_->ReadRecord(&offset_, value);\n    if (errors::IsOutOfRange(status)) {\n      *at_end = true;\n      return Status::OK();\n    }\n    if (!status.ok()) return status;\n    *produced = true;\n    return Status::OK();\n  }\n\n  Status ResetLocked() override {\n    offset_ = 0;\n    reader_.reset(nullptr);\n    file_.reset(nullptr);\n    return ReaderBase::ResetLocked();\n  }\n\n  // TODO(josh11b): Implement serializing and restoring the state.\n\n private:\n  Env* const env_;\n  uint64 offset_;\n  std::unique_ptr<RandomAccessFile> file_;\n  std::unique_ptr<io::RecordReader> reader_;\n  string compression_type_ = \"\";\n};\n\nclass TFRecordReaderOp : public ReaderOpKernel {\n public:\n  explicit TFRecordReaderOp(OpKernelConstruction* context)\n      : ReaderOpKernel(context) {\n    Env* env = context->env();\n\n    string compression_type;\n    OP_REQUIRES_OK(context,\n                   context->GetAttr(\"compression_type\", &compression_type));\n\n    SetReaderFactory([this, compression_type, env]() {\n      return new TFRecordReader(name(), compression_type, env);\n    });\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"TFRecordReader\").Device(DEVICE_CPU),\n                        TFRecordReaderOp);\nREGISTER_KERNEL_BUILDER(Name(\"TFRecordReaderV2\").Device(DEVICE_CPU),\n                        TFRecordReaderOp);\n\n}  // namespace tensorflow\n", "subject": "Merge pull request #2997 from maciekcc/branch_125575345", "message": "Merge pull request #2997 from maciekcc/branch_125575345\n\nBranch 125575345" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "tensorflow/tensorflow", "file": "evaluation_test.py", "language": "py", "commit_date": "2016-06-22 22:16:40.000 UTC", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for slim.evaluation.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os\nimport shutil\nimport time\n\nimport numpy as np\n\nfrom tensorflow.contrib.framework.python.ops import variables as variables_lib\nfrom tensorflow.contrib.metrics.python.ops import metric_ops\nfrom tensorflow.contrib.slim.python.slim import evaluation\nfrom tensorflow.contrib.training.python.training import evaluation as evaluation_lib\nfrom tensorflow.core.protobuf import saver_pb2\nfrom tensorflow.python.debug.lib import debug_data\nfrom tensorflow.python.debug.wrappers import hooks\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import flags\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.summary import summary_iterator\nfrom tensorflow.python.training import input  # pylint: disable=redefined-builtin\nfrom tensorflow.python.training import saver as saver_lib\nfrom tensorflow.python.training import session_run_hook\n\n\nFLAGS = flags.FLAGS\n\n\ndef GenerateTestData(num_classes, batch_size):\n  inputs = np.random.rand(batch_size, num_classes)\n\n  np.random.seed(0)\n  labels = np.random.randint(low=0, high=num_classes, size=batch_size)\n  labels = labels.reshape((batch_size,))\n  return inputs, labels\n\n\ndef TestModel(inputs):\n  scale = variables.Variable(1.0, trainable=False)\n\n  # Scaling the outputs wont change the result...\n  outputs = math_ops.multiply(inputs, scale)\n  return math_ops.argmax(outputs, 1), scale\n\n\ndef GroundTruthAccuracy(inputs, labels, batch_size):\n  predictions = np.argmax(inputs, 1)\n  num_correct = np.sum(predictions == labels)\n  return float(num_correct) / batch_size\n\n\nclass EvaluationTest(test.TestCase):\n\n  def setUp(self):\n    super(EvaluationTest, self).setUp()\n\n    num_classes = 8\n    batch_size = 16\n    inputs, labels = GenerateTestData(num_classes, batch_size)\n    self._expected_accuracy = GroundTruthAccuracy(inputs, labels, batch_size)\n\n    self._global_step = variables_lib.get_or_create_global_step()\n    self._inputs = constant_op.constant(inputs, dtype=dtypes.float32)\n    self._labels = constant_op.constant(labels, dtype=dtypes.int64)\n    self._predictions, self._scale = TestModel(self._inputs)\n\n  def testFinalOpsOnEvaluationLoop(self):\n    value_op, update_op = metric_ops.streaming_accuracy(self._predictions,\n                                                        self._labels)\n    init_op = control_flow_ops.group(variables.global_variables_initializer(),\n                                     variables.local_variables_initializer())\n    # Create checkpoint and log directories:\n    chkpt_dir = os.path.join(self.get_temp_dir(), 'tmp_logs/')\n    gfile.MakeDirs(chkpt_dir)\n    logdir = os.path.join(self.get_temp_dir(), 'tmp_logs2/')\n    gfile.MakeDirs(logdir)\n\n    # Save initialized variables to a checkpoint directory:\n    saver = saver_lib.Saver()\n    with self.test_session() as sess:\n      init_op.run()\n      saver.save(sess, os.path.join(chkpt_dir, 'chkpt'))\n\n    class Object(object):\n\n      def __init__(self):\n        self.hook_was_run = False\n\n    obj = Object()\n\n    # Create a custom session run hook.\n    class CustomHook(session_run_hook.SessionRunHook):\n\n      def __init__(self, obj):\n        self.obj = obj\n\n      def end(self, session):\n        self.obj.hook_was_run = True\n\n    # Now, run the evaluation loop:\n    accuracy_value = evaluation.evaluation_loop(\n        '',\n        chkpt_dir,\n        logdir,\n        eval_op=update_op,\n        final_op=value_op,\n        hooks=[CustomHook(obj)],\n        max_number_of_evaluations=1)\n    self.assertAlmostEqual(accuracy_value, self._expected_accuracy)\n\n    # Validate that custom hook ran.\n    self.assertTrue(obj.hook_was_run)\n\n  def _create_names_to_metrics(self, predictions, labels):\n    accuracy0, update_op0 = metric_ops.streaming_accuracy(predictions, labels)\n    accuracy1, update_op1 = metric_ops.streaming_accuracy(predictions + 1,\n                                                          labels)\n\n    names_to_values = {'Accuracy': accuracy0, 'Another_accuracy': accuracy1}\n    names_to_updates = {'Accuracy': update_op0, 'Another_accuracy': update_op1}\n    return names_to_values, names_to_updates\n\n  def _verify_summaries(self, output_dir, names_to_values):\n    \"\"\"Verifies that the given `names_to_values` are found in the summaries.\n\n    Args:\n      output_dir: An existing directory where summaries are found.\n      names_to_values: A dictionary of strings to values.\n    \"\"\"\n    # Check that the results were saved. The events file may have additional\n    # entries, e.g. the event version stamp, so have to parse things a bit.\n    output_filepath = glob.glob(os.path.join(output_dir, '*'))\n    self.assertEqual(len(output_filepath), 1)\n\n    events = summary_iterator.summary_iterator(output_filepath[0])\n    summaries = [e.summary for e in events if e.summary.value]\n    values = []\n    for summary in summaries:\n      for value in summary.value:\n        values.append(value)\n    saved_results = {v.tag: v.simple_value for v in values}\n    for name in names_to_values:\n      self.assertAlmostEqual(names_to_values[name], saved_results[name])\n\n  def testLatestCheckpointReturnsNoneAfterTimeout(self):\n    start = time.time()\n    ret = evaluation_lib.wait_for_new_checkpoint(\n        '/non-existent-dir', 'foo', timeout=1.0, seconds_to_sleep=0.5)\n    end = time.time()\n    self.assertIsNone(ret)\n    # We've waited one time.\n    self.assertGreater(end, start + 0.5)\n    # The timeout kicked in.\n    self.assertLess(end, start + 1.1)\n\n  def testMonitorCheckpointsLoopTimeout(self):\n    ret = list(\n        evaluation_lib.checkpoints_iterator(\n            '/non-existent-dir', timeout=0))\n    self.assertEqual(ret, [])\n\n  def testWithEpochLimit(self):\n    predictions_limited = input.limit_epochs(self._predictions, num_epochs=1)\n    labels_limited = input.limit_epochs(self._labels, num_epochs=1)\n\n    value_op, update_op = metric_ops.streaming_accuracy(\n        predictions_limited, labels_limited)\n\n    init_op = control_flow_ops.group(variables.global_variables_initializer(),\n                                     variables.local_variables_initializer())\n    # Create checkpoint and log directories:\n    chkpt_dir = os.path.join(self.get_temp_dir(), 'tmp_logs/')\n    gfile.MakeDirs(chkpt_dir)\n    logdir = os.path.join(self.get_temp_dir(), 'tmp_logs2/')\n    gfile.MakeDirs(logdir)\n\n    # Save initialized variables to a checkpoint directory:\n    saver = saver_lib.Saver()\n    with self.test_session() as sess:\n      init_op.run()\n      saver.save(sess, os.path.join(chkpt_dir, 'chkpt'))\n\n    # Now, run the evaluation loop:\n    accuracy_value = evaluation.evaluation_loop(\n        '', chkpt_dir, logdir, eval_op=update_op, final_op=value_op,\n        max_number_of_evaluations=1, num_evals=10000)\n    self.assertAlmostEqual(accuracy_value, self._expected_accuracy)\n\n\nclass SingleEvaluationTest(test.TestCase):\n\n  def setUp(self):\n    super(SingleEvaluationTest, self).setUp()\n\n    num_classes = 8\n    batch_size = 16\n    inputs, labels = GenerateTestData(num_classes, batch_size)\n    self._expected_accuracy = GroundTruthAccuracy(inputs, labels, batch_size)\n\n    self._global_step = variables_lib.get_or_create_global_step()\n    self._inputs = constant_op.constant(inputs, dtype=dtypes.float32)\n    self._labels = constant_op.constant(labels, dtype=dtypes.int64)\n    self._predictions, self._scale = TestModel(self._inputs)\n\n  def testErrorRaisedIfCheckpointDoesntExist(self):\n    checkpoint_path = os.path.join(self.get_temp_dir(),\n                                   'this_file_doesnt_exist')\n    log_dir = os.path.join(self.get_temp_dir(), 'error_raised')\n    with self.assertRaises(errors.NotFoundError):\n      evaluation.evaluate_once('', checkpoint_path, log_dir)\n\n  def _prepareCheckpoint(self, checkpoint_path):\n    init_op = control_flow_ops.group(variables.global_variables_initializer(),\n                                     variables.local_variables_initializer())\n    saver = saver_lib.Saver(write_version=saver_pb2.SaverDef.V1)\n    with self.test_session() as sess:\n      sess.run(init_op)\n      saver.save(sess, checkpoint_path)\n\n  def testRestoredModelPerformance(self):\n    checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')\n    log_dir = os.path.join(self.get_temp_dir(), 'log_dir1/')\n\n    # First, save out the current model to a checkpoint:\n    self._prepareCheckpoint(checkpoint_path)\n\n    # Next, determine the metric to evaluate:\n    value_op, update_op = metric_ops.streaming_accuracy(self._predictions,\n                                                        self._labels)\n\n    # Run the evaluation and verify the results:\n    accuracy_value = evaluation.evaluate_once(\n        '', checkpoint_path, log_dir, eval_op=update_op, final_op=value_op)\n    self.assertAlmostEqual(accuracy_value, self._expected_accuracy)\n\n  def testAdditionalHooks(self):\n    checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')\n    log_dir = os.path.join(self.get_temp_dir(), 'log_dir1/')\n\n    # First, save out the current model to a checkpoint:\n    self._prepareCheckpoint(checkpoint_path)\n\n    # Next, determine the metric to evaluate:\n    value_op, update_op = metric_ops.streaming_accuracy(self._predictions,\n                                                        self._labels)\n\n    dumping_root = os.path.join(self.get_temp_dir(), 'tfdbg_dump_dir')\n    dumping_hook = hooks.DumpingDebugHook(dumping_root, log_usage=False)\n    try:\n      # Run the evaluation and verify the results:\n      accuracy_value = evaluation.evaluate_once(\n          '', checkpoint_path, log_dir, eval_op=update_op, final_op=value_op,\n          hooks=[dumping_hook])\n      self.assertAlmostEqual(accuracy_value, self._expected_accuracy)\n\n      dump = debug_data.DebugDumpDir(\n          glob.glob(os.path.join(dumping_root, 'run_*'))[0])\n      # Here we simply assert that the dumped data has been loaded and is\n      # non-empty. We do not care about the detailed model-internal tensors or\n      # their values.\n      self.assertTrue(dump.dumped_tensor_data)\n    finally:\n      if os.path.isdir(dumping_root):\n        shutil.rmtree(dumping_root)\n\n\nif __name__ == '__main__':\n  test.main()\n", "subject": "Merge pull request #2997 from maciekcc/branch_125575345", "message": "Merge pull request #2997 from maciekcc/branch_125575345\n\nBranch 125575345" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "tensorflow/tensorflow", "file": "stream.h", "language": "h", "commit_date": "2016-06-22 22:16:40.000 UTC", "content": "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// The Stream is used in conjunction with the StreamExecutor \"parent\" to\n// perform actions with a linear stream of dependencies. Dependencies can also\n// be created between Streams to do task management (i.e. limit which tasks\n// can be performed concurrently and specify what task dependencies exist).\n\n#ifndef TENSORFLOW_STREAM_EXECUTOR_STREAM_H_\n#define TENSORFLOW_STREAM_EXECUTOR_STREAM_H_\n\n#include <complex>\n#include <functional>\n#include <memory>\n\n#include \"tensorflow/stream_executor/blas.h\"\n#include \"tensorflow/stream_executor/device_memory.h\"\n#include \"tensorflow/stream_executor/dnn.h\"\n#include \"tensorflow/stream_executor/event.h\"\n#include \"tensorflow/stream_executor/fft.h\"\n#include \"tensorflow/stream_executor/kernel.h\"\n#include \"tensorflow/stream_executor/launch_dim.h\"\n#include \"tensorflow/stream_executor/lib/array_slice.h\"\n#include \"tensorflow/stream_executor/platform/mutex.h\"\n#include \"tensorflow/stream_executor/platform/port.h\"\n#include \"tensorflow/stream_executor/platform/thread_annotations.h\"\n#include \"tensorflow/stream_executor/temporary_memory_manager.h\"\n\nnamespace perftools {\nnamespace gputools {\n\nnamespace host {\nclass HostBlas;\nclass HostFft;\nclass HostRng;\nclass HostTimer;\n}  // namespace host\n\nnamespace ocl {\nclass CLBlas;\n}  // namespace ocl\n\nnamespace internal {\nclass StreamInterface;\n}  // namespace internal\n\nclass DeviceMemoryBase;\ntemplate <typename ElemT>\nclass DeviceMemory;\n\nclass Timer;\n\nnamespace dnn {\nclass BatchDescriptor;\nclass FilterDescriptor;\nclass ConvolutionDescriptor;\nclass BatchDescriptor;\nclass FilterDescriptor;\nclass ConvolutionDescriptor;\nclass ProfileResult;\nclass AlgorithmDesc;\n}  // namespace dnn\n\nclass StreamExecutor;\nclass ScratchAllocator;\n\n// Convert a type to the corresponding QuantizedActivationMode.\ntemplate <typename ElementType>\nstruct Quantization;\n\n// Represents a stream of dependent computations on a GPU device.\n//\n// The operations within a stream execute linearly and asynchronously until\n// BlockHostUntilDone() is invoked, which synchronously joins host code with\n// the execution of the stream.\n//\n// If any given operation fails when entraining work for the stream, ok() will\n// indicate that an error has occurred. After initialization, once a stream is\n// !ok(), it will never be ok().\n//\n// Thread-safe post-initialization.\nclass Stream {\n public:\n  // Instantiate a stream tied to parent as a platform executor. Work\n  // entrained onto this stream will be launched/managed on that\n  // StreamExecutor's platform.\n  explicit Stream(StreamExecutor *parent);\n\n  // Test only. Use an externally-populated value (like a mock) for the\n  // platform-specific stream implementation.\n  Stream(StreamExecutor *parent, internal::StreamInterface *implementation);\n\n  // Deallocates any stream resources that the parent StreamExecutor has\n  // bestowed\n  // upon this object.\n  ~Stream();\n\n  // Returns whether any errors have occurred while entraining work for this\n  // stream.\n  bool ok() const { return !InErrorState(); }\n\n  // Initialize the stream. This must be performed before entraining any other\n  // operations.\n  Stream &Init() LOCKS_EXCLUDED(mu_);\n\n  // Initializes timer t via the StreamExecutor.\n  Stream &InitTimer(Timer *t);\n\n  // Convenience wrapper around Init() and InitTimer().\n  Stream &InitWithTimer(Timer *t);\n\n  // Get or create a sub-stream from this stream. If there is any sub-stream in\n  // the pool that can be reused then just return this sub-stream.  Otherwise\n  // create a new sub-stream.\n  Stream *GetOrCreateSubStream() LOCKS_EXCLUDED(mu_);\n\n  // Return the sub-stream back to the host stream so that it can be reused\n  // later.\n  void ReturnSubStream(Stream *sub_stream) LOCKS_EXCLUDED(mu_);\n\n  // Allocate temporary memories. The stream will deallocate them when blocked\n  // or destroyed.\n  template <typename T>\n  port::StatusOr<std::unique_ptr<TemporaryDeviceMemory<T>>>\n  AllocateTemporaryArray(uint64 element_count);\n\n  // Entrains onto the stream of operations: a kernel launch with the given\n  // (variadic) parameters for the invocation. These arguments can be things\n  // like DeviceMemory or primitive types such as int. What arguments you may\n  // pass to a given kernel are noted as the template parameters to the\n  // TypedKernel type that the machocc compiler generates.\n  //\n  // Template parameters:\n  //  Params...   The type list of formal parameters that the typed kernel\n  //              expects, which is matched against Args...\n  //  Args...     The deduced type list for passed actual arguments\n  //\n  // Implementation: A compile-time compatibility check is performed that has\n  // some leniency versus an exact parameter pack match -- for example,\n  // `const DeviceMemory<T>` is considered \"pack compatible\" with a\n  // `const DeviceMemory<T>&` formal parameter; in part, because we don't have\n  // perfect forwarding support without rvalue references. It also attempts to\n  // spit out helpful static_assert error traces with information as to the\n  // argument number and types that were mismatched.\n  template <typename... Params, typename... Args>\n  Stream &ThenLaunch(ThreadDim thread_dims, BlockDim block_dims,\n                     const TypedKernel<Params...> &kernel, Args... args);\n\n  // Record a \"start\" event for the interval timer at this point in the\n  // stream's\n  // execution (relative to the previously and subsequently enqueued items in\n  // the stream's execution). Streams may be started/stopped multiple times.\n  Stream &ThenStartTimer(Timer *t);\n\n  // Record a \"stop\" event for the interval timer at this point in the\n  // stream's\n  // execution. See also Stream::ThenStartTimer.\n  Stream &ThenStopTimer(Timer *t);\n\n  // TODO(leary) If work is added to the stream that is being depended upon,\n  //              then what? Have to describe what happens.\n  template <typename... Params>\n  Stream &ThenWaitFor(Stream *other, Params... more_streams) {\n    return ThenWaitFor(more_streams...).ThenWaitFor(other);\n  }\n\n  // Create a dependency for this stream's next work on the other stream\n  // completing. Does not take ownership of other, and other must not be\n  // null.\n  //\n  // Checks that a stream does not wait for itself, and it is up to the\n  // user to guarantee that a stream does not come to wait on itself in a\n  // cyclic\n  // manner; in that case, behavior is undefined.\n  //\n  // N.B. Base recursion case for the variadic ThenWaitFor.\n  Stream &ThenWaitFor(Stream *other);\n\n  // Waits for all streams values in others.\n  // Checks that there is no shallow circular wait (i.e. that \"this\" is not in\n  // others)\n  template <typename P>\n  Stream &ThenWaitFor(P others) {\n    for (auto &stream : *others) {\n      CHECK_NE(stream.get(), this);\n      ThenWaitFor(stream.get());\n    }\n    return *this;\n  }\n\n  // Waits for an event object to be set.\n  // Note that ThenRecordEvent must have been called on the event before\n  // you call this function; otherwise the event will be considered complete\n  // and this wait will do nothing.\n  Stream &ThenWaitFor(Event *event);\n\n  // Inserts the specified event into the end of this stream. Once the stream\n  // has processed all events prior to the insertion point, the event will be\n  // marked as completed.\n  // The stream does not take ownership of event - meaning that event's lifetime\n  // must extend past the point at which it is marked complete!\n  Stream &ThenRecordEvent(Event *event);\n\n  ////////////////\n  // DNN support\n  //\n  // See DnnSupport::* for comments on the following methods.\n\n  Stream &ThenBatchNormalizationForward(\n      const DeviceMemory<float> &x, const DeviceMemory<float> &scale,\n      const DeviceMemory<float> &offset,\n      const DeviceMemory<float> &estimated_mean,\n      const DeviceMemory<float> &estimated_variance,\n      const dnn::BatchDescriptor &x_desc,\n      const dnn::BatchDescriptor &scale_offset_desc, const double epsilon,\n      DeviceMemory<float> *y, DeviceMemory<float> *batch_mean,\n      DeviceMemory<float> *batch_var, DeviceMemory<float> *saved_mean,\n      DeviceMemory<float> *saved_inv_var, bool is_training,\n      std::function<const DeviceMemory<float> &()> var_to_inv_var,\n      std::function<void()> inv_var_to_var);\n\n  Stream &ThenBatchNormalizationBackward(\n      const DeviceMemory<float> &y_backprop, const DeviceMemory<float> &x,\n      const DeviceMemory<float> &scale, const DeviceMemory<float> &mean,\n      const DeviceMemory<float> &inv_var, const dnn::BatchDescriptor &x_desc,\n      const dnn::BatchDescriptor &scale_offset_desc, const double epsilon,\n      DeviceMemory<float> *x_backprop, DeviceMemory<float> *scale_backprop,\n      DeviceMemory<float> *offset_backprop);\n\n  Stream &ThenBatchNormalizationForward(\n      const DeviceMemory<Eigen::half> &x, const DeviceMemory<float> &scale,\n      const DeviceMemory<float> &offset,\n      const DeviceMemory<float> &estimated_mean,\n      const DeviceMemory<float> &estimated_variance,\n      const dnn::BatchDescriptor &x_desc,\n      const dnn::BatchDescriptor &scale_offset_desc, const double epsilon,\n      DeviceMemory<Eigen::half> *y, DeviceMemory<float> *batch_mean,\n      DeviceMemory<float> *batch_var, DeviceMemory<float> *saved_mean,\n      DeviceMemory<float> *saved_inv_var, bool is_training,\n      std::function<const DeviceMemory<float> &()> var_to_inv_var,\n      std::function<void()> inv_var_to_var);\n\n  Stream &ThenBatchNormalizationBackward(\n      const DeviceMemory<Eigen::half> &y_backprop,\n      const DeviceMemory<Eigen::half> &x, const DeviceMemory<float> &scale,\n      const DeviceMemory<float> &mean, const DeviceMemory<float> &inv_var,\n      const dnn::BatchDescriptor &x_desc,\n      const dnn::BatchDescriptor &scale_offset_desc, const double epsilon,\n      DeviceMemory<Eigen::half> *x_backprop,\n      DeviceMemory<float> *scale_backprop,\n      DeviceMemory<float> *offset_backprop);\n\n  // TODO(leary) add double-precision version of this interface.\n  Stream &ThenFusedConvolve(\n      const dnn::BatchDescriptor &conv_input_descriptor,\n      const DeviceMemory<int8> &conv_input_data, float conv_input_scale,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<int8> &filter_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const DeviceMemory<int8> &side_input_data, float side_input_scale,\n      const dnn::BatchDescriptor &bias_descriptor,\n      const DeviceMemory<float> &biases, dnn::ActivationMode activation_mode,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<int8> *output);\n\n  Stream &ThenConvolve(const dnn::BatchDescriptor &input_descriptor,\n                       const DeviceMemory<float> &input_data,\n                       const dnn::FilterDescriptor &filter_descriptor,\n                       const DeviceMemory<float> &filter_data,\n                       const dnn::ConvolutionDescriptor &convolution_descriptor,\n                       const dnn::BatchDescriptor &output_descriptor,\n                       DeviceMemory<float> *output);\n\n  Stream &ThenConvolveQuantized(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<float> &input_data,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<int8> &filter_coefficients,\n      const DeviceMemory<float> &coefficient_scales,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> *output_data);\n\n  Stream &ThenConvolveQuantized(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<float> &input_data,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<int16> &filter_coefficients,\n      const DeviceMemory<float> &coefficient_scales,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> *output_data);\n\n  Stream &ThenFusedConvolveWithScratch(\n      const dnn::BatchDescriptor &conv_input_descriptor,\n      const DeviceMemory<int8> &conv_input_data, float conv_input_scale,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<int8> &filter_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const DeviceMemory<int8> &side_input_data, float side_input_scale,\n      const dnn::BatchDescriptor &bias_descriptor,\n      const DeviceMemory<float> &biases, dnn::ActivationMode activation_mode,\n      const dnn::BatchDescriptor &output_descriptor, DeviceMemory<int8> *output,\n      ScratchAllocator *scratch_allocator);\n\n  Stream &ThenFusedConvolveWithScratch(\n      const dnn::BatchDescriptor &conv_input_descriptor,\n      const DeviceMemory<Eigen::half> &conv_input_data, float conv_input_scale,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<Eigen::half> &filter_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const DeviceMemory<Eigen::half> &side_input_data, float side_input_scale,\n      const dnn::BatchDescriptor &bias_descriptor,\n      const DeviceMemory<Eigen::half> &biases,\n      dnn::ActivationMode activation_mode,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<Eigen::half> *output, ScratchAllocator *scratch_allocator);\n\n  Stream &ThenFusedConvolveWithScratch(\n      const dnn::BatchDescriptor &conv_input_descriptor,\n      const DeviceMemory<float> &conv_input_data, float conv_input_scale,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<float> &filter_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const DeviceMemory<float> &side_input_data, float side_input_scale,\n      const dnn::BatchDescriptor &bias_descriptor,\n      const DeviceMemory<float> &biases, dnn::ActivationMode activation_mode,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> *output, ScratchAllocator *scratch_allocator);\n\n  Stream &ThenConvolveWithScratch(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<Eigen::half> &input_data,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<Eigen::half> &filter_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<Eigen::half> *output, ScratchAllocator *scratch_allocator);\n\n  Stream &ThenConvolveWithScratch(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<float> &input_data,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<float> &filter_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> *output, ScratchAllocator *scratch_allocator);\n\n  Stream &ThenConvolveWithAlgorithm(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<float> &input_data,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<float> &filter_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> *output, ScratchAllocator *scratch_allocator,\n      const dnn::AlgorithmConfig &algorithm_config,\n      dnn::ProfileResult *output_profile_result);\n\n  Stream &ThenConvolveWithAlgorithm(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<Eigen::half> &input_data,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<Eigen::half> &filter_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<Eigen::half> *output, ScratchAllocator *scratch_allocator,\n      const dnn::AlgorithmConfig &algorithm_config,\n      dnn::ProfileResult *output_profile_result);\n\n  Stream &ThenFusedConvolveWithAlgorithm(\n      const dnn::BatchDescriptor &conv_input_descriptor,\n      const DeviceMemory<double> &conv_input_data, double conv_input_scale,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<double> &filter_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const DeviceMemory<double> &side_input_data, double side_input_scale,\n      const dnn::BatchDescriptor &bias_descriptor,\n      const DeviceMemory<double> &biases, dnn::ActivationMode activation_mode,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<double> *output, ScratchAllocator *scratch_allocator,\n      const dnn::AlgorithmConfig &algorithm_config,\n      dnn::ProfileResult *output_profile_result);\n\n  Stream &ThenFusedConvolveWithAlgorithm(\n      const dnn::BatchDescriptor &conv_input_descriptor,\n      const DeviceMemory<float> &conv_input_data, float conv_input_scale,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<float> &filter_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const DeviceMemory<float> &side_input_data, float side_input_scale,\n      const dnn::BatchDescriptor &bias_descriptor,\n      const DeviceMemory<float> &biases, dnn::ActivationMode activation_mode,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> *output, ScratchAllocator *scratch_allocator,\n      const dnn::AlgorithmConfig &algorithm_config,\n      dnn::ProfileResult *output_profile_result);\n\n  Stream &ThenFusedConvolveWithAlgorithm(\n      const dnn::BatchDescriptor &conv_input_descriptor,\n      const DeviceMemory<Eigen::half> &conv_input_data, float conv_input_scale,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<Eigen::half> &filter_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const DeviceMemory<Eigen::half> &side_input_data, float side_input_scale,\n      const dnn::BatchDescriptor &bias_descriptor,\n      const DeviceMemory<Eigen::half> &biases,\n      dnn::ActivationMode activation_mode,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<Eigen::half> *output, ScratchAllocator *scratch_allocator,\n      const dnn::AlgorithmConfig &algorithm_config,\n      dnn::ProfileResult *output_profile_result);\n\n  Stream &ThenFusedConvolveWithAlgorithm(\n      const dnn::BatchDescriptor &conv_input_descriptor,\n      const DeviceMemory<int8> &conv_input_data, float conv_input_scale,\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<int8> &filter_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const DeviceMemory<int8> &side_input_data, float side_input_scale,\n      const dnn::BatchDescriptor &bias_descriptor,\n      const DeviceMemory<float> &biases, dnn::ActivationMode activation_mode,\n      const dnn::BatchDescriptor &output_descriptor, DeviceMemory<int8> *output,\n      ScratchAllocator *scratch_allocator,\n      const dnn::AlgorithmConfig &algorithm_config,\n      dnn::ProfileResult *output_profile_result);\n\n  Stream &ThenSeparableConvolve(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<float> &input_data,\n      const dnn::FilterDescriptor &filter_descriptor, int depth_multiplier,\n      const DeviceMemory<float> &first_weights,\n      const DeviceMemory<float> &second_weights,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> *output);\n\n  Stream &ThenConvolveBackwardData(\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<float> &filter_data,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> backward_output_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::BatchDescriptor &input_descriptor,\n      DeviceMemory<float> *backward_input_data);\n\n  Stream &ThenConvolveBackwardDataWithScratch(\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<float> &filter_data,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> backward_output_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::BatchDescriptor &input_descriptor,\n      DeviceMemory<float> *backward_input_data,\n      ScratchAllocator *scratch_allocator);\n\n  Stream &ThenConvolveBackwardDataWithScratch(\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<Eigen::half> &filter_data,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<Eigen::half> backward_output_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::BatchDescriptor &input_descriptor,\n      DeviceMemory<Eigen::half> *backward_input_data,\n      ScratchAllocator *scratch_allocator);\n\n  Stream &ThenConvolveBackwardDataWithAlgorithm(\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<float> &filter_data,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> backward_output_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::BatchDescriptor &input_descriptor,\n      DeviceMemory<float> *backward_input_data,\n      ScratchAllocator *scratch_allocator,\n      const dnn::AlgorithmConfig &algorithm_config,\n      dnn::ProfileResult *output_profile_result);\n\n  Stream &ThenConvolveBackwardDataWithAlgorithm(\n      const dnn::FilterDescriptor &filter_descriptor,\n      const DeviceMemory<Eigen::half> &filter_data,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<Eigen::half> backward_output_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::BatchDescriptor &input_descriptor,\n      DeviceMemory<Eigen::half> *backward_input_data,\n      ScratchAllocator *scratch_allocator,\n      const dnn::AlgorithmConfig &algorithm_config,\n      dnn::ProfileResult *output_profile_result);\n\n  Stream &ThenConvolveBackwardFilter(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<float> &input_data,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> backward_output_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::FilterDescriptor &filter_descriptor,\n      DeviceMemory<float> *backward_filter_data);\n\n  Stream &ThenConvolveBackwardFilterWithScratch(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<float> &input_data,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> backward_output_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::FilterDescriptor &filter_descriptor,\n      DeviceMemory<float> *backward_filter_data,\n      ScratchAllocator *scratch_allocator);\n\n  Stream &ThenConvolveBackwardFilterWithScratch(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<Eigen::half> &input_data,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<Eigen::half> backward_output_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::FilterDescriptor &filter_descriptor,\n      DeviceMemory<Eigen::half> *backward_filter_data,\n      ScratchAllocator *scratch_allocator);\n\n  Stream &ThenConvolveBackwardFilterWithAlgorithm(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<float> &input_data,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<float> backward_output_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::FilterDescriptor &filter_descriptor,\n      DeviceMemory<float> *backward_filter_data,\n      ScratchAllocator *scratch_allocator,\n      const dnn::AlgorithmConfig &algorithm_config,\n      dnn::ProfileResult *output_profile_result);\n\n  Stream &ThenConvolveBackwardFilterWithAlgorithm(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<Eigen::half> &input_data,\n      const dnn::BatchDescriptor &output_descriptor,\n      DeviceMemory<Eigen::half> backward_output_data,\n      const dnn::ConvolutionDescriptor &convolution_descriptor,\n      const dnn::FilterDescriptor &filter_descriptor,\n      DeviceMemory<Eigen::half> *backward_filter_data,\n      ScratchAllocator *scratch_allocator,\n      const dnn::AlgorithmConfig &algorithm_config,\n      dnn::ProfileResult *output_profile_result);\n\n  Stream &ThenConvolveBackwardBias(const dnn::BatchDescriptor &input_descriptor,\n                                   const DeviceMemory<double> &input_data,\n                                   const dnn::BatchDescriptor &bias_descriptor,\n                                   DeviceMemory<double> *backward_bias_data);\n\n  Stream &ThenConvolveBackwardBias(const dnn::BatchDescriptor &input_descriptor,\n                                   const DeviceMemory<float> &input_data,\n                                   const dnn::BatchDescriptor &bias_descriptor,\n                                   DeviceMemory<float> *backward_bias_data);\n\n  Stream &ThenConvolveBackwardBias(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<Eigen::half> &input_data,\n      const dnn::BatchDescriptor &bias_descriptor,\n      DeviceMemory<Eigen::half> *backward_bias_data);\n\n  Stream &ThenMatMul(const DeviceMemory<float> &input_data,\n                     const DeviceMemory<float> &weights,\n                     const dnn::BatchDescriptor &input_dimensions,\n                     const dnn::BatchDescriptor &output_dimensions,\n                     DeviceMemory<float> *output_data);\n\n  Stream &ThenMatMulQuantized(const DeviceMemory<float> &input_data,\n                              const DeviceMemory<int8> &weights,\n                              const DeviceMemory<float> &weight_scales,\n                              const dnn::BatchDescriptor &input_dimensions,\n                              const dnn::BatchDescriptor &output_dimensions,\n                              DeviceMemory<float> *output_data);\n\n  Stream &ThenMatMulQuantized(const DeviceMemory<float> &input_data,\n                              const DeviceMemory<int16> &weights,\n                              const DeviceMemory<float> &weight_scales,\n                              const dnn::BatchDescriptor &input_dimensions,\n                              const dnn::BatchDescriptor &output_dimensions,\n                              DeviceMemory<float> *output_data);\n\n  Stream &ThenBiasAdd(const DeviceMemory<float> &input_data,\n                      const DeviceMemory<float> &biases,\n                      const dnn::BatchDescriptor &dimensions,\n                      DeviceMemory<float> *output_data);\n\n  Stream &ThenPoolForward(const dnn::PoolingDescriptor &pooling_dimensions,\n                          const dnn::BatchDescriptor &input_dimensions,\n                          const DeviceMemory<double> &input_data,\n                          const dnn::BatchDescriptor &output_dimensions,\n                          DeviceMemory<double> *output_data);\n\n  Stream &ThenPoolForward(const dnn::PoolingDescriptor &pooling_dimensions,\n                          const dnn::BatchDescriptor &input_dimensions,\n                          const DeviceMemory<float> &input_data,\n                          const dnn::BatchDescriptor &output_dimensions,\n                          DeviceMemory<float> *output_data);\n\n  Stream &ThenPoolForward(const dnn::PoolingDescriptor &pooling_dimensions,\n                          const dnn::BatchDescriptor &input_dimensions,\n                          const DeviceMemory<Eigen::half> &input_data,\n                          const dnn::BatchDescriptor &output_dimensions,\n                          DeviceMemory<Eigen::half> *output_data);\n\n  Stream &ThenPoolBackward(const dnn::PoolingDescriptor &pooling_dimensions,\n                           const dnn::BatchDescriptor &input_dimensions,\n                           const DeviceMemory<double> &input_data,\n                           const dnn::BatchDescriptor &output_dimensions,\n                           const DeviceMemory<double> &output_data,\n                           const DeviceMemory<double> &input_diff_data,\n                           DeviceMemory<double> *output_diff_data);\n\n  Stream &ThenPoolBackward(const dnn::PoolingDescriptor &pooling_dimensions,\n                           const dnn::BatchDescriptor &input_dimensions,\n                           const DeviceMemory<float> &input_data,\n                           const dnn::BatchDescriptor &output_dimensions,\n                           const DeviceMemory<float> &output_data,\n                           const DeviceMemory<float> &input_diff_data,\n                           DeviceMemory<float> *output_diff_data);\n\n  Stream &ThenPoolBackward(const dnn::PoolingDescriptor &pooling_dimensions,\n                           const dnn::BatchDescriptor &input_dimensions,\n                           const DeviceMemory<Eigen::half> &input_data,\n                           const dnn::BatchDescriptor &output_dimensions,\n                           const DeviceMemory<Eigen::half> &output_data,\n                           const DeviceMemory<Eigen::half> &input_diff_data,\n                           DeviceMemory<Eigen::half> *output_diff_data);\n\n  Stream &ThenNormalize(const dnn::NormalizeDescriptor &normalize_descriptor,\n                        const DeviceMemory<float> &input_data,\n                        DeviceMemory<float> *output_data);\n\n  // Similar to ThenNormalize, but normalizes across feature maps and allows for\n  // specifying the dimensions of the tensor.\n  Stream &ThenNormalizeWithDimensions(\n      const dnn::NormalizeDescriptor &normalize_descriptor,\n      const dnn::BatchDescriptor &dimensions,\n      const DeviceMemory<float> &input_data, DeviceMemory<float> *output_data);\n\n  Stream &ThenNormalizeBackwardWithDimensions(\n      const dnn::NormalizeDescriptor &normalize_descriptor,\n      const dnn::BatchDescriptor &dimensions,\n      const DeviceMemory<float> &raw_data,\n      const DeviceMemory<float> &normalized_data,\n      const DeviceMemory<float> &normalized_variable_gradient,\n      DeviceMemory<float> *raw_variable_gradient);\n\n  Stream &ThenActivate(dnn::ActivationMode activation_mode,\n                       const dnn::BatchDescriptor &dimensions,\n                       const DeviceMemory<float> &input_data,\n                       DeviceMemory<float> *output_data);\n\n  // Same as ThenActivate, but also takes an options argument that can be used\n  // for platform-specific option flags.\n  Stream &ThenActivateWithOptions(dnn::ActivationMode activation_mode,\n                                  const dnn::BatchDescriptor &dimensions,\n                                  const DeviceMemory<float> &input_data,\n                                  DeviceMemory<float> *output_data,\n                                  uint64 options);\n\n  Stream &ThenDepthConcatenate(\n      port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n      port::ArraySlice<const DeviceMemory<float> *> input_data,\n      DeviceMemory<float> *output_data);\n\n  Stream &ThenSpaceConcatenate(\n      port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n      port::ArraySlice<const DeviceMemory<float> *> input_data,\n      DeviceMemory<float> *output_data,\n      dnn::SpaceConcatenateMode concat_direction);\n\n  // Change the layout of the data by shrinking one dimension (or set of\n  // dimensions) and growing another dimension (or set of dimensions), while\n  // keeping the total number of data elements constant, and maintaining the\n  // current data ordering.\n  Stream &ThenReshape(const dnn::BatchDescriptor &input_dimensions,\n                      const DeviceMemory<float> &input_data,\n                      const dnn::BatchDescriptor &output_dimensions,\n                      DeviceMemory<float> *output_data);\n\n  // Depth to space takes an X by Y image with depth D*M² and changes it to an\n  // MX x MY image with depth D. Each input location (x,y) with depth D*M² in\n  // the input image is changed to an MxM contiguous area in the output image,\n  // with the values being laid out in raster order specified by\n  // DepthToSpaceLayout, and will have a new depth of D.\n  // See the DoDepthToSpace comment for more information.\n  Stream &ThenDepthToSpace(const dnn::BatchDescriptor &input_dimensions,\n                           const DeviceMemory<float> &input_data,\n                           const dnn::DepthToSpaceLayout &depth_to_space_layout,\n                           const int sqrt_depth_reduction,\n                           DeviceMemory<float> *output_data);\n\n  // Space to depth is the inverse of depth to space. Space to depth takes each\n  // non-overlapping M by M patch (in the X and Y dimensions) with depth D of\n  // the input, and transforms it to a 1 by 1 patch with depth D*M². If the\n  // input has size (MX, MY, D), the output has size (X, Y, D*M²). The number of\n  // data elements is not changed.\n  Stream &ThenSpaceToDepth(const dnn::BatchDescriptor &input_dimensions,\n                           const DeviceMemory<float> &input_data,\n                           const dnn::DepthToSpaceLayout &space_to_depth_layout,\n                           const int sqrt_depth_increase,\n                           DeviceMemory<float> *output_data);\n\n  Stream &ThenElementwiseOperate(\n      dnn::ElementwiseOperation operation,\n      port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n      port::ArraySlice<const DeviceMemory<float> *> input_data,\n      const dnn::BatchDescriptor &output_dimensions,\n      DeviceMemory<float> *output_data);\n\n  Stream &ThenElementwiseOperateScaledQuantized(\n      dnn::ElementwiseOperation operation,\n      port::ArraySlice<int> input_multiplicands, int output_divisor,\n      port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n      port::ArraySlice<const DeviceMemory<float> *> input_data,\n      const dnn::BatchDescriptor &output_dimensions,\n      DeviceMemory<float> *output_data);\n\n  Stream &ThenXYPad(const dnn::BatchDescriptor &dimensions,\n                    const DeviceMemory<float> &input_data, int64 left_pad,\n                    int64 right_pad, int64 top_pad, int64 bottom_pad,\n                    DeviceMemory<float> *output_data);\n\n  Stream &ThenXYSlice(const dnn::BatchDescriptor &dimensions,\n                      const DeviceMemory<float> &input_data, int64 left_trim,\n                      int64 right_trim, int64 top_trim, int64 bottom_trim,\n                      DeviceMemory<float> *output_data);\n\n  // Grows the input tensor by replicating the X and Y dimensions. The batch and\n  // depth/feature_map dimensions are unchanged. Currently, the input tensor is\n  // limited to X=1 and Y=1.\n  Stream &ThenXYBroadcast(const dnn::BatchDescriptor &dimensions,\n                          const DeviceMemory<float> &input_data,\n                          int64 replicate_x, int64 replicate_y,\n                          DeviceMemory<float> *output_data);\n\n  // See DnnSupport::DoMemcpyD2HQuantized.\n  Stream &ThenMemcpyD2HQuantized(const DeviceMemory<float> &gpu_unquantized_src,\n                                 dnn::QuantizedActivationMode mode,\n                                 void *host_dst, uint64 size);\n\n  // Template version of ThenMemcpyD2HQuantized that takes a MutableArraySlice\n  // and uses the Quantization trait to call the generic version of\n  // ThenMemcpyD2HQuantized with the correct QuantizedActivationMode.\n  template <typename ElementType>\n  Stream &ThenMemcpyD2HQuantized(\n      const DeviceMemory<float> &gpu_unquantized_src,\n      port::MutableArraySlice<ElementType> host_dst) {\n    return ThenMemcpyD2HQuantized(\n        gpu_unquantized_src, Quantization<ElementType>::kModeId,\n        host_dst.data(), host_dst.size() * sizeof(ElementType));\n  }\n\n  // See DnnSupport::DoMemcpyH2DQuantized.\n  Stream &ThenMemcpyH2DQuantized(const void *host_src, uint64 size,\n                                 dnn::QuantizedActivationMode mode,\n                                 DeviceMemory<float> *gpu_unquantized_dst);\n\n  // Template version of ThenMemcpyH2DQuantized that takes an ArraySlice\n  // and uses the Quantization trait to call the generic version of\n  // ThenMemcpyH2DQuantized with the correct QuantizedActivationMode.\n  template <typename ElementType>\n  Stream &ThenMemcpyH2DQuantized(port::ArraySlice<ElementType> host_src,\n                                 DeviceMemory<float> *gpu_unquantized_dst) {\n    return ThenMemcpyH2DQuantized(\n        host_src.data(), host_src.size() * sizeof(ElementType),\n        Quantization<ElementType>::kModeId, gpu_unquantized_dst);\n  }\n\n  // See DnnSupport::DoCopyHostBuffer2Device.\n  Stream &ThenCopyHostBuffer2Device(HostBuffer *buffer_src,\n                                    DeviceMemory<float> *gpu_unquantized_dst);\n\n  // See DnnSupport::DoCopyDevice2HostBuffer.\n  Stream &ThenCopyDevice2HostBuffer(\n      const DeviceMemory<float> &gpu_unquantized_src, HostBuffer *buffer_dst);\n\n  /////////////////\n  // BLAS support\n\n  // See BlasSupport::DoBlasAsum.\n  Stream &ThenBlasAsum(uint64 elem_count, const DeviceMemory<float> &x,\n                       int incx, DeviceMemory<float> *result);\n  Stream &ThenBlasAsum(uint64 elem_count, const DeviceMemory<double> &x,\n                       int incx, DeviceMemory<double> *result);\n  Stream &ThenBlasAsum(uint64 elem_count,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       DeviceMemory<float> *result);\n  Stream &ThenBlasAsum(uint64 elem_count,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       DeviceMemory<double> *result);\n\n  // See BlasSupport::DoBlasAxpy. Note that, even for the case where alpha is\n  // present in DeviceMemory, it must be an execution-time constant (i.e. a\n  // value\n  // that the stream does not change or populate during the course of\n  // execution). The value is effectively captured at stream-enqueue time.\n  Stream &ThenBlasAxpy(uint64 elem_count, float alpha,\n                       const DeviceMemory<float> &x, int incx,\n                       DeviceMemory<float> *y, int incy);\n  Stream &ThenBlasAxpy(uint64 elem_count, double alpha,\n                       const DeviceMemory<double> &x, int incx,\n                       DeviceMemory<double> *y, int incy);\n  Stream &ThenBlasAxpy(uint64 elem_count, std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       DeviceMemory<std::complex<float>> *y, int incy);\n  Stream &ThenBlasAxpy(uint64 elem_count, std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       DeviceMemory<std::complex<double>> *y, int incy);\n\n  // See BlasSupport::DoBlasCopy.\n  Stream &ThenBlasCopy(uint64 elem_count, const DeviceMemory<float> &x,\n                       int incx, DeviceMemory<float> *y, int incy);\n  Stream &ThenBlasCopy(uint64 elem_count, const DeviceMemory<double> &x,\n                       int incx, DeviceMemory<double> *y, int incy);\n  Stream &ThenBlasCopy(uint64 elem_count,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       DeviceMemory<std::complex<float>> *y, int incy);\n  Stream &ThenBlasCopy(uint64 elem_count,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       DeviceMemory<std::complex<double>> *y, int incy);\n\n  // See BlasSupport::DoBlasDot.\n  Stream &ThenBlasDot(uint64 elem_count, const DeviceMemory<float> &x, int incx,\n                      const DeviceMemory<float> &y, int incy,\n                      DeviceMemory<float> *result);\n  Stream &ThenBlasDot(uint64 elem_count, const DeviceMemory<double> &x,\n                      int incx, const DeviceMemory<double> &y, int incy,\n                      DeviceMemory<double> *result);\n\n  // See BlasSupport::DoBlasDotc.\n  Stream &ThenBlasDotc(uint64 elem_count,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       const DeviceMemory<std::complex<float>> &y, int incy,\n                       DeviceMemory<std::complex<float>> *result);\n  Stream &ThenBlasDotc(uint64 elem_count,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       const DeviceMemory<std::complex<double>> &y, int incy,\n                       DeviceMemory<std::complex<double>> *result);\n\n  // See BlasSupport::DoBlasDotu.\n  Stream &ThenBlasDotu(uint64 elem_count,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       const DeviceMemory<std::complex<float>> &y, int incy,\n                       DeviceMemory<std::complex<float>> *result);\n  Stream &ThenBlasDotu(uint64 elem_count,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       const DeviceMemory<std::complex<double>> &y, int incy,\n                       DeviceMemory<std::complex<double>> *result);\n\n  // See BlasSupport::DoBlasNrm2.\n  Stream &ThenBlasNrm2(uint64 elem_count, const DeviceMemory<float> &x,\n                       int incx, DeviceMemory<float> *result);\n  Stream &ThenBlasNrm2(uint64 elem_count, const DeviceMemory<double> &x,\n                       int incx, DeviceMemory<double> *result);\n  Stream &ThenBlasNrm2(uint64 elem_count,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       DeviceMemory<float> *result);\n  Stream &ThenBlasNrm2(uint64 elem_count,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       DeviceMemory<double> *result);\n\n  // See BlasSupport::DoBlasRot.\n  Stream &ThenBlasRot(uint64 elem_count, DeviceMemory<float> *x, int incx,\n                      DeviceMemory<float> *y, int incy, float c, float s);\n  Stream &ThenBlasRot(uint64 elem_count, DeviceMemory<double> *x, int incx,\n                      DeviceMemory<double> *y, int incy, double c, double s);\n  Stream &ThenBlasRot(uint64 elem_count, DeviceMemory<std::complex<float>> *x,\n                      int incx, DeviceMemory<std::complex<float>> *y, int incy,\n                      float c, float s);\n  Stream &ThenBlasRot(uint64 elem_count, DeviceMemory<std::complex<double>> *x,\n                      int incx, DeviceMemory<std::complex<double>> *y, int incy,\n                      double c, double s);\n\n  // See BlasSupport::DoBlasRotg.\n  Stream &ThenBlasRotg(DeviceMemory<float> *a, DeviceMemory<float> *b,\n                       DeviceMemory<float> *c, DeviceMemory<float> *s);\n  Stream &ThenBlasRotg(DeviceMemory<double> *a, DeviceMemory<double> *b,\n                       DeviceMemory<double> *c, DeviceMemory<double> *s);\n  Stream &ThenBlasRotg(DeviceMemory<std::complex<float>> *a,\n                       DeviceMemory<std::complex<float>> *b,\n                       DeviceMemory<float> *c,\n                       DeviceMemory<std::complex<float>> *s);\n  Stream &ThenBlasRotg(DeviceMemory<std::complex<double>> *a,\n                       DeviceMemory<std::complex<double>> *b,\n                       DeviceMemory<double> *c,\n                       DeviceMemory<std::complex<double>> *s);\n\n  // See BlasSupport::DoBlasRotm.\n  Stream &ThenBlasRotm(uint64 elem_count, DeviceMemory<float> *x, int incx,\n                       DeviceMemory<float> *y, int incy,\n                       const DeviceMemory<float> &param);\n  Stream &ThenBlasRotm(uint64 elem_count, DeviceMemory<double> *x, int incx,\n                       DeviceMemory<double> *y, int incy,\n                       const DeviceMemory<double> &param);\n\n  // See BlasSupport::DoBlasRotmg.\n  Stream &ThenBlasRotmg(DeviceMemory<float> *d1, DeviceMemory<float> *d2,\n                        DeviceMemory<float> *x1, const DeviceMemory<float> &y1,\n                        DeviceMemory<float> *param);\n  Stream &ThenBlasRotmg(DeviceMemory<double> *d1, DeviceMemory<double> *d2,\n                        DeviceMemory<double> *x1,\n                        const DeviceMemory<double> &y1,\n                        DeviceMemory<double> *param);\n\n  // See BlasSupport::DoBlasScal.\n  Stream &ThenBlasScal(uint64 elem_count, float alpha, DeviceMemory<float> *x,\n                       int incx);\n  Stream &ThenBlasScal(uint64 elem_count, double alpha, DeviceMemory<double> *x,\n                       int incx);\n  Stream &ThenBlasScal(uint64 elem_count, float alpha,\n                       DeviceMemory<std::complex<float>> *x, int incx);\n  Stream &ThenBlasScal(uint64 elem_count, double alpha,\n                       DeviceMemory<std::complex<double>> *x, int incx);\n  Stream &ThenBlasScal(uint64 elem_count, std::complex<float> alpha,\n                       DeviceMemory<std::complex<float>> *x, int incx);\n  Stream &ThenBlasScal(uint64 elem_count, std::complex<double> alpha,\n                       DeviceMemory<std::complex<double>> *x, int incx);\n\n  // See BlasSupport::DoBlasSwap.\n  Stream &ThenBlasSwap(uint64 elem_count, DeviceMemory<float> *x, int incx,\n                       DeviceMemory<float> *y, int incy);\n  Stream &ThenBlasSwap(uint64 elem_count, DeviceMemory<double> *x, int incx,\n                       DeviceMemory<double> *y, int incy);\n  Stream &ThenBlasSwap(uint64 elem_count, DeviceMemory<std::complex<float>> *x,\n                       int incx, DeviceMemory<std::complex<float>> *y,\n                       int incy);\n  Stream &ThenBlasSwap(uint64 elem_count, DeviceMemory<std::complex<double>> *x,\n                       int incx, DeviceMemory<std::complex<double>> *y,\n                       int incy);\n\n  // See BlasSupport::DoBlasIamax.\n  Stream &ThenBlasIamax(uint64 elem_count, const DeviceMemory<float> &x,\n                        int incx, DeviceMemory<int> *result);\n  Stream &ThenBlasIamax(uint64 elem_count, const DeviceMemory<double> &x,\n                        int incx, DeviceMemory<int> *result);\n  Stream &ThenBlasIamax(uint64 elem_count,\n                        const DeviceMemory<std::complex<float>> &x, int incx,\n                        DeviceMemory<int> *result);\n  Stream &ThenBlasIamax(uint64 elem_count,\n                        const DeviceMemory<std::complex<double>> &x, int incx,\n                        DeviceMemory<int> *result);\n\n  // See BlasSupport::DoBlasIamin.\n  Stream &ThenBlasIamin(uint64 elem_count, const DeviceMemory<float> &x,\n                        int incx, DeviceMemory<int> *result);\n  Stream &ThenBlasIamin(uint64 elem_count, const DeviceMemory<double> &x,\n                        int incx, DeviceMemory<int> *result);\n  Stream &ThenBlasIamin(uint64 elem_count,\n                        const DeviceMemory<std::complex<float>> &x, int incx,\n                        DeviceMemory<int> *result);\n  Stream &ThenBlasIamin(uint64 elem_count,\n                        const DeviceMemory<std::complex<double>> &x, int incx,\n                        DeviceMemory<int> *result);\n\n  // See BlasSupport::DoBlasGbmv.\n  Stream &ThenBlasGbmv(blas::Transpose trans, uint64 m, uint64 n, uint64 kl,\n                       uint64 ku, float alpha, const DeviceMemory<float> &a,\n                       int lda, const DeviceMemory<float> &x, int incx,\n                       float beta, DeviceMemory<float> *y, int incy);\n  Stream &ThenBlasGbmv(blas::Transpose trans, uint64 m, uint64 n, uint64 kl,\n                       uint64 ku, double alpha, const DeviceMemory<double> &a,\n                       int lda, const DeviceMemory<double> &x, int incx,\n                       double beta, DeviceMemory<double> *y, int incy);\n  Stream &ThenBlasGbmv(blas::Transpose trans, uint64 m, uint64 n, uint64 kl,\n                       uint64 ku, std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       std::complex<float> beta,\n                       DeviceMemory<std::complex<float>> *y, int incy);\n  Stream &ThenBlasGbmv(blas::Transpose trans, uint64 m, uint64 n, uint64 kl,\n                       uint64 ku, std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       std::complex<double> beta,\n                       DeviceMemory<std::complex<double>> *y, int incy);\n\n  // See BlasSupport::DoBlasGemv.\n  Stream &ThenBlasGemv(blas::Transpose trans, uint64 m, uint64 n, float alpha,\n                       const DeviceMemory<float> &a, int lda,\n                       const DeviceMemory<float> &x, int incx, float beta,\n                       DeviceMemory<float> *y, int incy);\n  Stream &ThenBlasGemv(blas::Transpose trans, uint64 m, uint64 n, double alpha,\n                       const DeviceMemory<double> &a, int lda,\n                       const DeviceMemory<double> &x, int incx, double beta,\n                       DeviceMemory<double> *y, int incy);\n  Stream &ThenBlasGemv(blas::Transpose trans, uint64 m, uint64 n,\n                       std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       std::complex<float> beta,\n                       DeviceMemory<std::complex<float>> *y, int incy);\n  Stream &ThenBlasGemv(blas::Transpose trans, uint64 m, uint64 n,\n                       std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       std::complex<double> beta,\n                       DeviceMemory<std::complex<double>> *y, int incy);\n\n  Stream &ThenBlasGemvWithProfiling(blas::Transpose trans, uint64 m, uint64 n,\n                                    float alpha, const DeviceMemory<float> &a,\n                                    int lda, const DeviceMemory<float> &x,\n                                    int incx, float beta,\n                                    DeviceMemory<float> *y, int incy,\n                                    blas::ProfileResult *output_profile_result);\n  Stream &ThenBlasGemvWithProfiling(blas::Transpose trans, uint64 m, uint64 n,\n                                    double alpha, const DeviceMemory<double> &a,\n                                    int lda, const DeviceMemory<double> &x,\n                                    int incx, double beta,\n                                    DeviceMemory<double> *y, int incy,\n                                    blas::ProfileResult *output_profile_result);\n  Stream &ThenBlasGemvWithProfiling(\n      blas::Transpose trans, uint64 m, uint64 n, std::complex<float> alpha,\n      const DeviceMemory<std::complex<float>> &a, int lda,\n      const DeviceMemory<std::complex<float>> &x, int incx,\n      std::complex<float> beta, DeviceMemory<std::complex<float>> *y, int incy,\n      blas::ProfileResult *output_profile_result);\n  Stream &ThenBlasGemvWithProfiling(\n      blas::Transpose trans, uint64 m, uint64 n, std::complex<double> alpha,\n      const DeviceMemory<std::complex<double>> &a, int lda,\n      const DeviceMemory<std::complex<double>> &x, int incx,\n      std::complex<double> beta, DeviceMemory<std::complex<double>> *y,\n      int incy, blas::ProfileResult *output_profile_result);\n\n  // See BlasSupport::DoBlasGer.\n  Stream &ThenBlasGer(uint64 m, uint64 n, float alpha,\n                      const DeviceMemory<float> &x, int incx,\n                      const DeviceMemory<float> &y, int incy,\n                      DeviceMemory<float> *a, int lda);\n  Stream &ThenBlasGer(uint64 m, uint64 n, double alpha,\n                      const DeviceMemory<double> &x, int incx,\n                      const DeviceMemory<double> &y, int incy,\n                      DeviceMemory<double> *a, int lda);\n\n  // See BlasSupport::DoBlasGerc.\n  Stream &ThenBlasGerc(uint64 m, uint64 n, std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       const DeviceMemory<std::complex<float>> &y, int incy,\n                       DeviceMemory<std::complex<float>> *a, int lda);\n  Stream &ThenBlasGerc(uint64 m, uint64 n, std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       const DeviceMemory<std::complex<double>> &y, int incy,\n                       DeviceMemory<std::complex<double>> *a, int lda);\n\n  // See BlasSupport::DoBlasGeru.\n  Stream &ThenBlasGeru(uint64 m, uint64 n, std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       const DeviceMemory<std::complex<float>> &y, int incy,\n                       DeviceMemory<std::complex<float>> *a, int lda);\n  Stream &ThenBlasGeru(uint64 m, uint64 n, std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       const DeviceMemory<std::complex<double>> &y, int incy,\n                       DeviceMemory<std::complex<double>> *a, int lda);\n\n  // See BlasSupport::DoBlasHbmv.\n  Stream &ThenBlasHbmv(blas::UpperLower uplo, uint64 n, uint64 k,\n                       std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       std::complex<float> beta,\n                       DeviceMemory<std::complex<float>> *y, int incy);\n  Stream &ThenBlasHbmv(blas::UpperLower uplo, uint64 n, uint64 k,\n                       std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       std::complex<double> beta,\n                       DeviceMemory<std::complex<double>> *y, int incy);\n\n  // See BlasSupport::DoBlasHemv.\n  Stream &ThenBlasHemv(blas::UpperLower uplo, uint64 n,\n                       std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       std::complex<float> beta,\n                       DeviceMemory<std::complex<float>> *y, int incy);\n  Stream &ThenBlasHemv(blas::UpperLower uplo, uint64 n,\n                       std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       std::complex<double> beta,\n                       DeviceMemory<std::complex<double>> *y, int incy);\n\n  // See BlasSupport::DoBlasHer.\n  Stream &ThenBlasHer(blas::UpperLower uplo, uint64 n, float alpha,\n                      const DeviceMemory<std::complex<float>> &x, int incx,\n                      DeviceMemory<std::complex<float>> *a, int lda);\n  Stream &ThenBlasHer(blas::UpperLower uplo, uint64 n, double alpha,\n                      const DeviceMemory<std::complex<double>> &x, int incx,\n                      DeviceMemory<std::complex<double>> *a, int lda);\n\n  // See BlasSupport::DoBlasHer2.\n  Stream &ThenBlasHer2(blas::UpperLower uplo, uint64 n,\n                       std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       const DeviceMemory<std::complex<float>> &y, int incy,\n                       DeviceMemory<std::complex<float>> *a, int lda);\n  Stream &ThenBlasHer2(blas::UpperLower uplo, uint64 n,\n                       std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       const DeviceMemory<std::complex<double>> &y, int incy,\n                       DeviceMemory<std::complex<double>> *a, int lda);\n\n  // See BlasSupport::DoBlasHpmv.\n  Stream &ThenBlasHpmv(blas::UpperLower uplo, uint64 n,\n                       std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &ap,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       std::complex<float> beta,\n                       DeviceMemory<std::complex<float>> *y, int incy);\n  Stream &ThenBlasHpmv(blas::UpperLower uplo, uint64 n,\n                       std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &ap,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       std::complex<double> beta,\n                       DeviceMemory<std::complex<double>> *y, int incy);\n\n  // See BlasSupport::DoBlasHpr.\n  Stream &ThenBlasHpr(blas::UpperLower uplo, uint64 n, float alpha,\n                      const DeviceMemory<std::complex<float>> &x, int incx,\n                      DeviceMemory<std::complex<float>> *ap);\n  Stream &ThenBlasHpr(blas::UpperLower uplo, uint64 n, double alpha,\n                      const DeviceMemory<std::complex<double>> &x, int incx,\n                      DeviceMemory<std::complex<double>> *ap);\n\n  // See BlasSupport::DoBlasHpr2.\n  Stream &ThenBlasHpr2(blas::UpperLower uplo, uint64 n,\n                       std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &x, int incx,\n                       const DeviceMemory<std::complex<float>> &y, int incy,\n                       DeviceMemory<std::complex<float>> *ap);\n  Stream &ThenBlasHpr2(blas::UpperLower uplo, uint64 n,\n                       std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &x, int incx,\n                       const DeviceMemory<std::complex<double>> &y, int incy,\n                       DeviceMemory<std::complex<double>> *ap);\n\n  // See BlasSupport::DoBlasSbmv.\n  Stream &ThenBlasSbmv(blas::UpperLower uplo, uint64 n, uint64 k, float alpha,\n                       const DeviceMemory<float> &a, int lda,\n                       const DeviceMemory<float> &x, int incx, float beta,\n                       DeviceMemory<float> *y, int incy);\n  Stream &ThenBlasSbmv(blas::UpperLower uplo, uint64 n, uint64 k, double alpha,\n                       const DeviceMemory<double> &a, int lda,\n                       const DeviceMemory<double> &x, int incx, double beta,\n                       DeviceMemory<double> *y, int incy);\n\n  // See BlasSupport::DoBlasSpmv.\n  Stream &ThenBlasSpmv(blas::UpperLower uplo, uint64 n, float alpha,\n                       const DeviceMemory<float> &ap,\n                       const DeviceMemory<float> &x, int incx, float beta,\n                       DeviceMemory<float> *y, int incy);\n  Stream &ThenBlasSpmv(blas::UpperLower uplo, uint64 n, double alpha,\n                       const DeviceMemory<double> &ap,\n                       const DeviceMemory<double> &x, int incx, double beta,\n                       DeviceMemory<double> *y, int incy);\n\n  // See BlasSupport::DoBlasSpr.\n  Stream &ThenBlasSpr(blas::UpperLower uplo, uint64 n, float alpha,\n                      const DeviceMemory<float> &x, int incx,\n                      DeviceMemory<float> *ap);\n  Stream &ThenBlasSpr(blas::UpperLower uplo, uint64 n, double alpha,\n                      const DeviceMemory<double> &x, int incx,\n                      DeviceMemory<double> *ap);\n\n  // See BlasSupport::DoBlasSpr2.\n  Stream &ThenBlasSpr2(blas::UpperLower uplo, uint64 n, float alpha,\n                       const DeviceMemory<float> &x, int incx,\n                       const DeviceMemory<float> &y, int incy,\n                       DeviceMemory<float> *ap);\n  Stream &ThenBlasSpr2(blas::UpperLower uplo, uint64 n, double alpha,\n                       const DeviceMemory<double> &x, int incx,\n                       const DeviceMemory<double> &y, int incy,\n                       DeviceMemory<double> *ap);\n\n  // See BlasSupport::DoBlasSymv.\n  Stream &ThenBlasSymv(blas::UpperLower uplo, uint64 n, float alpha,\n                       const DeviceMemory<float> &a, int lda,\n                       const DeviceMemory<float> &x, int incx, float beta,\n                       DeviceMemory<float> *y, int incy);\n  Stream &ThenBlasSymv(blas::UpperLower uplo, uint64 n, double alpha,\n                       const DeviceMemory<double> &a, int lda,\n                       const DeviceMemory<double> &x, int incx, double beta,\n                       DeviceMemory<double> *y, int incy);\n\n  // See BlasSupport::DoBlasSyr.\n  Stream &ThenBlasSyr(blas::UpperLower uplo, uint64 n, float alpha,\n                      const DeviceMemory<float> &x, int incx,\n                      DeviceMemory<float> *a, int lda);\n  Stream &ThenBlasSyr(blas::UpperLower uplo, uint64 n, double alpha,\n                      const DeviceMemory<double> &x, int incx,\n                      DeviceMemory<double> *a, int lda);\n\n  // See BlasSupport::DoBlasSyr2.\n  Stream &ThenBlasSyr2(blas::UpperLower uplo, uint64 n, float alpha,\n                       const DeviceMemory<float> &x, int incx,\n                       const DeviceMemory<float> &y, int incy,\n                       DeviceMemory<float> *a, int lda);\n  Stream &ThenBlasSyr2(blas::UpperLower uplo, uint64 n, double alpha,\n                       const DeviceMemory<double> &x, int incx,\n                       const DeviceMemory<double> &y, int incy,\n                       DeviceMemory<double> *a, int lda);\n\n  // See BlasSupport::DoBlasTbmv.\n  Stream &ThenBlasTbmv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n, uint64 k,\n                       const DeviceMemory<float> &a, int lda,\n                       DeviceMemory<float> *x, int incx);\n  Stream &ThenBlasTbmv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n, uint64 k,\n                       const DeviceMemory<double> &a, int lda,\n                       DeviceMemory<double> *x, int incx);\n  Stream &ThenBlasTbmv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n, uint64 k,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       DeviceMemory<std::complex<float>> *x, int incx);\n  Stream &ThenBlasTbmv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n, uint64 k,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       DeviceMemory<std::complex<double>> *x, int incx);\n\n  // See BlasSupport::DoBlasTbsv.\n  Stream &ThenBlasTbsv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n, uint64 k,\n                       const DeviceMemory<float> &a, int lda,\n                       DeviceMemory<float> *x, int incx);\n  Stream &ThenBlasTbsv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n, uint64 k,\n                       const DeviceMemory<double> &a, int lda,\n                       DeviceMemory<double> *x, int incx);\n  Stream &ThenBlasTbsv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n, uint64 k,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       DeviceMemory<std::complex<float>> *x, int incx);\n  Stream &ThenBlasTbsv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n, uint64 k,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       DeviceMemory<std::complex<double>> *x, int incx);\n\n  // See BlasSupport::DoBlasTpmv.\n  Stream &ThenBlasTpmv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<float> &ap, DeviceMemory<float> *x,\n                       int incx);\n  Stream &ThenBlasTpmv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<double> &ap, DeviceMemory<double> *x,\n                       int incx);\n  Stream &ThenBlasTpmv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<std::complex<float>> &ap,\n                       DeviceMemory<std::complex<float>> *x, int incx);\n  Stream &ThenBlasTpmv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<std::complex<double>> &ap,\n                       DeviceMemory<std::complex<double>> *x, int incx);\n\n  // See BlasSupport::DoBlasTpsv.\n  Stream &ThenBlasTpsv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<float> &ap, DeviceMemory<float> *x,\n                       int incx);\n  Stream &ThenBlasTpsv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<double> &ap, DeviceMemory<double> *x,\n                       int incx);\n  Stream &ThenBlasTpsv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<std::complex<float>> &ap,\n                       DeviceMemory<std::complex<float>> *x, int incx);\n  Stream &ThenBlasTpsv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<std::complex<double>> &ap,\n                       DeviceMemory<std::complex<double>> *x, int incx);\n\n  // See BlasSupport::DoBlasTrmv.\n  Stream &ThenBlasTrmv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<float> &a, int lda,\n                       DeviceMemory<float> *x, int incx);\n  Stream &ThenBlasTrmv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<double> &a, int lda,\n                       DeviceMemory<double> *x, int incx);\n  Stream &ThenBlasTrmv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       DeviceMemory<std::complex<float>> *x, int incx);\n  Stream &ThenBlasTrmv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       DeviceMemory<std::complex<double>> *x, int incx);\n\n  // See BlasSupport::DoBlasTrsv.\n  Stream &ThenBlasTrsv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<float> &a, int lda,\n                       DeviceMemory<float> *x, int incx);\n  Stream &ThenBlasTrsv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<double> &a, int lda,\n                       DeviceMemory<double> *x, int incx);\n  Stream &ThenBlasTrsv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       DeviceMemory<std::complex<float>> *x, int incx);\n  Stream &ThenBlasTrsv(blas::UpperLower uplo, blas::Transpose trans,\n                       blas::Diagonal diag, uint64 n,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       DeviceMemory<std::complex<double>> *x, int incx);\n\n  // See BlasSupport::DoBlasGemm.\n  Stream &ThenBlasGemm(blas::Transpose transa, blas::Transpose transb, uint64 m,\n                       uint64 n, uint64 k, float alpha,\n                       const DeviceMemory<Eigen::half> &a, int lda,\n                       const DeviceMemory<Eigen::half> &b, int ldb, float beta,\n                       DeviceMemory<Eigen::half> *c, int ldc);\n  Stream &ThenBlasGemm(blas::Transpose transa, blas::Transpose transb, uint64 m,\n                       uint64 n, uint64 k, float alpha,\n                       const DeviceMemory<float> &a, int lda,\n                       const DeviceMemory<float> &b, int ldb, float beta,\n                       DeviceMemory<float> *c, int ldc);\n  Stream &ThenBlasGemm(blas::Transpose transa, blas::Transpose transb, uint64 m,\n                       uint64 n, uint64 k, double alpha,\n                       const DeviceMemory<double> &a, int lda,\n                       const DeviceMemory<double> &b, int ldb, double beta,\n                       DeviceMemory<double> *c, int ldc);\n  Stream &ThenBlasGemm(blas::Transpose transa, blas::Transpose transb, uint64 m,\n                       uint64 n, uint64 k, std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       const DeviceMemory<std::complex<float>> &b, int ldb,\n                       std::complex<float> beta,\n                       DeviceMemory<std::complex<float>> *c, int ldc);\n  Stream &ThenBlasGemm(blas::Transpose transa, blas::Transpose transb, uint64 m,\n                       uint64 n, uint64 k, std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       const DeviceMemory<std::complex<double>> &b, int ldb,\n                       std::complex<double> beta,\n                       DeviceMemory<std::complex<double>> *c, int ldc);\n\n  Stream &ThenBlasGemmWithProfiling(blas::Transpose transa,\n                                    blas::Transpose transb, uint64 m, uint64 n,\n                                    uint64 k, float alpha,\n                                    const DeviceMemory<Eigen::half> &a, int lda,\n                                    const DeviceMemory<Eigen::half> &b, int ldb,\n                                    float beta, DeviceMemory<Eigen::half> *c,\n                                    int ldc,\n                                    blas::ProfileResult *output_profile_result);\n  Stream &ThenBlasGemmWithProfiling(blas::Transpose transa,\n                                    blas::Transpose transb, uint64 m, uint64 n,\n                                    uint64 k, float alpha,\n                                    const DeviceMemory<float> &a, int lda,\n                                    const DeviceMemory<float> &b, int ldb,\n                                    float beta, DeviceMemory<float> *c, int ldc,\n                                    blas::ProfileResult *output_profile_result);\n  Stream &ThenBlasGemmWithProfiling(blas::Transpose transa,\n                                    blas::Transpose transb, uint64 m, uint64 n,\n                                    uint64 k, double alpha,\n                                    const DeviceMemory<double> &a, int lda,\n                                    const DeviceMemory<double> &b, int ldb,\n                                    double beta, DeviceMemory<double> *c,\n                                    int ldc,\n                                    blas::ProfileResult *output_profile_result);\n  Stream &ThenBlasGemmWithProfiling(\n      blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n,\n      uint64 k, std::complex<float> alpha,\n      const DeviceMemory<std::complex<float>> &a, int lda,\n      const DeviceMemory<std::complex<float>> &b, int ldb,\n      std::complex<float> beta, DeviceMemory<std::complex<float>> *c, int ldc,\n      blas::ProfileResult *output_profile_result);\n  Stream &ThenBlasGemmWithProfiling(\n      blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n,\n      uint64 k, std::complex<double> alpha,\n      const DeviceMemory<std::complex<double>> &a, int lda,\n      const DeviceMemory<std::complex<double>> &b, int ldb,\n      std::complex<double> beta, DeviceMemory<std::complex<double>> *c, int ldc,\n      blas::ProfileResult *output_profile_result);\n\n  // See BlasSupport::DoBlasGemmWithAlgorithm.\n  Stream &ThenBlasGemmWithAlgorithm(\n      blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n,\n      uint64 k, const Eigen::half &alpha, const DeviceMemory<Eigen::half> &a,\n      int lda, const DeviceMemory<Eigen::half> &b, int ldb,\n      const Eigen::half &beta, DeviceMemory<Eigen::half> *c, int ldc,\n      blas::ComputationType computation_type, blas::AlgorithmType algorithm,\n      blas::ProfileResult *output_profile_result);\n  Stream &ThenBlasGemmWithAlgorithm(blas::Transpose transa,\n                                    blas::Transpose transb, uint64 m, uint64 n,\n                                    uint64 k, int alpha,\n                                    const DeviceMemory<int8> &a, int lda,\n                                    const DeviceMemory<int8> &b, int ldb,\n                                    int beta, DeviceMemory<int> *c, int ldc,\n                                    blas::ComputationType computation_type,\n                                    blas::AlgorithmType algorithm,\n                                    blas::ProfileResult *output_profile_result);\n  Stream &ThenBlasGemmWithAlgorithm(blas::Transpose transa,\n                                    blas::Transpose transb, uint64 m, uint64 n,\n                                    uint64 k, float alpha,\n                                    const DeviceMemory<float> &a, int lda,\n                                    const DeviceMemory<float> &b, int ldb,\n                                    float beta, DeviceMemory<float> *c, int ldc,\n                                    blas::ComputationType computation_type,\n                                    blas::AlgorithmType algorithm,\n                                    blas::ProfileResult *output_profile_result);\n  Stream &ThenBlasGemmWithAlgorithm(\n      blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n,\n      uint64 k, double alpha, const DeviceMemory<double> &a, int lda,\n      const DeviceMemory<double> &b, int ldb, double beta,\n      DeviceMemory<double> *c, int ldc, blas::ComputationType computation_type,\n      blas::AlgorithmType algorithm,\n      blas::ProfileResult *output_profile_result);\n  Stream &ThenBlasGemmWithAlgorithm(\n      blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n,\n      uint64 k, std::complex<float> alpha,\n      const DeviceMemory<std::complex<float>> &a, int lda,\n      const DeviceMemory<std::complex<float>> &b, int ldb,\n      std::complex<float> beta, DeviceMemory<std::complex<float>> *c, int ldc,\n      blas::ComputationType computation_type, blas::AlgorithmType algorithm,\n      blas::ProfileResult *output_profile_result);\n  Stream &ThenBlasGemmWithAlgorithm(\n      blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n,\n      uint64 k, std::complex<double> alpha,\n      const DeviceMemory<std::complex<double>> &a, int lda,\n      const DeviceMemory<std::complex<double>> &b, int ldb,\n      std::complex<double> beta, DeviceMemory<std::complex<double>> *c, int ldc,\n      blas::ComputationType computation_type, blas::AlgorithmType algorithm,\n      blas::ProfileResult *output_profile_result);\n\n  // See BlasSupport::DoBlasGemmBatched.\n  Stream &ThenBlasGemmBatched(blas::Transpose transa, blas::Transpose transb,\n                              uint64 m, uint64 n, uint64 k, float alpha,\n                              const port::ArraySlice<DeviceMemory<float> *> &a,\n                              int lda,\n                              const port::ArraySlice<DeviceMemory<float> *> &b,\n                              int ldb, float beta,\n                              const port::ArraySlice<DeviceMemory<float> *> &c,\n                              int ldc, int batch_count);\n  Stream &ThenBlasGemmBatched(blas::Transpose transa, blas::Transpose transb,\n                              uint64 m, uint64 n, uint64 k, double alpha,\n                              const port::ArraySlice<DeviceMemory<double> *> &a,\n                              int lda,\n                              const port::ArraySlice<DeviceMemory<double> *> &b,\n                              int ldb, double beta,\n                              const port::ArraySlice<DeviceMemory<double> *> &c,\n                              int ldc, int batch_count);\n  Stream &ThenBlasGemmBatched(\n      blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n,\n      uint64 k, std::complex<float> alpha,\n      const port::ArraySlice<DeviceMemory<std::complex<float>> *> &a, int lda,\n      const port::ArraySlice<DeviceMemory<std::complex<float>> *> &b, int ldb,\n      std::complex<float> beta,\n      const port::ArraySlice<DeviceMemory<std::complex<float>> *> &c, int ldc,\n      int batch_count);\n  Stream &ThenBlasGemmBatched(\n      blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n,\n      uint64 k, std::complex<double> alpha,\n      const port::ArraySlice<DeviceMemory<std::complex<double>> *> &a, int lda,\n      const port::ArraySlice<DeviceMemory<std::complex<double>> *> &b, int ldb,\n      std::complex<double> beta,\n      const port::ArraySlice<DeviceMemory<std::complex<double>> *> &c, int ldc,\n      int batch_count);\n  Stream &ThenBlasGemmBatchedWithScratch(\n      blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n,\n      uint64 k, float alpha, const port::ArraySlice<DeviceMemory<float> *> &a,\n      int lda, const port::ArraySlice<DeviceMemory<float> *> &b, int ldb,\n      float beta, const port::ArraySlice<DeviceMemory<float> *> &c, int ldc,\n      int batch_count, ScratchAllocator *scratch_allocator);\n  Stream &ThenBlasGemmBatchedWithScratch(\n      blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n,\n      uint64 k, double alpha, const port::ArraySlice<DeviceMemory<double> *> &a,\n      int lda, const port::ArraySlice<DeviceMemory<double> *> &b, int ldb,\n      double beta, const port::ArraySlice<DeviceMemory<double> *> &c, int ldc,\n      int batch_count, ScratchAllocator *scratch_allocator);\n  Stream &ThenBlasGemmBatchedWithScratch(\n      blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n,\n      uint64 k, std::complex<float> alpha,\n      const port::ArraySlice<DeviceMemory<std::complex<float>> *> &a, int lda,\n      const port::ArraySlice<DeviceMemory<std::complex<float>> *> &b, int ldb,\n      std::complex<float> beta,\n      const port::ArraySlice<DeviceMemory<std::complex<float>> *> &c, int ldc,\n      int batch_count, ScratchAllocator *scratch_allocator);\n  Stream &ThenBlasGemmBatchedWithScratch(\n      blas::Transpose transa, blas::Transpose transb, uint64 m, uint64 n,\n      uint64 k, std::complex<double> alpha,\n      const port::ArraySlice<DeviceMemory<std::complex<double>> *> &a, int lda,\n      const port::ArraySlice<DeviceMemory<std::complex<double>> *> &b, int ldb,\n      std::complex<double> beta,\n      const port::ArraySlice<DeviceMemory<std::complex<double>> *> &c, int ldc,\n      int batch_count, ScratchAllocator *scratch_allocator);\n\n  // See BlasSupport::DoBlasHemm.\n  Stream &ThenBlasHemm(blas::Side side, blas::UpperLower uplo, uint64 m,\n                       uint64 n, std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       const DeviceMemory<std::complex<float>> &b, int ldb,\n                       std::complex<float> beta,\n                       DeviceMemory<std::complex<float>> *c, int ldc);\n  Stream &ThenBlasHemm(blas::Side side, blas::UpperLower uplo, uint64 m,\n                       uint64 n, std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       const DeviceMemory<std::complex<double>> &b, int ldb,\n                       std::complex<double> beta,\n                       DeviceMemory<std::complex<double>> *c, int ldc);\n\n  // See BlasSupport::DoBlasHerk.\n  Stream &ThenBlasHerk(blas::UpperLower uplo, blas::Transpose trans, uint64 n,\n                       uint64 k, float alpha,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       float beta, DeviceMemory<std::complex<float>> *c,\n                       int ldc);\n  Stream &ThenBlasHerk(blas::UpperLower uplo, blas::Transpose trans, uint64 n,\n                       uint64 k, double alpha,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       double beta, DeviceMemory<std::complex<double>> *c,\n                       int ldc);\n\n  // See BlasSupport::DoBlasHer2k.\n  Stream &ThenBlasHer2k(blas::UpperLower uplo, blas::Transpose trans, uint64 n,\n                        uint64 k, std::complex<float> alpha,\n                        const DeviceMemory<std::complex<float>> &a, int lda,\n                        const DeviceMemory<std::complex<float>> &b, int ldb,\n                        float beta, DeviceMemory<std::complex<float>> *c,\n                        int ldc);\n  Stream &ThenBlasHer2k(blas::UpperLower uplo, blas::Transpose trans, uint64 n,\n                        uint64 k, std::complex<double> alpha,\n                        const DeviceMemory<std::complex<double>> &a, int lda,\n                        const DeviceMemory<std::complex<double>> &b, int ldb,\n                        double beta, DeviceMemory<std::complex<double>> *c,\n                        int ldc);\n\n  // See BlasSupport::DoBlasSymm.\n  Stream &ThenBlasSymm(blas::Side side, blas::UpperLower uplo, uint64 m,\n                       uint64 n, float alpha, const DeviceMemory<float> &a,\n                       int lda, const DeviceMemory<float> &b, int ldb,\n                       float beta, DeviceMemory<float> *c, int ldc);\n  Stream &ThenBlasSymm(blas::Side side, blas::UpperLower uplo, uint64 m,\n                       uint64 n, double alpha, const DeviceMemory<double> &a,\n                       int lda, const DeviceMemory<double> &b, int ldb,\n                       double beta, DeviceMemory<double> *c, int ldc);\n  Stream &ThenBlasSymm(blas::Side side, blas::UpperLower uplo, uint64 m,\n                       uint64 n, std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       const DeviceMemory<std::complex<float>> &b, int ldb,\n                       std::complex<float> beta,\n                       DeviceMemory<std::complex<float>> *c, int ldc);\n  Stream &ThenBlasSymm(blas::Side side, blas::UpperLower uplo, uint64 m,\n                       uint64 n, std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       const DeviceMemory<std::complex<double>> &b, int ldb,\n                       std::complex<double> beta,\n                       DeviceMemory<std::complex<double>> *c, int ldc);\n\n  // See BlasSupport::DoBlasSyrk.\n  Stream &ThenBlasSyrk(blas::UpperLower uplo, blas::Transpose trans, uint64 n,\n                       uint64 k, float alpha, const DeviceMemory<float> &a,\n                       int lda, float beta, DeviceMemory<float> *c, int ldc);\n  Stream &ThenBlasSyrk(blas::UpperLower uplo, blas::Transpose trans, uint64 n,\n                       uint64 k, double alpha, const DeviceMemory<double> &a,\n                       int lda, double beta, DeviceMemory<double> *c, int ldc);\n  Stream &ThenBlasSyrk(blas::UpperLower uplo, blas::Transpose trans, uint64 n,\n                       uint64 k, std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       std::complex<float> beta,\n                       DeviceMemory<std::complex<float>> *c, int ldc);\n  Stream &ThenBlasSyrk(blas::UpperLower uplo, blas::Transpose trans, uint64 n,\n                       uint64 k, std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       std::complex<double> beta,\n                       DeviceMemory<std::complex<double>> *c, int ldc);\n\n  // See BlasSupport::DoBlasSyr2k.\n  Stream &ThenBlasSyr2k(blas::UpperLower uplo, blas::Transpose trans, uint64 n,\n                        uint64 k, float alpha, const DeviceMemory<float> &a,\n                        int lda, const DeviceMemory<float> &b, int ldb,\n                        float beta, DeviceMemory<float> *c, int ldc);\n  Stream &ThenBlasSyr2k(blas::UpperLower uplo, blas::Transpose trans, uint64 n,\n                        uint64 k, double alpha, const DeviceMemory<double> &a,\n                        int lda, const DeviceMemory<double> &b, int ldb,\n                        double beta, DeviceMemory<double> *c, int ldc);\n  Stream &ThenBlasSyr2k(blas::UpperLower uplo, blas::Transpose trans, uint64 n,\n                        uint64 k, std::complex<float> alpha,\n                        const DeviceMemory<std::complex<float>> &a, int lda,\n                        const DeviceMemory<std::complex<float>> &b, int ldb,\n                        std::complex<float> beta,\n                        DeviceMemory<std::complex<float>> *c, int ldc);\n  Stream &ThenBlasSyr2k(blas::UpperLower uplo, blas::Transpose trans, uint64 n,\n                        uint64 k, std::complex<double> alpha,\n                        const DeviceMemory<std::complex<double>> &a, int lda,\n                        const DeviceMemory<std::complex<double>> &b, int ldb,\n                        std::complex<double> beta,\n                        DeviceMemory<std::complex<double>> *c, int ldc);\n\n  // See BlasSupport::DoBlasTrmm.\n  Stream &ThenBlasTrmm(blas::Side side, blas::UpperLower uplo,\n                       blas::Transpose transa, blas::Diagonal diag, uint64 m,\n                       uint64 n, float alpha, const DeviceMemory<float> &a,\n                       int lda, DeviceMemory<float> *b, int ldb);\n  Stream &ThenBlasTrmm(blas::Side side, blas::UpperLower uplo,\n                       blas::Transpose transa, blas::Diagonal diag, uint64 m,\n                       uint64 n, double alpha, const DeviceMemory<double> &a,\n                       int lda, DeviceMemory<double> *b, int ldb);\n  Stream &ThenBlasTrmm(blas::Side side, blas::UpperLower uplo,\n                       blas::Transpose transa, blas::Diagonal diag, uint64 m,\n                       uint64 n, std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       DeviceMemory<std::complex<float>> *b, int ldb);\n  Stream &ThenBlasTrmm(blas::Side side, blas::UpperLower uplo,\n                       blas::Transpose transa, blas::Diagonal diag, uint64 m,\n                       uint64 n, std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       DeviceMemory<std::complex<double>> *b, int ldb);\n\n  // See BlasSupport::DoBlasTrsm.\n  Stream &ThenBlasTrsm(blas::Side side, blas::UpperLower uplo,\n                       blas::Transpose transa, blas::Diagonal diag, uint64 m,\n                       uint64 n, float alpha, const DeviceMemory<float> &a,\n                       int lda, DeviceMemory<float> *b, int ldb);\n  Stream &ThenBlasTrsm(blas::Side side, blas::UpperLower uplo,\n                       blas::Transpose transa, blas::Diagonal diag, uint64 m,\n                       uint64 n, double alpha, const DeviceMemory<double> &a,\n                       int lda, DeviceMemory<double> *b, int ldb);\n  Stream &ThenBlasTrsm(blas::Side side, blas::UpperLower uplo,\n                       blas::Transpose transa, blas::Diagonal diag, uint64 m,\n                       uint64 n, std::complex<float> alpha,\n                       const DeviceMemory<std::complex<float>> &a, int lda,\n                       DeviceMemory<std::complex<float>> *b, int ldb);\n  Stream &ThenBlasTrsm(blas::Side side, blas::UpperLower uplo,\n                       blas::Transpose transa, blas::Diagonal diag, uint64 m,\n                       uint64 n, std::complex<double> alpha,\n                       const DeviceMemory<std::complex<double>> &a, int lda,\n                       DeviceMemory<std::complex<double>> *b, int ldb);\n\n  // See FftSupport::DoFft.\n  Stream &ThenFft(fft::Plan *plan,\n                  const DeviceMemory<std::complex<float>> &input,\n                  DeviceMemory<std::complex<float>> *output);\n  Stream &ThenFft(fft::Plan *plan,\n                  const DeviceMemory<std::complex<double>> &input,\n                  DeviceMemory<std::complex<double>> *output);\n  Stream &ThenFft(fft::Plan *plan, const DeviceMemory<float> &input,\n                  DeviceMemory<std::complex<float>> *output);\n  Stream &ThenFft(fft::Plan *plan, const DeviceMemory<double> &input,\n                  DeviceMemory<std::complex<double>> *output);\n  Stream &ThenFft(fft::Plan *plan,\n                  const DeviceMemory<std::complex<float>> &input,\n                  DeviceMemory<float> *output);\n  Stream &ThenFft(fft::Plan *plan,\n                  const DeviceMemory<std::complex<double>> &input,\n                  DeviceMemory<double> *output);\n\n  // Makes the RNG use the provided value as the basis for further generation.\n  // /dev/urandom (good) and /dev/random (better, but sometimes slow) are good\n  // sources of seed data if the default (high quality) sources are not\n  // desired.\n  // For most use cases, this function will not be necessary; each provided\n  // back-end implementation will be appropriately seeded by default.\n  // At a minimum 16 bytes of data are required in the seed buffer.\n  //\n  // To seed with good (non-reproducible) data:\n  //   File* f = File::Open(\"/dev/random\", \"r\");\n  //   int64 bytes_read = f->Read(seed_data, bytes_to_read);\n  //   < error checking >\n  //   stream.ThenSetRngSeed(seed_data, bytes_read);\n  //\n  // To seed with reproducible data:\n  //   uint64_t seed_data[2] = { <data> };\n  //   stream.ThenSetRngSeed(seed_data, 16);\n  Stream &ThenSetRngSeed(const uint8 *seed, uint64 seed_bytes);\n\n  // Populates the memory indicated by values with uniform-random-distribution\n  // values. TODO(leary) seeding API/description\n  //\n  // Uses the type and size of the DeviceMemory to infer what data should be\n  // populated.\n  Stream &ThenPopulateRandUniform(DeviceMemory<float> *values);\n  Stream &ThenPopulateRandUniform(DeviceMemory<double> *values);\n  Stream &ThenPopulateRandUniform(DeviceMemory<std::complex<float>> *values);\n  Stream &ThenPopulateRandUniform(DeviceMemory<std::complex<double>> *values);\n  Stream &ThenPopulateRandGaussian(float mean, float stddev,\n                                   DeviceMemory<float> *values);\n  Stream &ThenPopulateRandGaussian(double mean, double stddev,\n                                   DeviceMemory<double> *values);\n\n  // Entrain onto the stream: a memcpy to a host destination from a GPU source\n  // of the given target size. host_dst must be a pointer to host memory\n  // allocated by StreamExecutor::HostMemoryAllocate or otherwise allocated and\n  // then registered with StreamExecutor::HostMemoryRegister.\n  Stream &ThenMemcpy(void *host_dst, const DeviceMemoryBase &gpu_src,\n                     uint64 size);\n\n  // Entrain onto the stream: a memcpy to a GPU destination from a host source\n  // of the given target size. host_src must be a pointer to host memory\n  // allocated by StreamExecutor::HostMemoryAllocate or otherwise allocated and\n  // then registered with StreamExecutor::HostMemoryRegister.\n  Stream &ThenMemcpy(DeviceMemoryBase *gpu_dst, const void *host_src,\n                     uint64 size);\n\n  // Alternative interface for memcpying from device to host that takes an\n  // array slice. Checks that the destination size can accommodate the host\n  // slice size.\n  template <typename T>\n  Stream &ThenMemcpyD2H(const DeviceMemory<T> &gpu_src,\n                        port::MutableArraySlice<T> host_dst) {\n    auto host_size = host_dst.size() * sizeof(T);\n    CHECK(gpu_src.size() == 0 || host_size >= gpu_src.size());\n    return ThenMemcpy(host_dst.begin(), gpu_src, host_size);\n  }\n\n  // Alternative interface for memcpying from host to device that takes an\n  // array slice. Checks that the destination size can accommodate the host\n  // slice size.\n  template <typename T>\n  Stream &ThenMemcpyH2D(port::ArraySlice<T> host_src,\n                        DeviceMemory<T> *gpu_dst) {\n    auto host_size = host_src.size() * sizeof(T);\n    CHECK(gpu_dst->size() == 0 || gpu_dst->size() >= host_size);\n    return ThenMemcpy(gpu_dst, host_src.begin(), host_size);\n  }\n\n  // Entrain onto the stream: a memcpy to a GPU destination from a GPU source\n  // of the given target size. gpu_src/dst must be pointers to GPU memory and\n  // peer access must be enabled between their owning StreamExecutors.\n  Stream &ThenMemcpy(DeviceMemoryBase *gpu_dst, const DeviceMemoryBase &gpu_src,\n                     uint64 size);\n\n  // Calls to the device-to-device copy overload of ThenMemcpy -- useful for\n  // ensuring that the host pointer isn't getting confused accidentally with a\n  // device pointer if you're not doing metaprogramming against the API.\n  Stream &ThenMemcpyD2D(DeviceMemoryBase *gpu_dst,\n                        const DeviceMemoryBase &gpu_src, uint64 size) {\n    return ThenMemcpy(gpu_dst, gpu_src, size);\n  }\n\n  // Entrain onto the stream: a memset of zero at a GPU location of size bytes.\n  // The location must not be null.\n  Stream &ThenMemZero(DeviceMemoryBase *location, uint64 size);\n\n  // Entrain onto the stream: a memset of a 32-bit pattern at a GPU location of\n  // size bytes, where bytes must be evenly 32-bit sized (i.e. evenly divisible\n  // by 4). The location must not be null.\n  Stream &ThenMemset32(DeviceMemoryBase *location, uint32 pattern, uint64 size);\n\n  // Enqueue a forward operation of the RNN model onto the stream.\n  // See DnnSupport::DoRnnForward for more details.\n  Stream &ThenRnnForward(const dnn::RnnDescriptor &rnn_desc,\n                         const dnn::RnnSequenceTensorDescriptor &input_desc,\n                         const DeviceMemory<Eigen::half> &input_data,\n                         const dnn::RnnStateTensorDescriptor &input_h_desc,\n                         const DeviceMemory<Eigen::half> &input_h_data,\n                         const dnn::RnnStateTensorDescriptor &input_c_desc,\n                         const DeviceMemory<Eigen::half> &input_c_data,\n                         const DeviceMemory<Eigen::half> &params,\n                         const dnn::RnnSequenceTensorDescriptor &output_desc,\n                         DeviceMemory<Eigen::half> *output_data,\n                         const dnn::RnnStateTensorDescriptor &output_h_desc,\n                         DeviceMemory<Eigen::half> *output_h_data,\n                         const dnn::RnnStateTensorDescriptor &output_c_desc,\n                         DeviceMemory<Eigen::half> *output_c_data,\n                         bool is_training,\n                         ScratchAllocator *reserve_space_allocator,\n                         ScratchAllocator *workspace_allocator);\n\n  Stream &ThenRnnForward(const dnn::RnnDescriptor &rnn_desc,\n                         const dnn::RnnSequenceTensorDescriptor &input_desc,\n                         const DeviceMemory<float> &input_data,\n                         const dnn::RnnStateTensorDescriptor &input_h_desc,\n                         const DeviceMemory<float> &input_h_data,\n                         const dnn::RnnStateTensorDescriptor &input_c_desc,\n                         const DeviceMemory<float> &input_c_data,\n                         const DeviceMemory<float> &params,\n                         const dnn::RnnSequenceTensorDescriptor &output_desc,\n                         DeviceMemory<float> *output_data,\n                         const dnn::RnnStateTensorDescriptor &output_h_desc,\n                         DeviceMemory<float> *output_h_data,\n                         const dnn::RnnStateTensorDescriptor &output_c_desc,\n                         DeviceMemory<float> *output_c_data, bool is_training,\n                         ScratchAllocator *reserve_space_allocator,\n                         ScratchAllocator *workspace_allocator);\n\n  Stream &ThenRnnForward(const dnn::RnnDescriptor &rnn_desc,\n                         const dnn::RnnSequenceTensorDescriptor &input_desc,\n                         const DeviceMemory<double> &input_data,\n                         const dnn::RnnStateTensorDescriptor &input_h_desc,\n                         const DeviceMemory<double> &input_h_data,\n                         const dnn::RnnStateTensorDescriptor &input_c_desc,\n                         const DeviceMemory<double> &input_c_data,\n                         const DeviceMemory<double> &params,\n                         const dnn::RnnSequenceTensorDescriptor &output_desc,\n                         DeviceMemory<double> *output_data,\n                         const dnn::RnnStateTensorDescriptor &output_h_desc,\n                         DeviceMemory<double> *output_h_data,\n                         const dnn::RnnStateTensorDescriptor &output_c_desc,\n                         DeviceMemory<double> *output_c_data, bool is_training,\n                         ScratchAllocator *reserve_space_allocator,\n                         ScratchAllocator *workspace_allocator);\n\n  // Enqueue a backward operation of the RNN model onto the stream.\n  // See DnnSupport::DoRnnBackward for more details.\n  Stream &ThenRnnBackward(\n      const dnn::RnnDescriptor &rnn_desc,\n      const dnn::RnnSequenceTensorDescriptor &input_desc,\n      const DeviceMemory<Eigen::half> &input_data,\n      const dnn::RnnStateTensorDescriptor &input_h_desc,\n      const DeviceMemory<Eigen::half> &input_h_data,\n      const dnn::RnnStateTensorDescriptor &input_c_desc,\n      const DeviceMemory<Eigen::half> &input_c_data,\n      const DeviceMemory<Eigen::half> &params,\n      const dnn::RnnSequenceTensorDescriptor &output_desc,\n      const DeviceMemory<Eigen::half> &output_data,\n      const dnn::RnnStateTensorDescriptor &output_h_desc,\n      const DeviceMemory<Eigen::half> &output_h_data,\n      const dnn::RnnStateTensorDescriptor &output_c_desc,\n      const DeviceMemory<Eigen::half> &output_c_data,\n      const DeviceMemory<Eigen::half> &output_backprop_data,\n      const DeviceMemory<Eigen::half> &output_h_backprop_data,\n      const DeviceMemory<Eigen::half> &output_c_backprop_data,\n      DeviceMemory<Eigen::half> *input_backprop_data,\n      DeviceMemory<Eigen::half> *input_h_backprop_data,\n      DeviceMemory<Eigen::half> *input_c_backprop_data,\n      DeviceMemory<Eigen::half> *params_backprop_data,\n      DeviceMemory<uint8> *reserve_space_data,\n      ScratchAllocator *workspace_allocator);\n\n  Stream &ThenRnnBackward(const dnn::RnnDescriptor &rnn_desc,\n                          const dnn::RnnSequenceTensorDescriptor &input_desc,\n                          const DeviceMemory<float> &input_data,\n                          const dnn::RnnStateTensorDescriptor &input_h_desc,\n                          const DeviceMemory<float> &input_h_data,\n                          const dnn::RnnStateTensorDescriptor &input_c_desc,\n                          const DeviceMemory<float> &input_c_data,\n                          const DeviceMemory<float> &params,\n                          const dnn::RnnSequenceTensorDescriptor &output_desc,\n                          const DeviceMemory<float> &output_data,\n                          const dnn::RnnStateTensorDescriptor &output_h_desc,\n                          const DeviceMemory<float> &output_h_data,\n                          const dnn::RnnStateTensorDescriptor &output_c_desc,\n                          const DeviceMemory<float> &output_c_data,\n                          const DeviceMemory<float> &output_backprop_data,\n                          const DeviceMemory<float> &output_h_backprop_data,\n                          const DeviceMemory<float> &output_c_backprop_data,\n                          DeviceMemory<float> *input_backprop_data,\n                          DeviceMemory<float> *input_h_backprop_data,\n                          DeviceMemory<float> *input_c_backprop_data,\n                          DeviceMemory<float> *params_backprop_data,\n                          DeviceMemory<uint8> *reserve_space_data,\n                          ScratchAllocator *workspace_allocator);\n\n  Stream &ThenRnnBackward(const dnn::RnnDescriptor &rnn_desc,\n                          const dnn::RnnSequenceTensorDescriptor &input_desc,\n                          const DeviceMemory<double> &input_data,\n                          const dnn::RnnStateTensorDescriptor &input_h_desc,\n                          const DeviceMemory<double> &input_h_data,\n                          const dnn::RnnStateTensorDescriptor &input_c_desc,\n                          const DeviceMemory<double> &input_c_data,\n                          const DeviceMemory<double> &params,\n                          const dnn::RnnSequenceTensorDescriptor &output_desc,\n                          const DeviceMemory<double> &output_data,\n                          const dnn::RnnStateTensorDescriptor &output_h_desc,\n                          const DeviceMemory<double> &output_h_data,\n                          const dnn::RnnStateTensorDescriptor &output_c_desc,\n                          const DeviceMemory<double> &output_c_data,\n                          const DeviceMemory<double> &output_backprop_data,\n                          const DeviceMemory<double> &output_h_backprop_data,\n                          const DeviceMemory<double> &output_c_backprop_data,\n                          DeviceMemory<double> *input_backprop_data,\n                          DeviceMemory<double> *input_h_backprop_data,\n                          DeviceMemory<double> *input_c_backprop_data,\n                          DeviceMemory<double> *params_backprop_data,\n                          DeviceMemory<uint8> *reserve_space_data,\n                          ScratchAllocator *workspace_allocator);\n\n  // Enqueue onto the stream a operation that transforms a tensor.\n  // See DnnSupport::DoTransformTensor for more details.\n  Stream &ThenTransformTensor(const dnn::BatchDescriptor &input_desc,\n                              dnn::DataType input_type,\n                              const DeviceMemoryBase &input_data,\n                              const dnn::BatchDescriptor &output_desc,\n                              dnn::DataType output_type, float scale,\n                              DeviceMemoryBase *output_data);\n\n  // The templated version of the above ThenTransformTensor. Useful when the\n  // input and output types are statically known.\n  template <typename InElemT, typename OutElemT>\n  Stream &ThenTransformTensor(const dnn::BatchDescriptor &input_desc,\n                              const DeviceMemory<InElemT> &input_data,\n                              const dnn::BatchDescriptor &output_desc,\n                              DeviceMemory<OutElemT> *output_data) {\n    return ThenTransformTensor(input_desc, dnn::ToDataType<InElemT>(),\n                               input_data, output_desc,\n                               dnn::ToDataType<OutElemT>(), output_data);\n  }\n\n  // (Synchronously) block the host code waiting for the operations\n  // entrained on the stream (enqueued to this point in program\n  // execution) to complete.\n  //\n  // Returns an OK status if the blocking was successful and the stream is ok().\n  // Otherwise returns an error describing why the blocking failed.\n  port::Status BlockHostUntilDone() LOCKS_EXCLUDED(mu_);\n\n  // Warning! This method interacts with internal threads in\n  // sometimes-unpredictable ways and is intended for GPU-Executor-internal\n  // use\n  // only. Please check with a member of the FASTR team before making use of\n  // this method.\n  //\n  // Entrains onto the stream a function to be executed on the host at some\n  // point in the future.\n  // Async host callbacks DO NOT block the stream as device functions (or as\n  // synchronous host callbacks). No synchronization is possible with\n  // asynchronous callbacks; they are strictly fire-and-forget.\n  // This method is private due to the potential for undefined behavior with\n  // synchronization using OpenCL user events.\n  // The ONLY lifetime guarantee in these calls is that the StreamExecutor\n  // parameter will still be valid - this Stream may not be!\n  // Any callbacks requiring device API calls must use this method.\n  Stream &ThenEnqueueOnBackgroundThread(\n      std::function<void(StreamExecutor *)> task);\n\n  // Returns the (opaque) platform-specific backing object. Ownership is not\n  // transferred to the caller.\n  internal::StreamInterface *implementation() { return implementation_.get(); }\n\n  // Entrains onto the stream a callback to the host (from the device).\n  // Host callbacks block/occupy the stream just as device functions\n  // (execute one at a time, block later stream operations).\n  // Behavior is undefined when synchronizing using OpenCL user events.\n  // Behavior is undefined if host callbacks call device routines or insert\n  // them into any stream.\n  // On certain platforms, ThenDoHostCallback is expected to have significant\n  // negative effects on performance.\n  Stream &ThenDoHostCallback(std::function<void()> callback);\n\n  // Identical to ThenDoHostCallback; only exposed for testing purposes.\n  Stream &ThenDoHostCallbackForTest(std::function<void()> callback);\n\n  // Returns the StreamExecutor (parent object) associated with this stream.\n  StreamExecutor *parent() const {\n    CHECK(parent_ != nullptr);\n    return parent_;\n  }\n\n  // Returns the (internal usage) temporary-memory-allocation manager associated\n  // with this stream.\n  internal::TemporaryMemoryManager *temporary_memory_manager();\n\n private:\n  friend class host::HostBlas;  // for parent_.\n  friend class host::HostFft;   // for parent_.\n  friend class host::HostRng;   // for parent_.\n  template <typename... Args>\n  friend struct ThenBlasImpl;  // for implementing ThenBlasXXX.\n  friend class ocl::CLBlas;    // for parent_.\n\n  bool InErrorState() const LOCKS_EXCLUDED(mu_) {\n    tf_shared_lock lock{mu_};\n    return !ok_;\n  }\n\n  // Sets the error state if operation_retcode is false.\n  // This is a useful shorthand for many stream routines.\n  void CheckError(bool operation_retcode) LOCKS_EXCLUDED(mu_) {\n    if (operation_retcode) {\n      return;\n    }\n    mutex_lock lock{mu_};\n    ok_ = false;\n  }\n\n  void SetError() { CheckError(false /* = operation_retcode */); }\n\n  void SetErrorAndLogNoDnnSupport() {\n    SetError();\n    LOG(WARNING) << \"attempting to perform DNN operation using StreamExecutor \"\n                    \"without DNN support\";\n  }\n\n  // The StreamExecutor that supports the operation of this stream.\n  StreamExecutor *parent_;\n\n  // The platform-dependent implementation that the StreamExecutor interface\n  // delegates to.\n  std::unique_ptr<internal::StreamInterface> implementation_;\n\n  // mutex that guards the allocation / error state flags.\n  // Mutable so that it can be obtained via const reader lock.\n  mutable mutex mu_;\n\n  // Whether Init() was successfully called to allocate this stream on the\n  // underlying platform. It simply flips from 0 to 1 with a sanity check.\n  // See StreamExecutor::AllocateStream.\n  bool allocated_ GUARDED_BY(mu_);\n\n  // Whether all operations have entrained successfully to the current program\n  // point.\n  bool ok_ GUARDED_BY(mu_);\n\n  // Sub-streams that are generated from this stream. Each element has a pointer\n  // to sub-stream and a boolean value indicating if this substream is ready to\n  // be reused.\n  std::vector<std::pair<std::unique_ptr<Stream>, bool>> sub_streams_\n      GUARDED_BY(mu_);\n\n  // Streams can allocate temporary memories to help with work they enqueue\n  // (e.g. for scratch memory spaces). This member tracks those allocations and\n  // notes when they can be reclaimed -- reclamation is attempted when\n  // BlockHostUntilDone() is called.\n  internal::TemporaryMemoryManager temporary_memory_manager_;\n\n  // Implementation of ThenConvolveBackwardBias that is shared by all types.\n  template <typename T>\n  Stream &ThenConvolveBackwardBiasImpl(\n      const dnn::BatchDescriptor &input_descriptor,\n      const DeviceMemory<T> &input_data,\n      const dnn::BatchDescriptor &bias_descriptor,\n      DeviceMemory<T> *backward_bias_data);\n\n  SE_DISALLOW_COPY_AND_ASSIGN(Stream);\n};\n\n////////////\n// Inlines\n\ntemplate <typename T>\ninline port::StatusOr<std::unique_ptr<TemporaryDeviceMemory<T>>>\nStream::AllocateTemporaryArray(uint64 element_count) {\n  return temporary_memory_manager_.AllocateArray<T>(element_count);\n}\n\ninline internal::TemporaryMemoryManager *Stream::temporary_memory_manager() {\n  return &temporary_memory_manager_;\n}\n\ntemplate <>\nstruct Quantization<uint8> {\n  static constexpr dnn::QuantizedActivationMode kModeId =\n      dnn::QuantizedActivationMode::k8Bit;\n};\n\ntemplate <>\nstruct Quantization<uint16> {\n  static constexpr dnn::QuantizedActivationMode kModeId =\n      dnn::QuantizedActivationMode::k16Bit;\n};\n\ntemplate <>\nstruct Quantization<int32> {\n  static constexpr dnn::QuantizedActivationMode kModeId =\n      dnn::QuantizedActivationMode::k32Bit;\n};\n\n}  // namespace gputools\n}  // namespace perftools\n\n#endif  // TENSORFLOW_STREAM_EXECUTOR_STREAM_H_\n", "subject": "Merge pull request #2997 from maciekcc/branch_125575345", "message": "Merge pull request #2997 from maciekcc/branch_125575345\n\nBranch 125575345" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "tensorflow/tensorflow", "file": "Dockerfile.devel-gpu", "language": "devel-gpu", "commit_date": "2016-06-22 22:16:40.000 UTC", "content": "FROM nvidia/cuda:9.0-base-ubuntu16.04\n\nLABEL maintainer=\"Craig Citro <craigcitro@google.com>\"\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        build-essential \\\n        cuda-command-line-tools-9-0 \\\n        cuda-cublas-dev-9-0 \\\n        cuda-cudart-dev-9-0 \\\n        cuda-cufft-dev-9-0 \\\n        cuda-curand-dev-9-0 \\\n        cuda-cusolver-dev-9-0 \\\n        cuda-cusparse-dev-9-0 \\\n        curl \\\n        git \\\n        libcudnn7=7.0.5.15-1+cuda9.0 \\\n        libcudnn7-dev=7.0.5.15-1+cuda9.0 \\\n        libcurl3-dev \\\n        libfreetype6-dev \\\n        libpng12-dev \\\n        libzmq3-dev \\\n        pkg-config \\\n        python-dev \\\n        rsync \\\n        software-properties-common \\\n        unzip \\\n        zip \\\n        zlib1g-dev \\\n        wget \\\n        && \\\n    rm -rf /var/lib/apt/lists/* && \\\n    find /usr/local/cuda-9.0/lib64/ -type f -name 'lib*_static.a' -not -name 'libcudart_static.a' -delete && \\\n    rm /usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\n\nRUN curl -fSsL -O https://bootstrap.pypa.io/get-pip.py && \\\n    python get-pip.py && \\\n    rm get-pip.py\n\nRUN pip --no-cache-dir install \\\n        ipykernel \\\n        jupyter \\\n        matplotlib \\\n        numpy \\\n        scipy \\\n        sklearn \\\n        pandas \\\n        && \\\n    python -m ipykernel.kernelspec\n\n# Set up our notebook config.\nCOPY jupyter_notebook_config.py /root/.jupyter/\n\n# Jupyter has issues with being run directly:\n#   https://github.com/ipython/ipython/issues/7062\n# We just add a little wrapper script.\nCOPY run_jupyter.sh /\n\n# Set up Bazel.\n\n# Running bazel inside a `docker build` command causes trouble, cf:\n#   https://github.com/bazelbuild/bazel/issues/134\n# The easiest solution is to set up a bazelrc file forcing --batch.\nRUN echo \"startup --batch\" >>/etc/bazel.bazelrc\n# Similarly, we need to workaround sandboxing issues:\n#   https://github.com/bazelbuild/bazel/issues/418\nRUN echo \"build --spawn_strategy=standalone --genrule_strategy=standalone\" \\\n    >>/etc/bazel.bazelrc\n# Install the most recent bazel release.\nENV BAZEL_VERSION 0.11.0\nWORKDIR /\nRUN mkdir /bazel && \\\n    cd /bazel && \\\n    curl -H \"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" -fSsL -O https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \\\n    curl -H \"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" -fSsL -o /bazel/LICENSE.txt https://raw.githubusercontent.com/bazelbuild/bazel/master/LICENSE && \\\n    chmod +x bazel-*.sh && \\\n    ./bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \\\n    cd / && \\\n    rm -f /bazel/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\n\n# Download and build TensorFlow.\nWORKDIR /tensorflow\nRUN git clone --branch=r1.6 --depth=1 https://github.com/tensorflow/tensorflow.git .\n\n# Configure the build for our CUDA configuration.\nENV CI_BUILD_PYTHON python\nENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\nENV TF_NEED_CUDA 1\nENV TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,5.2,6.0,6.1\nENV TF_CUDA_VERSION=9.0\nENV TF_CUDNN_VERSION=7\n\nRUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 && \\\n    LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:${LD_LIBRARY_PATH} \\\n    tensorflow/tools/ci_build/builds/configured GPU \\\n    bazel build -c opt --config=cuda \\\n\t--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" \\\n        tensorflow/tools/pip_package:build_pip_package && \\\n    rm /usr/local/cuda/lib64/stubs/libcuda.so.1 && \\\n    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \\\n    pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl && \\\n    rm -rf /tmp/pip && \\\n    rm -rf /root/.cache\n# Clean up pip wheel and Bazel cache when done.\n\nWORKDIR /root\n\n# TensorBoard\nEXPOSE 6006\n# IPython\nEXPOSE 8888\n", "subject": "Merge pull request #2997 from maciekcc/branch_125575345", "message": "Merge pull request #2997 from maciekcc/branch_125575345\n\nBranch 125575345" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "tensorflow/tensorflow", "file": "conv_grad_ops.h", "language": "h", "commit_date": "2016-06-22 22:16:40.000 UTC", "content": "/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// This is the common header for the input and filter backprop kernels.\n//\n// The operation to compute Conv2D gradients.\n//\n// To compute the gradients for Conv2D, we need three input tensors:\n//    input, filter, and backprop for output.\n// And we need to compute two backprops: one for input and one for filter. We\n// compute them in two different kernels.\n//\n// Both backprops can be computed as straightforward conv2d.\n//\n// Consider a case where the input is 3x3 and the filter is 2x1:\n//\n// INPUT = [ A  B  C ]\n//         [ D  E  F ]\n//         [ G  H  I ]\n//\n// where each \"A\", \"B\", etc is batch x in_depth\n//\n// FILTER = [ X  Y ]\n//\n// where both \"X\" and \"Y\" are in_depth x out_depth\n//\n// With VALID padding, the output is 3x2:\n//\n// OUTPUT = [ a  b ]\n//          [ c  d ]\n//          [ e  f ]\n//\n// where each \"a\", \"b\", etc is batch x out_depth\n//\n// So we have:\n//\n//   a = A * X + B * Y\n//   b = B * X + C * Y\n//   c = D * X + E * Y\n//   d = E * X + F * Y\n//   e = G * X + H * Y\n//   f = H * X + I * Y\n//\n// So when we have backprops for the outputs (we denote them by\n// a', b', ... ):\n//\n// The backprops for the input are:\n//\n//   A' = a' * X^t\n//   B' = a' * Y^t + b' * X^t\n//   C' = b' * Y^t\n//   ...\n//\n// This is essentially computing a 2d conv of\n//\n// INPUT = [ 0  a'  b'  0 ]\n//         [ 0  c'  d'  0 ]\n//         [ 0  e'  f'  0 ]\n// and\n//\n// FILTER = [ Y^t X^t ]\n//\n// The backprops for the filter are:\n//\n//   X' = A^t * a' + B^t * b' + D^t * c' + E^t * d' + G^t * e' + H^t * f'\n//   Y' = B^t * a' + C^t * b' + E^t + c' + F^t * d' + H^t * e' + I^t * f'\n//\n// This is essentially computing a 2d conv of\n//\n// INPUT = [ A^t  B^t  C^t ]\n//         [ D^t  E^t  F^t ]\n//         [ G^t  H^t  I^t ]\n//\n// and\n//\n// FILTER = [ a'  b' ]\n//          [ c'  d' ]\n//          [ e'  f' ]\n//\n//\n//////////////////////////////////////////////////////////\n//\n// With stride more than one, it's a bit more complicated (we will need to\n// create holes to the backprop).\n//\n// Consider the case where\n//\n// INPUT = [ A B C D E ]\n//         [ F G H I J ]\n//         [ K L M N O ]\n// and\n//\n// FILTER = [ X Y Z ]\n//\n// with stride 2.\n//\n// The output will be\n//\n// OUTPUT = [ a b ]\n//          [ c d ]\n//\n// where:\n//\n//   a = A * X + B * Y + C * Z\n//   b = C * X + D * Y + E * Z\n//   c = K * X + L * Y + M * Z\n//   d = M * X + N * Y + O * Z\n//\n//\n// To compute the backprop for INPUT, we need to convolve\n//\n// INPUT = [ 0  0  a' 0  b' 0  0 ]\n//         [ 0  0  0  0  0  0  0 ]\n//         [ 0  0  c' 0  d' 0  0 ]\n//\n// (notice the holes in INPUT)\n//\n// and\n//\n// FILTER = [ Z^t  Y^t  X^t ]\n//\n// with stride 1.\n//\n// To compute the backprop for FILTER, we need to convolve\n\n//\n// INPUT = [ A^t  B^t  C^t  D^t  E^t ]\n//         [ F^t  G^t  H^t  I^t  J^t ]\n//         [ K^t  L^t  M^t  N^t  O^t ]\n// and\n//\n// FILTER = [ a' 0  b' ]\n//          [ 0  0  0  ]\n//          [ c' 0  d' ]\n//\n// (notice the holes in FILTER)\n//\n//\n// with stride 1\n//\n//////////////////////////////////////////////////////////\n//\n//\n// The case for SAME padding is in fact very similar to VALID -- we just\n// need to pad the input tensor a bit when computing the filter_backprop.\n\n#ifndef TENSORFLOW_CORE_KERNELS_CONV_GRAD_OPS_H_\n#define TENSORFLOW_CORE_KERNELS_CONV_GRAD_OPS_H_\n\n#include <vector>\n\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/lib/core/stringpiece.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\n\n// Forward declaration.\nclass OpKernelContext;\n\ntemplate <typename Device, typename T>\nstruct LaunchConv2DBackpropInputOp {\n  void operator()(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n                  const Tensor& out_backprop, const Tensor& filter,\n                  int row_dilation, int col_dilation, int row_stride,\n                  int col_stride, const Padding& padding, Tensor* in_backprop,\n                  TensorFormat data_format);\n};\n\ntemplate <typename Device, typename T>\nstruct LaunchConv2DBackpropFilterOp {\n  void operator()(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n                  const Tensor& out_backprop, const Tensor& input,\n                  int row_dilation, int col_dilation, int row_stride,\n                  int col_stride, const Padding& padding,\n                  Tensor* filter_backprop, TensorFormat data_format);\n};\n\n#ifdef GOOGLE_CUDA\ntemplate <typename T>\nstruct LaunchConv2DBackpropInputOp<Eigen::GpuDevice, T> {\n  void operator()(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n                  const Tensor& input, const Tensor& filter, int row_dilation,\n                  int col_dilation, int row_stride, int col_stride,\n                  const Padding& padding, Tensor* output,\n                  TensorFormat data_format);\n};\n\ntemplate <typename T>\nstruct LaunchConv2DBackpropFilterOp<Eigen::GpuDevice, T> {\n  void operator()(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n                  const Tensor& out_backprop, const Tensor& input,\n                  int row_dilation, int col_dilation, int row_stride,\n                  int col_stride, const Padding& padding,\n                  Tensor* filter_backprop, TensorFormat data_format);\n};\n#endif  // GOOGLE_CUDA\n\n// Information about a single spatial dimension for a convolution\n// backpropagation.\nstruct ConvBackpropSpatialDimension {\n  int64 input_size;\n  int64 filter_size;\n  int64 output_size;\n  int64 stride;\n  int64 dilation;\n  int64 expanded_output_size;\n\n  // Number of padding elements to be added before/after this dimension of\n  // the input when computing Conv?DBackpropInput.\n  int64 pad_before, pad_after;\n};\n\n// Computed dimensions for a backwards convolution.\nstruct ConvBackpropDimensions {\n  // Information about each spatial dimension.\n  gtl::InlinedVector<ConvBackpropSpatialDimension, 3> spatial_dims;\n\n  // Batch size.\n  int64 batch_size;\n\n  // Input and output feature depth.\n  int64 in_depth, out_depth;\n};\n\n// Common code between implementations of Conv?DBackpropInput and\n// Conv?DBackpropFilter. Verifies that the dimensions all match, and computes\n// sizes/padding for the spatial dimensions.\nStatus ConvBackpropComputeDimensions(StringPiece label, int num_spatial_dims,\n                                     const TensorShape& input_shape,\n                                     const TensorShape& filter_shape,\n                                     const TensorShape& out_backprop_shape,\n                                     const std::vector<int32>& strides,\n                                     Padding padding, TensorFormat data_format,\n                                     ConvBackpropDimensions* dims);\n\n// The V2 version computes the same outputs with arbitrary dilation rate.\n// TODO(b/67112639): Merge V2 versions and the original versions eventually.\nStatus ConvBackpropComputeDimensionsV2(\n    StringPiece label, int num_spatial_dims, const TensorShape& input_shape,\n    const TensorShape& filter_shape, const TensorShape& out_backprop_shape,\n    const gtl::ArraySlice<int32>& dilations, const std::vector<int32>& strides,\n    Padding padding, TensorFormat data_format, ConvBackpropDimensions* dims);\n}  // namespace tensorflow\n\n#endif  // TENSORFLOW_CORE_KERNELS_CONV_GRAD_OPS_H_\n", "subject": "Merge pull request #2997 from maciekcc/branch_125575345", "message": "Merge pull request #2997 from maciekcc/branch_125575345\n\nBranch 125575345" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "tensorflow/tensorflow", "file": "tensorflow.bzl" "language": "bzl", "commit_date": "2016-06-22 22:16:40.000 UTC", "content": "# -*- Python -*-\n\n# Return the options to use for a C++ library or binary build.\n# Uses the \":optmode\" config_setting to pick the options.\nload(\n    \"//tensorflow/core:platform/default/build_config_root.bzl\",\n    \"tf_cuda_tests_tags\",\n    \"tf_sycl_tests_tags\",\n    \"tf_additional_grpc_deps_py\",\n    \"tf_additional_xla_deps_py\",\n    \"if_static\",\n)\nload(\n    \"@local_config_tensorrt//:build_defs.bzl\",\n    \"if_tensorrt\",\n)\nload(\n    \"@local_config_cuda//cuda:build_defs.bzl\",\n    \"if_cuda\",\n    \"cuda_default_copts\",\n)\nload(\n    \"//third_party/mkl:build_defs.bzl\",\n    \"if_mkl\",\n)\n\ndef register_extension_info(**kwargs):\n    pass\n\n# Given a source file, generate a test name.\n# i.e. \"common_runtime/direct_session_test.cc\" becomes\n#      \"common_runtime_direct_session_test\"\ndef src_to_test_name(src):\n  return src.replace(\"/\", \"_\").split(\".\")[0]\n\ndef full_path(relative_paths):\n  return [PACKAGE_NAME + \"/\" + relative for relative in relative_paths]\n\n# List of proto files for android builds\ndef tf_android_core_proto_sources(core_proto_sources_relative):\n  return [\n      \"//tensorflow/core:\" + p for p in core_proto_sources_relative\n  ]\n\n# Returns the list of pb.h and proto.h headers that are generated for\n# tf_android_core_proto_sources().\ndef tf_android_core_proto_headers(core_proto_sources_relative):\n  return ([\n      \"//tensorflow/core/\" + p.replace(\".proto\", \".pb.h\")\n      for p in core_proto_sources_relative\n  ] + [\n      \"//tensorflow/core/\" + p.replace(\".proto\", \".proto.h\")\n      for p in core_proto_sources_relative\n  ])\n\n# Sanitize a dependency so that it works correctly from code that includes\n# TensorFlow as a submodule.\ndef clean_dep(dep):\n  return str(Label(dep))\n\ndef if_android_x86(a):\n  return select({\n      clean_dep(\"//tensorflow:android_x86\"): a,\n      clean_dep(\"//tensorflow:android_x86_64\"): a,\n      \"//conditions:default\": [],\n  })\n\ndef if_android_arm(a):\n  return select({\n      clean_dep(\"//tensorflow:android_arm\"): a,\n      \"//conditions:default\": [],\n  })\n\ndef if_android_arm64(a):\n  return select({\n      clean_dep(\"//tensorflow:android_arm64\"): a,\n      \"//conditions:default\": [],\n  })\n\ndef if_android_mips(a):\n  return select({\n      clean_dep(\"//tensorflow:android_mips\"): a,\n      \"//conditions:default\": [],\n  })\n\ndef if_not_android(a):\n  return select({\n      clean_dep(\"//tensorflow:android\"): [],\n      \"//conditions:default\": a,\n  })\n\ndef if_not_android_mips_and_mips64(a):\n  return select({\n      clean_dep(\"//tensorflow:android_mips\"): [],\n      clean_dep(\"//tensorflow:android_mips64\"): [],\n      \"//conditions:default\": a,\n  })\n\ndef if_android(a):\n  return select({\n      clean_dep(\"//tensorflow:android\"): a,\n      \"//conditions:default\": [],\n  })\n\ndef if_ios(a):\n  return select({\n      clean_dep(\"//tensorflow:ios\"): a,\n      \"//conditions:default\": [],\n  })\n\ndef if_ios_x86_64(a):\n  return select({\n      clean_dep(\"//tensorflow:ios_x86_64\"): a,\n      \"//conditions:default\": [],\n  })\n\ndef if_mobile(a):\n  return select({\n      clean_dep(\"//tensorflow:android\"): a,\n      clean_dep(\"//tensorflow:ios\"): a,\n      \"//conditions:default\": [],\n  })\n\ndef if_not_mobile(a):\n  return select({\n      clean_dep(\"//tensorflow:android\"): [],\n      clean_dep(\"//tensorflow:ios\"): [],\n      \"//conditions:default\": a,\n  })\n\ndef if_not_windows(a):\n  return select({\n      clean_dep(\"//tensorflow:windows\"): [],\n      clean_dep(\"//tensorflow:windows_msvc\"): [],\n      \"//conditions:default\": a,\n  })\n\ndef if_windows(a):\n  return select({\n      clean_dep(\"//tensorflow:windows\"): a,\n      clean_dep(\"//tensorflow:windows_msvc\"): a,\n      \"//conditions:default\": [],\n  })\n\ndef if_linux_x86_64(a):\n  return select({\n      clean_dep(\"//tensorflow:linux_x86_64\"): a,\n      \"//conditions:default\": [],\n  })\n\ndef if_darwin(a):\n  return select({\n      clean_dep(\"//tensorflow:darwin\"): a,\n      \"//conditions:default\": [],\n  })\n\ndef if_override_eigen_strong_inline(a):\n  return select({\n      clean_dep(\"//tensorflow:override_eigen_strong_inline\"): a,\n      \"//conditions:default\": [],\n  })\n\ndef get_win_copts(is_external=False):\n    WINDOWS_COPTS = [\n        \"/D__VERSION__=\\\\\\\"MSVC\\\\\\\"\",\n        \"/DPLATFORM_WINDOWS\",\n        \"/DEIGEN_HAS_C99_MATH\",\n        \"/DTENSORFLOW_USE_EIGEN_THREADPOOL\",\n        \"/DEIGEN_AVOID_STL_ARRAY\",\n        \"/Iexternal/gemmlowp\",\n        \"/wd4018\",  # -Wno-sign-compare\n        \"/U_HAS_EXCEPTIONS\",\n        \"/D_HAS_EXCEPTIONS=1\",\n        \"/EHsc\",  # -fno-exceptions\n        \"/DNOGDI\",\n    ]\n    if is_external:\n      return WINDOWS_COPTS + [\"/UTF_COMPILE_LIBRARY\"]\n    else:\n      return WINDOWS_COPTS + [\"/DTF_COMPILE_LIBRARY\"]\n\n# LINT.IfChange\ndef tf_copts(android_optimization_level_override=\"-O2\", is_external=False):\n  # For compatibility reasons, android_optimization_level_override\n  # is currently only being set for Android.\n  # To clear this value, and allow the CROSSTOOL default\n  # to be used, pass android_optimization_level_override=None\n  android_copts = [\n      \"-std=c++11\",\n      \"-DTF_LEAN_BINARY\",\n      \"-Wno-narrowing\",\n      \"-fomit-frame-pointer\",\n  ]\n  if android_optimization_level_override:\n    android_copts.append(android_optimization_level_override)\n  return (\n      if_not_windows([\n          \"-DEIGEN_AVOID_STL_ARRAY\",\n          \"-Iexternal/gemmlowp\",\n          \"-Wno-sign-compare\",\n          \"-fno-exceptions\",\n          \"-ftemplate-depth=900\"])\n      + if_cuda([\"-DGOOGLE_CUDA=1\"])\n      + if_tensorrt([\"-DGOOGLE_TENSORRT=1\"])\n      + if_mkl([\"-DINTEL_MKL=1\", \"-DEIGEN_USE_VML\", \"-fopenmp\",])\n      + if_android_arm([\"-mfpu=neon\"])\n      + if_linux_x86_64([\"-msse3\"])\n      + if_ios_x86_64([\"-msse4.1\"])\n      + select({\n            clean_dep(\"//tensorflow:framework_shared_object\"): [],\n            \"//conditions:default\": [\"-DTENSORFLOW_MONOLITHIC_BUILD\"],\n      })\n      + select({\n            clean_dep(\"//tensorflow:android\"): android_copts,\n            clean_dep(\"//tensorflow:darwin\"): [],\n            clean_dep(\"//tensorflow:windows\"): get_win_copts(is_external),\n            clean_dep(\"//tensorflow:windows_msvc\"): get_win_copts(is_external),\n            clean_dep(\"//tensorflow:ios\"): [\"-std=c++11\"],\n            \"//conditions:default\": [\"-pthread\"]\n      }))\n\n\ndef tfe_xla_copts():\n  return select({\n      \"//tensorflow:with_xla_support\": [\"-DTENSORFLOW_EAGER_USE_XLA\"],\n      \"//conditions:default\": [],\n  })\n\ndef tf_opts_nortti_if_android():\n  return if_android([\n      \"-fno-rtti\",\n      \"-DGOOGLE_PROTOBUF_NO_RTTI\",\n      \"-DGOOGLE_PROTOBUF_NO_STATIC_INITIALIZER\",\n  ])\n\n# LINT.ThenChange(//tensorflow/contrib/android/cmake/CMakeLists.txt)\n\n# Given a list of \"op_lib_names\" (a list of files in the ops directory\n# without their .cc extensions), generate a library for that file.\ndef tf_gen_op_libs(op_lib_names, deps=None, is_external=True):\n  # Make library out of each op so it can also be used to generate wrappers\n  # for various languages.\n  if not deps:\n    deps = []\n  for n in op_lib_names:\n    native.cc_library(\n        name=n + \"_op_lib\",\n        copts=tf_copts(is_external=is_external),\n        srcs=[\"ops/\" + n + \".cc\"],\n        deps=deps + [clean_dep(\"//tensorflow/core:framework\")],\n        visibility=[\"//visibility:public\"],\n        alwayslink=1,\n        linkstatic=1,)\n\ndef _make_search_paths(prefix, levels_to_root):\n  return \",\".join(\n      [\"-rpath,%s/%s\" % (prefix, \"/\".join([\"..\"] * search_level))\n       for search_level in range(levels_to_root + 1)])\n\ndef _rpath_linkopts(name):\n  # Search parent directories up to the TensorFlow root directory for shared\n  # object dependencies, even if this op shared object is deeply nested\n  # (e.g. tensorflow/contrib/package:python/ops/_op_lib.so). tensorflow/ is then\n  # the root and tensorflow/libtensorflow_framework.so should exist when\n  # deployed. Other shared object dependencies (e.g. shared between contrib/\n  # ops) are picked up as long as they are in either the same or a parent\n  # directory in the tensorflow/ tree.\n  levels_to_root = PACKAGE_NAME.count(\"/\") + name.count(\"/\")\n  return select({\n      clean_dep(\"//tensorflow:darwin\"): [\n          \"-Wl,%s\" % (_make_search_paths(\"@loader_path\", levels_to_root),),\n      ],\n      clean_dep(\"//tensorflow:windows\"): [],\n      clean_dep(\"//tensorflow:windows_msvc\"): [],\n      \"//conditions:default\": [\n          \"-Wl,%s\" % (_make_search_paths(\"$$ORIGIN\", levels_to_root),),\n      ],\n  })\n\n# Bazel-generated shared objects which must be linked into TensorFlow binaries\n# to define symbols from //tensorflow/core:framework and //tensorflow/core:lib.\ndef tf_binary_additional_srcs():\n  return if_static(\n      extra_deps=[],\n      otherwise=[\n          clean_dep(\"//tensorflow:libtensorflow_framework.so\"),\n      ])\n\ndef tf_cc_shared_object(\n    name,\n    srcs=[],\n    deps=[],\n    linkopts=[],\n    framework_so=tf_binary_additional_srcs(),\n    **kwargs):\n  native.cc_binary(\n      name=name,\n      srcs=srcs + framework_so,\n      deps=deps,\n      linkshared = 1,\n      linkopts=linkopts + _rpath_linkopts(name) + select({\n          clean_dep(\"//tensorflow:darwin\"): [\n              \"-Wl,-install_name,@rpath/\" + name.split(\"/\")[-1],\n          ],\n          \"//conditions:default\": [\n              \"-Wl,-soname,\" + name.split(\"/\")[-1],\n          ],\n      }),\n      **kwargs)\n\nregister_extension_info(\n    extension_name = \"tf_cc_shared_object\",\n    label_regex_for_dep = \"{extension_name}\",\n)\n\n# Links in the framework shared object\n# (//third_party/tensorflow:libtensorflow_framework.so) when not building\n# statically. Also adds linker options (rpaths) so that the framework shared\n# object can be found.\ndef tf_cc_binary(name,\n                 srcs=[],\n                 deps=[],\n                 linkopts=[],\n                 copts=tf_copts(),\n                 **kwargs):\n  native.cc_binary(\n      name=name,\n      copts=copts,\n      srcs=srcs + tf_binary_additional_srcs(),\n      deps=deps + if_mkl(\n          [\n              \"//third_party/mkl:intel_binary_blob\",\n          ],\n      ),\n      linkopts=linkopts + _rpath_linkopts(name),\n      **kwargs)\n\nregister_extension_info(\n    extension_name = \"tf_cc_binary\",\n    label_regex_for_dep = \"{extension_name}.*\",\n)\n\ndef tf_gen_op_wrapper_cc(name,\n                         out_ops_file,\n                         pkg=\"\",\n                         op_gen=clean_dep(\"//tensorflow/cc:cc_op_gen_main\"),\n                         deps=None,\n                         include_internal_ops=0,\n                         # ApiDefs will be loaded in the order specified in this list.\n                         api_def_srcs=[]):\n  # Construct an op generator binary for these ops.\n  tool = out_ops_file + \"_gen_cc\"\n  if deps == None:\n    deps = [pkg + \":\" + name + \"_op_lib\"]\n  tf_cc_binary(\n      name=tool,\n      copts=tf_copts(),\n      linkopts=if_not_windows([\"-lm\"]),\n      linkstatic=1,  # Faster to link this one-time-use binary dynamically\n      deps=[op_gen] + deps)\n\n  srcs = api_def_srcs[:]\n\n  if not api_def_srcs:\n    api_def_args_str = \",\"\n  else:\n    api_def_args = []\n    for api_def_src in api_def_srcs:\n      # Add directory of the first ApiDef source to args.\n      # We are assuming all ApiDefs in a single api_def_src are in the\n      # same directory.\n      api_def_args.append(\n          \" $$(dirname $$(echo $(locations \" + api_def_src +\n          \") | cut -d\\\" \\\" -f1))\")\n    api_def_args_str = \",\".join(api_def_args)\n\n  native.genrule(\n      name=name + \"_genrule\",\n      outs=[\n          out_ops_file + \".h\", out_ops_file + \".cc\",\n          out_ops_file + \"_internal.h\", out_ops_file + \"_internal.cc\"\n      ],\n      srcs=srcs,\n      tools=[\":\" + tool] + tf_binary_additional_srcs(),\n      cmd=(\"$(location :\" + tool + \") $(location :\" + out_ops_file + \".h) \" +\n           \"$(location :\" + out_ops_file + \".cc) \" +\n           str(include_internal_ops) + \" \" + api_def_args_str))\n\n# Given a list of \"op_lib_names\" (a list of files in the ops directory\n# without their .cc extensions), generate individual C++ .cc and .h\n# files for each of the ops files mentioned, and then generate a\n# single cc_library called \"name\" that combines all the\n# generated C++ code.\n#\n# For example, for:\n#  tf_gen_op_wrappers_cc(\"tf_ops_lib\", [ \"array_ops\", \"math_ops\" ])\n#\n#\n# This will ultimately generate ops/* files and a library like:\n#\n# cc_library(name = \"tf_ops_lib\",\n#            srcs = [ \"ops/array_ops.cc\",\n#                     \"ops/math_ops.cc\" ],\n#            hdrs = [ \"ops/array_ops.h\",\n#                     \"ops/math_ops.h\" ],\n#            deps = [ ... ])\n#\n# Plus a private library for the \"hidden\" ops.\n# cc_library(name = \"tf_ops_lib_internal\",\n#            srcs = [ \"ops/array_ops_internal.cc\",\n#                     \"ops/math_ops_internal.cc\" ],\n#            hdrs = [ \"ops/array_ops_internal.h\",\n#                     \"ops/math_ops_internal.h\" ],\n#            deps = [ ... ])\n# TODO(joshl): Cleaner approach for hidden ops.\ndef tf_gen_op_wrappers_cc(name,\n                          op_lib_names=[],\n                          other_srcs=[],\n                          other_hdrs=[],\n                          pkg=\"\",\n                          deps=[\n                              clean_dep(\"//tensorflow/cc:ops\"),\n                              clean_dep(\"//tensorflow/cc:scope\"),\n                              clean_dep(\"//tensorflow/cc:const_op\"),\n                          ],\n                          op_gen=clean_dep(\"//tensorflow/cc:cc_op_gen_main\"),\n                          include_internal_ops=0,\n                          visibility=None,\n                          # ApiDefs will be loaded in the order apecified in this list.\n                          api_def_srcs=[]):\n  subsrcs = other_srcs[:]\n  subhdrs = other_hdrs[:]\n  internalsrcs = []\n  internalhdrs = []\n  for n in op_lib_names:\n    tf_gen_op_wrapper_cc(\n        n,\n        \"ops/\" + n,\n        pkg=pkg,\n        op_gen=op_gen,\n        include_internal_ops=include_internal_ops,\n        api_def_srcs=api_def_srcs)\n    subsrcs += [\"ops/\" + n + \".cc\"]\n    subhdrs += [\"ops/\" + n + \".h\"]\n    internalsrcs += [\"ops/\" + n + \"_internal.cc\"]\n    internalhdrs += [\"ops/\" + n + \"_internal.h\"]\n\n  native.cc_library(\n      name=name,\n      srcs=subsrcs,\n      hdrs=subhdrs,\n      deps=deps + if_not_android([\n          clean_dep(\"//tensorflow/core:core_cpu\"),\n          clean_dep(\"//tensorflow/core:framework\"),\n          clean_dep(\"//tensorflow/core:lib\"),\n          clean_dep(\"//tensorflow/core:protos_all_cc\"),\n      ]) + if_android([\n          clean_dep(\"//tensorflow/core:android_tensorflow_lib\"),\n      ]),\n      copts=tf_copts(),\n      alwayslink=1,\n      visibility=visibility)\n  native.cc_library(\n      name=name + \"_internal\",\n      srcs=internalsrcs,\n      hdrs=internalhdrs,\n      deps=deps + if_not_android([\n          clean_dep(\"//tensorflow/core:core_cpu\"),\n          clean_dep(\"//tensorflow/core:framework\"),\n          clean_dep(\"//tensorflow/core:lib\"),\n          clean_dep(\"//tensorflow/core:protos_all_cc\"),\n      ]) + if_android([\n          clean_dep(\"//tensorflow/core:android_tensorflow_lib\"),\n      ]),\n      copts=tf_copts(),\n      alwayslink=1,\n      visibility=[clean_dep(\"//tensorflow:internal\")])\n\n# Generates a Python library target wrapping the ops registered in \"deps\".\n#\n# Args:\n#   name: used as the name of the generated target and as a name component of\n#     the intermediate files.\n#   out: name of the python file created by this rule. If None, then\n#     \"ops/gen_{name}.py\" is used.\n#   hidden: Optional list of ops names to make private in the Python module.\n#     It is invalid to specify both \"hidden\" and \"op_whitelist\".\n#   visibility: passed to py_library.\n#   deps: list of dependencies for the generated target.\n#   require_shape_functions: leave this as False.\n#   hidden_file: optional file that contains a list of op names to make private\n#     in the generated Python module. Each op name should be on a line by\n#     itself. Lines that start with characters that are invalid op name\n#     starting characters are treated as comments and ignored.\n#   generated_target_name: name of the generated target (overrides the\n#     \"name\" arg)\n#   op_whitelist: if not empty, only op names in this list will be wrapped. It\n#     is invalid to specify both \"hidden\" and \"op_whitelist\".\n#   cc_linkopts: Optional linkopts to be added to tf_cc_binary that contains the\n#     specified ops.\n#   gen_locally: if True, the genrule to generate the Python library will be run\n#     without sandboxing. This would help when the genrule depends on symlinks\n#     which may not be supported in the sandbox.\ndef tf_gen_op_wrapper_py(name,\n                         out=None,\n                         hidden=None,\n                         visibility=None,\n                         deps=[],\n                         require_shape_functions=False,\n                         hidden_file=None,\n                         generated_target_name=None,\n                         op_whitelist=[],\n                         cc_linkopts=[],\n                         api_def_srcs=[],\n                         gen_locally=False):\n  if (hidden or hidden_file) and op_whitelist:\n    fail('Cannot pass specify both hidden and op_whitelist.')\n\n  # Construct a cc_binary containing the specified ops.\n  tool_name = \"gen_\" + name + \"_py_wrappers_cc\"\n  if not deps:\n    deps = [str(Label(\"//tensorflow/core:\" + name + \"_op_lib\"))]\n  tf_cc_binary(\n      name=tool_name,\n      linkopts=if_not_windows([\"-lm\"]) + cc_linkopts,\n      copts=tf_copts(),\n      linkstatic=1,  # Faster to link this one-time-use binary dynamically\n      deps=([\n          clean_dep(\"//tensorflow/core:framework\"),\n          clean_dep(\"//tensorflow/python:python_op_gen_main\")\n      ] + deps),\n      visibility=[clean_dep(\"//tensorflow:internal\")],)\n\n  # Invoke the previous cc_binary to generate a python file.\n  if not out:\n    out = \"ops/gen_\" + name + \".py\"\n\n  if hidden:\n    op_list_arg = \",\".join(hidden)\n    op_list_is_whitelist = False\n  elif op_whitelist:\n    op_list_arg = \",\".join(op_whitelist)\n    op_list_is_whitelist = True\n  else:\n    op_list_arg = \"''\"\n    op_list_is_whitelist = False\n\n  # Prepare ApiDef directories to pass to the genrule.\n  if not api_def_srcs:\n    api_def_args_str = \",\"\n  else:\n    api_def_args = []\n    for api_def_src in api_def_srcs:\n      # Add directory of the first ApiDef source to args.\n      # We are assuming all ApiDefs in a single api_def_src are in the\n      # same directory.\n      api_def_args.append(\n          \"$$(dirname $$(echo $(locations \" + api_def_src +\n          \") | cut -d\\\" \\\" -f1))\")\n    api_def_args_str = \",\".join(api_def_args)\n\n  if hidden_file:\n    # `hidden_file` is file containing a list of op names to be hidden in the\n    # generated module.\n    native.genrule(\n        name=name + \"_pygenrule\",\n        outs=[out],\n        srcs=api_def_srcs + [hidden_file],\n        tools=[tool_name] + tf_binary_additional_srcs(),\n        local = (1 if gen_locally else 0),\n        cmd=(\"$(location \" + tool_name + \") \" + api_def_args_str +\n             \" @$(location \" + hidden_file + \") \" +\n             (\"1\" if require_shape_functions else \"0\") + \" > $@\"))\n  else:\n    native.genrule(\n        name=name + \"_pygenrule\",\n        outs=[out],\n        srcs=api_def_srcs,\n        tools=[tool_name] + tf_binary_additional_srcs(),\n        local = (1 if gen_locally else 0),\n        cmd=(\"$(location \" + tool_name + \") \" + api_def_args_str + \" \" +\n             op_list_arg + \" \" +\n             (\"1\" if require_shape_functions else \"0\") + \" \" +\n             (\"1\" if op_list_is_whitelist else \"0\") + \" > $@\"))\n\n  # Make a py_library out of the generated python file.\n  if not generated_target_name:\n    generated_target_name = name\n  native.py_library(\n      name=generated_target_name,\n      srcs=[out],\n      srcs_version=\"PY2AND3\",\n      visibility=visibility,\n      deps=[\n          clean_dep(\"//tensorflow/python:framework_for_generated_wrappers_v2\"),\n      ],)\n\n# Define a bazel macro that creates cc_test for tensorflow.\n#\n# Links in the framework shared object\n# (//third_party/tensorflow:libtensorflow_framework.so) when not building\n# statically. Also adds linker options (rpaths) so that the framework shared\n# object can be found.\n#\n# TODO(opensource): we need to enable this to work around the hidden symbol\n# __cudaRegisterFatBinary error. Need more investigations.\ndef tf_cc_test(name,\n               srcs,\n               deps,\n               linkstatic=0,\n               extra_copts=[],\n               suffix=\"\",\n               linkopts=[],\n               nocopts=None,\n               **kwargs):\n  native.cc_test(\n      name=\"%s%s\" % (name, suffix),\n      srcs=srcs + tf_binary_additional_srcs(),\n      copts=tf_copts() + extra_copts,\n      linkopts=select({\n        clean_dep(\"//tensorflow:android\"): [\n            \"-pie\",\n          ],\n        clean_dep(\"//tensorflow:windows\"): [],\n        clean_dep(\"//tensorflow:windows_msvc\"): [],\n        \"//conditions:default\": [\n            \"-lpthread\",\n            \"-lm\"\n        ],\n      }) + linkopts + _rpath_linkopts(name),\n      deps=deps + if_mkl(\n          [\n              \"//third_party/mkl:intel_binary_blob\",\n          ],\n      ),\n      # Nested select() statements seem not to be supported when passed to\n      # linkstatic, and we already have a cuda select() passed in to this\n      # function.\n      linkstatic=linkstatic or select({\n          # cc_tests with \".so\"s in srcs incorrectly link on Darwin unless\n          # linkstatic=1 (https://github.com/bazelbuild/bazel/issues/3450).\n          # TODO(allenl): Remove Mac static linking when Bazel 0.6 is out.\n          clean_dep(\"//tensorflow:darwin\"): 1,\n          \"//conditions:default\": 0,\n      }),\n      nocopts=nocopts,\n      **kwargs)\n\nregister_extension_info(\n    extension_name = \"tf_cc_test\",\n    label_regex_for_dep = \"{extension_name}.*\",\n)\n\n# Part of the testing workflow requires a distinguishable name for the build\n# rules that involve a GPU, even if otherwise identical to the base rule.\ndef tf_cc_test_gpu(name,\n                   srcs,\n                   deps,\n                   linkstatic=0,\n                   tags=[],\n                   data=[],\n                   size=\"medium\",\n                   suffix=\"\",\n                   args=None):\n  tf_cc_test(\n      name,\n      srcs,\n      deps,\n      linkstatic=linkstatic,\n      tags=tags,\n      data=data,\n      size=size,\n      suffix=suffix,\n      args=args)\n\nregister_extension_info(\n    extension_name = \"tf_cc_test_gpu\",\n    label_regex_for_dep = \"{extension_name}\",\n)\n\ndef tf_cuda_cc_test(name,\n                    srcs=[],\n                    deps=[],\n                    tags=[],\n                    data=[],\n                    size=\"medium\",\n                    extra_copts=[],\n                    linkstatic=0,\n                    args=[],\n                    linkopts=[]):\n  tf_cc_test(\n      name=name,\n      srcs=srcs,\n      deps=deps,\n      tags=tags + [\"manual\"],\n      data=data,\n      size=size,\n      extra_copts=extra_copts,\n      linkstatic=linkstatic,\n      linkopts=linkopts,\n      args=args)\n  tf_cc_test(\n      name=name,\n      srcs=srcs,\n      suffix=\"_gpu\",\n      deps=deps + if_cuda([\n          clean_dep(\"//tensorflow/core:gpu_runtime\"),\n      ]),\n      linkstatic=select({\n          # TODO(allenl): Remove Mac static linking when Bazel 0.6 is out.\n          clean_dep(\"//tensorflow:darwin\"): 1,\n          \"@local_config_cuda//cuda:using_nvcc\": 1,\n          \"@local_config_cuda//cuda:using_clang\": 1,\n          \"//conditions:default\": 0,\n      }),\n      tags=tags + tf_cuda_tests_tags(),\n      data=data,\n      size=size,\n      extra_copts=extra_copts,\n      linkopts=linkopts,\n      args=args)\n\nregister_extension_info(\n    extension_name = \"tf_cuda_cc_test\",\n    label_regex_for_dep = \"{extension_name}\",\n)\n\ndef tf_cuda_only_cc_test(name,\n                    srcs=[],\n                    deps=[],\n                    tags=[],\n                    data=[],\n                    size=\"medium\",\n                    linkstatic=0,\n                    args=[],\n                    linkopts=[]):\n  native.cc_test(\n      name=\"%s%s\" % (name, \"_gpu\"),\n      srcs=srcs + tf_binary_additional_srcs(),\n      size=size,\n      args=args,\n      copts= _cuda_copts() + tf_copts(),\n      data=data,\n      deps=deps + if_cuda([\n          clean_dep(\"//tensorflow/core:cuda\"),\n          clean_dep(\"//tensorflow/core:gpu_lib\")]),\n      linkopts=if_not_windows([\"-lpthread\", \"-lm\"]) + linkopts + _rpath_linkopts(name),\n      linkstatic=linkstatic or select({\n          # cc_tests with \".so\"s in srcs incorrectly link on Darwin\n          # unless linkstatic=1.\n          # TODO(allenl): Remove Mac static linking when Bazel 0.6 is out.\n          clean_dep(\"//tensorflow:darwin\"): 1,\n          \"//conditions:default\": 0,\n      }),\n      tags=tags + tf_cuda_tests_tags())\n\nregister_extension_info(\n    extension_name = \"tf_cuda_only_cc_test\",\n    label_regex_for_dep = \"{extension_name}_gpu\",\n)\n\n# Create a cc_test for each of the tensorflow tests listed in \"tests\"\ndef tf_cc_tests(srcs,\n                deps,\n                name=\"\",\n                linkstatic=0,\n                tags=[],\n                size=\"medium\",\n                args=None,\n                linkopts=[],\n                nocopts=None):\n  for src in srcs:\n    tf_cc_test(\n        name=src_to_test_name(src),\n        srcs=[src],\n        deps=deps,\n        linkstatic=linkstatic,\n        tags=tags,\n        size=size,\n        args=args,\n        linkopts=linkopts,\n        nocopts=nocopts)\n\ndef tf_cc_test_mkl(srcs,\n                   deps,\n                   name=\"\",\n                   linkstatic=0,\n                   tags=[],\n                   size=\"medium\",\n                   args=None):\n  if_mkl(tf_cc_tests(srcs, deps, name, linkstatic=linkstatic, tags=tags, size=size, args=args, nocopts=\"-fno-exceptions\"))\n\ndef tf_cc_tests_gpu(srcs,\n                    deps,\n                    name=\"\",\n                    linkstatic=0,\n                    tags=[],\n                    size=\"medium\",\n                    args=None):\n  tf_cc_tests(srcs, deps, linkstatic, tags=tags, size=size, args=args)\n\ndef tf_cuda_cc_tests(srcs,\n                     deps,\n                     name=\"\",\n                     tags=[],\n                     size=\"medium\",\n                     linkstatic=0,\n                     args=None,\n                     linkopts=[]):\n  for src in srcs:\n    tf_cuda_cc_test(\n        name=src_to_test_name(src),\n        srcs=[src],\n        deps=deps,\n        tags=tags,\n        size=size,\n        linkstatic=linkstatic,\n        args=args,\n        linkopts=linkopts)\n\ndef tf_java_test(name,\n                 srcs=[],\n                 deps=[],\n                 *args,\n                 **kwargs):\n  native.java_test(\n      name=name,\n      srcs=srcs,\n      deps=deps + tf_binary_additional_srcs(),\n      *args,\n      **kwargs)\n\nregister_extension_info(\n    extension_name = \"tf_java_test\",\n    label_regex_for_dep = \"{extension_name}\",\n)\n\ndef _cuda_copts():\n  \"\"\"Gets the appropriate set of copts for (maybe) CUDA compilation.\n\n    If we're doing CUDA compilation, returns copts for our particular CUDA\n    compiler.  If we're not doing CUDA compilation, returns an empty list.\n\n    \"\"\"\n  return cuda_default_copts() + select({\n      \"//conditions:default\": [],\n      \"@local_config_cuda//cuda:using_nvcc\": ([\n          \"-nvcc_options=relaxed-constexpr\",\n          \"-nvcc_options=ftz=true\",\n      ]),\n      \"@local_config_cuda//cuda:using_clang\": ([\n          \"-fcuda-flush-denormals-to-zero\",\n      ]),\n  })\n\n# Build defs for TensorFlow kernels\n\n# When this target is built using --config=cuda, a cc_library is built\n# that passes -DGOOGLE_CUDA=1 and '-x cuda', linking in additional\n# libraries needed by GPU kernels.\ndef tf_gpu_kernel_library(srcs,\n                          copts=[],\n                          cuda_copts=[],\n                          deps=[],\n                          hdrs=[],\n                          **kwargs):\n  copts = copts + _cuda_copts() + if_cuda(cuda_copts) + tf_copts()\n\n  native.cc_library(\n      srcs=srcs,\n      hdrs=hdrs,\n      copts=copts,\n      deps=deps + if_cuda([\n          clean_dep(\"//tensorflow/core:cuda\"),\n          clean_dep(\"//tensorflow/core:gpu_lib\"),\n      ]),\n      alwayslink=1,\n      **kwargs)\n\nregister_extension_info(\n    extension_name = \"tf_gpu_kernel_library\",\n    label_regex_for_dep = \"{extension_name}\",\n)\n\ndef tf_cuda_library(deps=None, cuda_deps=None, copts=tf_copts(), **kwargs):\n  \"\"\"Generate a cc_library with a conditional set of CUDA dependencies.\n\n  When the library is built with --config=cuda:\n\n  - Both deps and cuda_deps are used as dependencies.\n  - The cuda runtime is added as a dependency (if necessary).\n  - The library additionally passes -DGOOGLE_CUDA=1 to the list of copts.\n  - In addition, when the library is also built with TensorRT enabled, it\n      additionally passes -DGOOGLE_TENSORRT=1 to the list of copts.\n\n  Args:\n  - cuda_deps: BUILD dependencies which will be linked if and only if:\n      '--config=cuda' is passed to the bazel command line.\n  - deps: dependencies which will always be linked.\n  - copts: copts always passed to the cc_library.\n  - kwargs: Any other argument to cc_library.\n  \"\"\"\n  if not deps:\n    deps = []\n  if not cuda_deps:\n    cuda_deps = []\n\n  native.cc_library(\n      deps=deps + if_cuda(cuda_deps + [\n          clean_dep(\"//tensorflow/core:cuda\"),\n          \"@local_config_cuda//cuda:cuda_headers\"\n      ]),\n      copts=(copts + if_cuda([\"-DGOOGLE_CUDA=1\"]) + if_mkl([\"-DINTEL_MKL=1\"]) +\n             if_tensorrt([\"-DGOOGLE_TENSORRT=1\"])),\n      **kwargs)\n\nregister_extension_info(\n    extension_name = \"tf_cuda_library\",\n    label_regex_for_dep = \"{extension_name}\",\n)\n\ndef tf_kernel_library(name,\n                      prefix=None,\n                      srcs=None,\n                      gpu_srcs=None,\n                      hdrs=None,\n                      deps=None,\n                      alwayslink=1,\n                      copts=None,\n                      is_external=False,\n                      **kwargs):\n  \"\"\"A rule to build a TensorFlow OpKernel.\n\n  May either specify srcs/hdrs or prefix.  Similar to tf_cuda_library,\n  but with alwayslink=1 by default.  If prefix is specified:\n    * prefix*.cc (except *.cu.cc) is added to srcs\n    * prefix*.h (except *.cu.h) is added to hdrs\n    * prefix*.cu.cc and prefix*.h (including *.cu.h) are added to gpu_srcs.\n  With the exception that test files are excluded.\n  For example, with prefix = \"cast_op\",\n    * srcs = [\"cast_op.cc\"]\n    * hdrs = [\"cast_op.h\"]\n    * gpu_srcs = [\"cast_op_gpu.cu.cc\", \"cast_op.h\"]\n    * \"cast_op_test.cc\" is excluded\n  With prefix = \"cwise_op\"\n    * srcs = [\"cwise_op_abs.cc\", ..., \"cwise_op_tanh.cc\"],\n    * hdrs = [\"cwise_ops.h\", \"cwise_ops_common.h\"],\n    * gpu_srcs = [\"cwise_op_gpu_abs.cu.cc\", ..., \"cwise_op_gpu_tanh.cu.cc\",\n                  \"cwise_ops.h\", \"cwise_ops_common.h\",\n                  \"cwise_ops_gpu_common.cu.h\"]\n    * \"cwise_ops_test.cc\" is excluded\n  \"\"\"\n  if not srcs:\n    srcs = []\n  if not hdrs:\n    hdrs = []\n  if not deps:\n    deps = []\n  if not copts:\n    copts = []\n  copts = copts + tf_copts(is_external=is_external)\n  if prefix:\n    if native.glob([prefix + \"*.cu.cc\"], exclude=[\"*test*\"]):\n      if not gpu_srcs:\n        gpu_srcs = []\n      gpu_srcs = gpu_srcs + native.glob(\n          [prefix + \"*.cu.cc\", prefix + \"*.h\"], exclude=[prefix + \"*test*\"])\n    srcs = srcs + native.glob(\n        [prefix + \"*.cc\"], exclude=[prefix + \"*test*\", prefix + \"*.cu.cc\"])\n    hdrs = hdrs + native.glob(\n        [prefix + \"*.h\"], exclude=[prefix + \"*test*\", prefix + \"*.cu.h\"])\n\n  cuda_deps = [clean_dep(\"//tensorflow/core:gpu_lib\")]\n  if gpu_srcs:\n    for gpu_src in gpu_srcs:\n      if gpu_src.endswith(\".cc\") and not gpu_src.endswith(\".cu.cc\"):\n        fail(\"{} not allowed in gpu_srcs. .cc sources must end with .cu.cc\".\n             format(gpu_src))\n    tf_gpu_kernel_library(\n        name=name + \"_gpu\", srcs=gpu_srcs, deps=deps, **kwargs)\n    cuda_deps.extend([\":\" + name + \"_gpu\"])\n  tf_cuda_library(\n      name=name,\n      srcs=srcs,\n      hdrs=hdrs,\n      copts=copts,\n      cuda_deps=cuda_deps,\n      linkstatic=1,  # Needed since alwayslink is broken in bazel b/27630669\n      alwayslink=alwayslink,\n      deps=deps,\n      **kwargs)\n\nregister_extension_info(\n    extension_name = \"tf_kernel_library\",\n    label_regex_for_dep = \"{extension_name}(_gpu)?\",\n)\n\ndef tf_mkl_kernel_library(name,\n                          prefix=None,\n                          srcs=None,\n                          gpu_srcs=None,\n                          hdrs=None,\n                          deps=None,\n                          alwayslink=1,\n                          copts=tf_copts(),\n                          nocopts=\"-fno-exceptions\",\n                          **kwargs):\n  \"\"\"A rule to build MKL-based TensorFlow kernel libraries.\"\"\"\n  gpu_srcs = gpu_srcs  # unused argument\n  kwargs = kwargs  # unused argument\n\n  if not bool(srcs):\n    srcs = []\n  if not bool(hdrs):\n    hdrs = []\n\n  if prefix:\n    srcs = srcs + native.glob(\n        [prefix + \"*.cc\"])\n    hdrs = hdrs + native.glob(\n        [prefix + \"*.h\"])\n\n  if_mkl(\n      native.cc_library(\n          name=name,\n          srcs=srcs,\n          hdrs=hdrs,\n          deps=deps,\n          alwayslink=alwayslink,\n          copts=copts,\n          nocopts=nocopts\n      ))\n\nregister_extension_info(\n    extension_name = \"tf_mkl_kernel_library\",\n    label_regex_for_dep = \"{extension_name}\",\n)\n\n# Bazel rules for building swig files.\ndef _py_wrap_cc_impl(ctx):\n  srcs = ctx.files.srcs\n  if len(srcs) != 1:\n    fail(\"Exactly one SWIG source file label must be specified.\", \"srcs\")\n  module_name = ctx.attr.module_name\n  src = ctx.files.srcs[0]\n  inputs = depset([src])\n  inputs += ctx.files.swig_includes\n  for dep in ctx.attr.deps:\n    inputs += dep.cc.transitive_headers\n  inputs += ctx.files._swiglib\n  inputs += ctx.files.toolchain_deps\n  swig_include_dirs = depset(_get_repository_roots(ctx, inputs))\n  swig_include_dirs += sorted([f.dirname for f in ctx.files._swiglib])\n  args = [\n      \"-c++\", \"-python\", \"-module\", module_name, \"-o\", ctx.outputs.cc_out.path,\n      \"-outdir\", ctx.outputs.py_out.dirname\n  ]\n  args += [\"-l\" + f.path for f in ctx.files.swig_includes]\n  args += [\"-I\" + i for i in swig_include_dirs]\n  args += [src.path]\n  outputs = [ctx.outputs.cc_out, ctx.outputs.py_out]\n  ctx.action(\n      executable=ctx.executable._swig,\n      arguments=args,\n      inputs=list(inputs),\n      outputs=outputs,\n      mnemonic=\"PythonSwig\",\n      progress_message=\"SWIGing \" + src.path)\n  return struct(files=depset(outputs))\n\n_py_wrap_cc = rule(\n    attrs = {\n        \"srcs\": attr.label_list(\n            mandatory = True,\n            allow_files = True,\n        ),\n        \"swig_includes\": attr.label_list(\n            cfg = \"data\",\n            allow_files = True,\n        ),\n        \"deps\": attr.label_list(\n            allow_files = True,\n            providers = [\"cc\"],\n        ),\n        \"toolchain_deps\": attr.label_list(\n            allow_files = True,\n        ),\n        \"module_name\": attr.string(mandatory = True),\n        \"py_module_name\": attr.string(mandatory = True),\n        \"_swig\": attr.label(\n            default = Label(\"@swig//:swig\"),\n            executable = True,\n            cfg = \"host\",\n        ),\n        \"_swiglib\": attr.label(\n            default = Label(\"@swig//:templates\"),\n            allow_files = True,\n        ),\n    },\n    outputs = {\n        \"cc_out\": \"%{module_name}.cc\",\n        \"py_out\": \"%{py_module_name}.py\",\n    },\n    implementation = _py_wrap_cc_impl,\n)\n\ndef _get_repository_roots(ctx, files):\n  \"\"\"Returns abnormal root directories under which files reside.\n\n  When running a ctx.action, source files within the main repository are all\n  relative to the current directory; however, files that are generated or exist\n  in remote repositories will have their root directory be a subdirectory,\n  e.g. bazel-out/local-fastbuild/genfiles/external/jpeg_archive. This function\n  returns the set of these devious directories, ranked and sorted by popularity\n  in order to hopefully minimize the number of I/O system calls within the\n  compiler, because includes have quadratic complexity.\n  \"\"\"\n  result = {}\n  for f in files:\n    root = f.root.path\n    if root:\n      if root not in result:\n        result[root] = 0\n      result[root] -= 1\n    work = f.owner.workspace_root\n    if work:\n      if root:\n        root += \"/\"\n      root += work\n    if root:\n      if root not in result:\n        result[root] = 0\n      result[root] -= 1\n  return [k for v, k in sorted([(v, k) for k, v in result.items()])]\n\n# Bazel rule for collecting the header files that a target depends on.\ndef _transitive_hdrs_impl(ctx):\n  outputs = depset()\n  for dep in ctx.attr.deps:\n    outputs += dep.cc.transitive_headers\n  return struct(files=outputs)\n\n_transitive_hdrs = rule(\n    attrs = {\n        \"deps\": attr.label_list(\n            allow_files = True,\n            providers = [\"cc\"],\n        ),\n    },\n    implementation = _transitive_hdrs_impl,\n)\n\ndef transitive_hdrs(name, deps=[], **kwargs):\n  _transitive_hdrs(name=name + \"_gather\", deps=deps)\n  native.filegroup(name=name, srcs=[\":\" + name + \"_gather\"])\n\n# Create a header only library that includes all the headers exported by\n# the libraries in deps.\ndef cc_header_only_library(name, deps=[], includes=[], **kwargs):\n  _transitive_hdrs(name=name + \"_gather\", deps=deps)\n\n  # We could generalize the following, but rather than complicate things\n  # here, we'll do the minimal use case for now, and hope bazel comes up\n  # with a better solution before too long.  We'd expect it to compute\n  # the right include path by itself, but it doesn't, possibly because\n  # _transitive_hdrs lost some information about the include path.\n  if \"@nsync//:nsync_headers\" in deps:\n    # Buiding tensorflow from @org_tensorflow finds this two up.\n    nsynch = \"../../external/nsync/public\"\n    # Building tensorflow from elsewhere finds it four up.\n    # Note that native.repository_name() is not yet available in TF's Kokoro.\n    if REPOSITORY_NAME != \"@\":\n      nsynch = \"../../\" + nsynch\n    includes = includes[:]\n    includes.append(nsynch)\n\n  native.cc_library(name=name,\n                    hdrs=[\":\" + name + \"_gather\"],\n                    includes=includes,\n                    **kwargs)\n\ndef tf_custom_op_library_additional_deps():\n  return [\n      \"@protobuf_archive//:protobuf_headers\",\n      \"@nsync//:nsync_headers\",\n      clean_dep(\"//third_party/eigen3\"),\n      clean_dep(\"//tensorflow/core:framework_headers_lib\"),\n  ]\n\n# Traverse the dependency graph along the \"deps\" attribute of the\n# target and return a struct with one field called 'tf_collected_deps'.\n# tf_collected_deps will be the union of the deps of the current target\n# and the tf_collected_deps of the dependencies of this target.\ndef _collect_deps_aspect_impl(target, ctx):\n  alldeps = depset()\n  if hasattr(ctx.rule.attr, \"deps\"):\n    for dep in ctx.rule.attr.deps:\n      alldeps = alldeps | depset([dep.label])\n      if hasattr(dep, \"tf_collected_deps\"):\n        alldeps = alldeps | dep.tf_collected_deps\n  return struct(tf_collected_deps=alldeps)\n\ncollect_deps_aspect = aspect(\n    attr_aspects = [\"deps\"],\n    implementation = _collect_deps_aspect_impl,\n)\n\ndef _dep_label(dep):\n  label = dep.label\n  return label.package + \":\" + label.name\n\n# This rule checks that the transitive dependencies of targets listed\n# in the 'deps' attribute don't depend on the targets listed in\n# the 'disallowed_deps' attribute.\ndef _check_deps_impl(ctx):\n  disallowed_deps = ctx.attr.disallowed_deps\n  for input_dep in ctx.attr.deps:\n    if not hasattr(input_dep, \"tf_collected_deps\"):\n      continue\n    for dep in input_dep.tf_collected_deps:\n      for disallowed_dep in disallowed_deps:\n        if dep == disallowed_dep.label:\n          fail(\n              _dep_label(input_dep) + \" cannot depend on \" + _dep_label(\n                  disallowed_dep))\n  return struct()\n\ncheck_deps = rule(\n    _check_deps_impl,\n    attrs = {\n        \"deps\": attr.label_list(\n            aspects = [collect_deps_aspect],\n            mandatory = True,\n            allow_files = True,\n        ),\n        \"disallowed_deps\": attr.label_list(\n            mandatory = True,\n            allow_files = True,\n        ),\n    },\n)\n\n# Helper to build a dynamic library (.so) from the sources containing\n# implementations of custom ops and kernels.\ndef tf_custom_op_library(name, srcs=[], gpu_srcs=[], deps=[], linkopts=[]):\n  cuda_deps = [\n      clean_dep(\"//tensorflow/core:stream_executor_headers_lib\"),\n      \"@local_config_cuda//cuda:cuda_headers\",\n      \"@local_config_cuda//cuda:cudart_static\",\n  ]\n  deps = deps + tf_custom_op_library_additional_deps()\n  if gpu_srcs:\n    basename = name.split(\".\")[0]\n    native.cc_library(\n        name=basename + \"_gpu\",\n        srcs=gpu_srcs,\n        copts=_cuda_copts(),\n        deps=deps + if_cuda(cuda_deps))\n    cuda_deps.extend([\":\" + basename + \"_gpu\"])\n\n  check_deps(\n      name=name + \"_check_deps\",\n      deps=deps + if_cuda(cuda_deps),\n      disallowed_deps=[\n          clean_dep(\"//tensorflow/core:framework\"),\n          clean_dep(\"//tensorflow/core:lib\")\n      ])\n  tf_cc_shared_object(\n      name=name,\n      srcs=srcs,\n      deps=deps + if_cuda(cuda_deps),\n      data=[name + \"_check_deps\"],\n      copts=tf_copts(is_external=True),\n      linkopts=linkopts + select({\n          \"//conditions:default\": [\n              \"-lm\",\n          ],\n          clean_dep(\"//tensorflow:windows\"): [],\n          clean_dep(\"//tensorflow:windows_msvc\"): [],\n          clean_dep(\"//tensorflow:darwin\"): [],\n      }),)\n\nregister_extension_info(\n    extension_name = \"tf_custom_op_library\",\n    label_regex_for_dep = \"{extension_name}\",\n)\n\ndef tf_custom_op_py_library(name,\n                            srcs=[],\n                            dso=[],\n                            kernels=[],\n                            srcs_version=\"PY2AND3\",\n                            visibility=None,\n                            deps=[]):\n  kernels = kernels  # unused argument\n  native.py_library(\n      name=name,\n      data=dso,\n      srcs=srcs,\n      srcs_version=srcs_version,\n      visibility=visibility,\n      deps=deps,)\n\nregister_extension_info(\n    extension_name = \"tf_custom_op_py_library\",\n    label_regex_for_dep = \"{extension_name}\",\n)\n\ndef tf_extension_linkopts():\n  return []  # No extension link opts\n\ndef tf_extension_copts():\n  return []  # No extension c opts\n\n# In tf_py_wrap_cc generated libraries\n# module init functions are not exported unless\n# they contain one of the keywords in the version file\n# this prevents custom python modules.\n# This function attempts to append init_module_name to list of\n# exported functions in version script\ndef _append_init_to_versionscript_impl(ctx):\n  mod_name = ctx.attr.module_name\n  if ctx.attr.is_version_script:\n    ctx.actions.expand_template(\n      template=ctx.file.template_file,\n      output=ctx.outputs.versionscript,\n      substitutions={\n        \"global:\":\"global:\\n     init_%s;\\n     PyInit_*;\"%(mod_name),\n      },\n      is_executable=False,\n    )\n  else:\n    ctx.actions.expand_template(\n      template=ctx.file.template_file,\n      output=ctx.outputs.versionscript,\n      substitutions={\n        \"*tensorflow*\":\"*tensorflow*\\ninit_%s\\nPyInit_*\\n\"%(mod_name),\n      },\n      is_executable=False,\n    )\n\n\n_append_init_to_versionscript= rule(\n  implementation=_append_init_to_versionscript_impl,\n  attrs={\n    \"module_name\":attr.string(mandatory=True),\n    \"template_file\":attr.label(allow_files=True,single_file=True,mandatory=True),\n    \"is_version_script\":attr.bool(default=True,\n      doc='whether target is a ld version script or exported symbol list',\n      mandatory=False),\n  },\n  outputs={\"versionscript\":\"%{name}.lds\"},\n)\n\ndef tf_py_wrap_cc(name,\n                             srcs,\n                             swig_includes=[],\n                             deps=[],\n                             copts=[],\n                             **kwargs):\n  module_name = name.split(\"/\")[-1]\n  # Convert a rule name such as foo/bar/baz to foo/bar/_baz.so\n  # and use that as the name for the rule producing the .so file.\n  cc_library_name = \"/\".join(name.split(\"/\")[:-1] + [\"_\" + module_name + \".so\"])\n  cc_library_pyd_name = \"/\".join(\n      name.split(\"/\")[:-1] + [\"_\" + module_name + \".pyd\"])\n  extra_deps = []\n  _py_wrap_cc(\n      name=name + \"_py_wrap\",\n      srcs=srcs,\n      swig_includes=swig_includes,\n      deps=deps + extra_deps,\n      toolchain_deps=[\"//tools/defaults:crosstool\"],\n      module_name=module_name,\n      py_module_name=name)\n  vscriptname=name+\"_versionscript\"\n  _append_init_to_versionscript(\n      name=vscriptname,\n      module_name=module_name,\n      is_version_script=select({\n          \"@local_config_cuda//cuda:darwin\":False,\n          \"//conditions:default\":True,\n          }),\n      template_file=select({\n          \"@local_config_cuda//cuda:darwin\":clean_dep(\"//tensorflow:tf_exported_symbols.lds\"),\n          \"//conditions:default\":clean_dep(\"//tensorflow:tf_version_script.lds\")\n      })\n  )\n  extra_linkopts = select({\n      \"@local_config_cuda//cuda:darwin\": [\n          \"-Wl,-exported_symbols_list\",\n          \"%s.lds\"%vscriptname,\n      ],\n      clean_dep(\"//tensorflow:windows\"): [],\n      clean_dep(\"//tensorflow:windows_msvc\"): [],\n      \"//conditions:default\": [\n          \"-Wl,--version-script\",\n          \"%s.lds\"%vscriptname,\n      ]\n  })\n  extra_deps += select({\n      \"@local_config_cuda//cuda:darwin\": [\n          \"%s.lds\"%vscriptname,\n      ],\n      clean_dep(\"//tensorflow:windows\"): [],\n      clean_dep(\"//tensorflow:windows_msvc\"): [],\n      \"//conditions:default\": [\n          \"%s.lds\"%vscriptname,\n      ]\n  })\n\n  tf_cc_shared_object(\n      name=cc_library_name,\n      srcs=[module_name + \".cc\"],\n      copts=(copts + if_not_windows([\n          \"-Wno-self-assign\", \"-Wno-sign-compare\", \"-Wno-write-strings\"\n      ]) + tf_extension_copts()),\n      linkopts=tf_extension_linkopts() + extra_linkopts,\n      linkstatic=1,\n      deps=deps + extra_deps)\n  native.genrule(\n      name=\"gen_\" + cc_library_pyd_name,\n      srcs=[\":\" + cc_library_name],\n      outs=[cc_library_pyd_name],\n      cmd=\"cp $< $@\",)\n  native.py_library(\n      name=name,\n      srcs=[\":\" + name + \".py\"],\n      srcs_version=\"PY2AND3\",\n      data=select({\n          clean_dep(\"//tensorflow:windows\"): [\":\" + cc_library_pyd_name],\n          \"//conditions:default\": [\":\" + cc_library_name],\n      }))\n\n# This macro is for running python tests against system installed pip package\n# on Windows.\n#\n# py_test is built as an exectuable python zip file on Windows, which contains all\n# dependencies of the target. Because of the C++ extensions, it would be very\n# inefficient if the py_test zips all runfiles, plus we don't need them when running\n# tests against system installed pip package. So we'd like to get rid of the deps\n# of py_test in this case.\n#\n# In order to trigger the tests without bazel clean after getting rid of deps,\n# we introduce the following :\n# 1. When --define=no_tensorflow_py_deps=true, the py_test depends on a marker\n#    file of the pip package, the test gets to rerun when the pip package change.\n#    Note that this only works on Windows. See the definition of\n#    //third_party/tensorflow/tools/pip_package:win_pip_package_marker for specific reasons.\n# 2. When --define=no_tensorflow_py_deps=false (by default), it's a normal py_test.\ndef py_test(deps=[], data=[], **kwargs):\n  native.py_test(\n      deps=select({\n          \"//conditions:default\": deps,\n          clean_dep(\"//tensorflow:no_tensorflow_py_deps\"): [],\n      }),\n      data = data + select({\n          \"//conditions:default\": [],\n          clean_dep(\"//tensorflow:no_tensorflow_py_deps\"):\n          [\"//tensorflow/tools/pip_package:win_pip_package_marker\"],\n      }),\n      **kwargs)\n\nregister_extension_info(\n    extension_name = \"py_test\",\n    label_regex_for_dep = \"{extension_name}\",\n)\n\ndef tf_py_test(name,\n               srcs,\n               size=\"medium\",\n               data=[],\n               main=None,\n               args=[],\n               tags=[],\n               shard_count=1,\n               additional_deps=[],\n               flaky=0,\n               xla_enabled=False,\n               grpc_enabled=False):\n  if xla_enabled:\n    additional_deps = additional_deps + tf_additional_xla_deps_py()\n  if grpc_enabled:\n    additional_deps = additional_deps + tf_additional_grpc_deps_py()\n  py_test(\n      name=name,\n      size=size,\n      srcs=srcs,\n      main=main,\n      args=args,\n      tags=tags,\n      visibility=[clean_dep(\"//tensorflow:internal\")],\n      shard_count=shard_count,\n      data=data,\n      deps=[\n            clean_dep(\"//tensorflow/python:extra_py_tests_deps\"),\n            clean_dep(\"//tensorflow/python:gradient_checker\"),\n          ] + additional_deps,\n      flaky=flaky,\n      srcs_version=\"PY2AND3\")\n\nregister_extension_info(\n    extension_name = \"tf_py_test\",\n    label_regex_map = {\"additional_deps\": \"deps:{extension_name}\"},\n)\n\ndef cuda_py_test(name,\n                 srcs,\n                 size=\"medium\",\n                 data=[],\n                 main=None,\n                 args=[],\n                 shard_count=1,\n                 additional_deps=[],\n                 tags=[],\n                 flaky=0,\n                 xla_enabled=False,\n                 grpc_enabled=False):\n  test_tags = tags + tf_cuda_tests_tags()\n  tf_py_test(\n      name=name,\n      size=size,\n      srcs=srcs,\n      data=data,\n      main=main,\n      args=args,\n      tags=test_tags,\n      shard_count=shard_count,\n      additional_deps=additional_deps,\n      flaky=flaky,\n      xla_enabled=xla_enabled,\n      grpc_enabled=grpc_enabled)\n\nregister_extension_info(\n    extension_name = \"cuda_py_test\",\n    label_regex_map = {\"additional_deps\": \"additional_deps:{extension_name}\"},\n)\n\ndef sycl_py_test(name,\n                 srcs,\n                 size=\"medium\",\n                 data=[],\n                 main=None,\n                 args=[],\n                 shard_count=1,\n                 additional_deps=[],\n                 tags=[],\n                 flaky=0,\n                 xla_enabled=False,\n                 grpc_enabled=False):\n  test_tags = tags + tf_sycl_tests_tags()\n  tf_py_test(\n      name=name,\n      size=size,\n      srcs=srcs,\n      data=data,\n      main=main,\n      args=args,\n      tags=test_tags,\n      shard_count=shard_count,\n      additional_deps=additional_deps,\n      flaky=flaky,\n      xla_enabled=xla_enabled,\n      grpc_enabled=grpc_enabled)\n\nregister_extension_info(\n    extension_name = \"sycl_py_test\",\n    label_regex_map = {\"additional_deps\": \"additional_deps:{extension_name}\"},\n)\n\ndef py_tests(name,\n             srcs,\n             size=\"medium\",\n             additional_deps=[],\n             data=[],\n             tags=[],\n             shard_count=1,\n             prefix=\"\",\n             xla_enabled=False,\n             grpc_enabled=False):\n  for src in srcs:\n    test_name = src.split(\"/\")[-1].split(\".\")[0]\n    if prefix:\n      test_name = \"%s_%s\" % (prefix, test_name)\n    tf_py_test(\n        name=test_name,\n        size=size,\n        srcs=[src],\n        main=src,\n        tags=tags,\n        shard_count=shard_count,\n        data=data,\n        additional_deps=additional_deps,\n        xla_enabled=xla_enabled,\n        grpc_enabled=grpc_enabled)\n\ndef cuda_py_tests(name,\n                  srcs,\n                  size=\"medium\",\n                  additional_deps=[],\n                  data=[],\n                  shard_count=1,\n                  tags=[],\n                  prefix=\"\",\n                  xla_enabled=False,\n                  grpc_enabled=False):\n  test_tags = tags + tf_cuda_tests_tags()\n  py_tests(\n      name=name,\n      size=size,\n      srcs=srcs,\n      additional_deps=additional_deps,\n      data=data,\n      tags=test_tags,\n      shard_count=shard_count,\n      prefix=prefix,\n      xla_enabled=xla_enabled,\n      grpc_enabled=grpc_enabled)\n\n# Creates a genrule named <name> for running tools/proto_text's generator to\n# make the proto_text functions, for the protos passed in <srcs>.\n#\n# Return a struct with fields (hdrs, srcs) containing the names of the\n# generated files.\ndef tf_generate_proto_text_sources(name, srcs_relative_dir, srcs):\n  out_hdrs = (\n      [p.replace(\".proto\", \".pb_text.h\")\n       for p in srcs] + [p.replace(\".proto\", \".pb_text-impl.h\") for p in srcs])\n  out_srcs = [p.replace(\".proto\", \".pb_text.cc\") for p in srcs]\n  native.genrule(\n      name=name,\n      srcs=srcs + [clean_dep(\"//tensorflow/tools/proto_text:placeholder.txt\")],\n      outs=out_hdrs + out_srcs,\n      cmd=\n      \"$(location //tensorflow/tools/proto_text:gen_proto_text_functions) \"\n      + \"$(@D) \" + srcs_relative_dir + \" $(SRCS)\",\n      tools=[\n          clean_dep(\"//tensorflow/tools/proto_text:gen_proto_text_functions\")\n      ],)\n  return struct(hdrs=out_hdrs, srcs=out_srcs)\n\ndef tf_genrule_cmd_append_to_srcs(to_append):\n  return (\"cat $(SRCS) > $(@) && \" + \"echo >> $(@) && \" + \"echo \" + to_append +\n          \" >> $(@)\")\n\ndef tf_version_info_genrule():\n  native.genrule(\n      name=\"version_info_gen\",\n      srcs=[\n          clean_dep(\"@local_config_git//:gen/spec.json\"),\n          clean_dep(\"@local_config_git//:gen/head\"),\n          clean_dep(\"@local_config_git//:gen/branch_ref\"),\n      ],\n      outs=[\"util/version_info.cc\"],\n      cmd=\n      \"$(location //tensorflow/tools/git:gen_git_source.py) --generate $(SRCS) \\\"$@\\\"\",\n      local=1,\n      tools=[clean_dep(\"//tensorflow/tools/git:gen_git_source.py\")],)\n\ndef tf_py_build_info_genrule():\n  native.genrule(\n      name=\"py_build_info_gen\",\n      outs=[\"platform/build_info.py\"],\n      cmd=\n      \"$(location //tensorflow/tools/build_info:gen_build_info.py) --raw_generate \\\"$@\\\" --build_config \" + if_cuda(\"cuda\", \"cpu\"),\n      local=1,\n      tools=[clean_dep(\"//tensorflow/tools/build_info:gen_build_info.py\")],)\n\ndef cc_library_with_android_deps(deps,\n                                 android_deps=[],\n                                 common_deps=[],\n                                 copts=tf_copts(),\n                                 **kwargs):\n  deps = if_not_android(deps) + if_android(android_deps) + common_deps\n  native.cc_library(deps=deps, copts=copts, **kwargs)\n\nregister_extension_info(\n    extension_name = \"cc_library_with_android_deps\",\n    label_regex_for_dep = \"{extension_name}\",\n)\n", "subject": "Merge pull request #2997 from maciekcc/branch_125575345", "message": "Merge pull request #2997 from maciekcc/branch_125575345\n\nBranch 125575345" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "tensorflow/tensorflow", "file": "cuda_dnn.cc", "language": "cc", "commit_date": "2016-06-22 22:16:40.000 UTC", "content": "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/stream_executor/cuda/cuda_dnn.h\"\n\n#include <functional>\n#include <memory>\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"tensorflow/core/util/env_var.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_activation.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_diagnostics.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_driver.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_gpu_executor.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_platform_id.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_stream.h\"\n#include \"tensorflow/stream_executor/cuda/cuda_timer.h\"\n#include \"tensorflow/stream_executor/dnn.h\"\n#include \"tensorflow/stream_executor/lib/env.h\"\n#include \"tensorflow/stream_executor/lib/error.h\"\n#include \"tensorflow/stream_executor/lib/initialize.h\"\n#include \"tensorflow/stream_executor/lib/strcat.h\"\n#include \"tensorflow/stream_executor/lib/stringpiece.h\"\n#include \"tensorflow/stream_executor/lib/threadpool.h\"\n#include \"tensorflow/stream_executor/platform/logging.h\"\n#include \"tensorflow/stream_executor/plugin_registry.h\"\n#include \"tensorflow/stream_executor/scratch_allocator.h\"\n#include \"tensorflow/stream_executor/stream.h\"\n#include \"tensorflow/stream_executor/stream_executor_pimpl.h\"\n// clang-format off\n#include \"cuda/include/cudnn.h\"\n// clang-format on\n\nnamespace {\n\n// Converts (via narrowing) a type T value to a type U, and checks that the\n// value has no value change due to the conversion.\ntemplate <typename WideT, typename NarrowT>\nNarrowT CheckedNarrowing(const WideT& wide) {\n  NarrowT narrow = wide;\n  CHECK_EQ(narrow, wide)\n      << \"checked narrowing failed; values not equal post-conversion\";\n  return narrow;\n}\n\n// Returns the \"Compatibility\" version number from the CuDNN version number.\n// This is the number that tries to indicate ABI compatibility.\n//\n// For example, if cudnn_version is 5107, the compatibility version\n// number will be 5100.\nsize_t cudnnCompatibilityVersion(size_t cudnn_version) {\n  return (cudnn_version / 100) * 100;\n}\n\n}  // namespace\n\nnamespace perftools {\nnamespace gputools {\n\nusing dnn::BatchDescriptor;\nusing dnn::FilterDescriptor;\nusing dnn::ConvolutionDescriptor;\nusing dnn::PoolingDescriptor;\nusing dnn::NormalizeDescriptor;\n\nnamespace cuda {\n\nPLUGIN_REGISTRY_DEFINE_PLUGIN_ID(kCuDnnPlugin);\n\nstring ToString(cudnnStatus_t status) {\n  switch (status) {\n    case CUDNN_STATUS_SUCCESS:\n      return \"CUDNN_STATUS_SUCCESS\";\n    case CUDNN_STATUS_NOT_INITIALIZED:\n      return \"CUDNN_STATUS_NOT_INITIALIZED\";\n    case CUDNN_STATUS_ALLOC_FAILED:\n      return \"CUDNN_STATUS_ALLOC_FAILED\";\n    case CUDNN_STATUS_BAD_PARAM:\n      return \"CUDNN_STATUS_BAD_PARAM\";\n    case CUDNN_STATUS_INTERNAL_ERROR:\n      return \"CUDNN_STATUS_INTERNAL_ERROR\";\n    case CUDNN_STATUS_INVALID_VALUE:\n      return \"CUDNN_STATUS_INVALID_VALUE\";\n    case CUDNN_STATUS_ARCH_MISMATCH:\n      return \"CUDNN_STATUS_ARCH_MISMATCH\";\n    case CUDNN_STATUS_MAPPING_ERROR:\n      return \"CUDNN_STATUS_MAPPING_ERROR\";\n    case CUDNN_STATUS_EXECUTION_FAILED:\n      return \"CUDNN_STATUS_EXECUTION_FAILED\";\n    case CUDNN_STATUS_NOT_SUPPORTED:\n      return \"CUDNN_STATUS_NOT_SUPPORTED\";\n    case CUDNN_STATUS_LICENSE_ERROR:\n      return \"CUDNN_STATUS_LICENSE_ERROR\";\n    default:\n      return port::StrCat(\"<unknown cudnn status: \", static_cast<int>(status),\n                          \">\");\n  }\n}\n\ntemplate <typename T>\ncudnnDataType_t GetCudnnDataType();\n\ntemplate <>\ncudnnDataType_t GetCudnnDataType<double>() {\n  return CUDNN_DATA_DOUBLE;\n}\n\ntemplate <>\ncudnnDataType_t GetCudnnDataType<float>() {\n  return CUDNN_DATA_FLOAT;\n}\n\ntemplate <>\ncudnnDataType_t GetCudnnDataType<Eigen::half>() {\n  return CUDNN_DATA_HALF;\n}\n\nnamespace wrap {\n\nstatic port::ThreadPool* InitCudnnThreadpool() {\n  port::ThreadPool* cudnn_threadpool_;\n  port::ThreadOptions options;\n  // TBD(keveman): Conservatively setting the stack size and guard size to 2MB,\n  // until we can get some guarantees from NVIDIA on the minimum stack space\n  // they will work with.\n  options.stack_size = 2 * 1024 * 1024;\n  options.guard_size = 2 * 1024 * 1024;\n  cudnn_threadpool_ = new port::ThreadPool(port::Env::Default(), options,\n                                           \"cudnn_threadpool\", 1);\n  CHECK(cudnn_threadpool_);\n  return cudnn_threadpool_;\n}\n\nstatic mutex cudnn_threadpool_mu(LINKER_INITIALIZED);\nstatic port::ThreadPool* GetCudaThreadpool() {\n  mutex_lock lock(cudnn_threadpool_mu);\n  static port::ThreadPool* cudnn_threadpool = InitCudnnThreadpool();\n  return cudnn_threadpool;\n}\n\n#define PERFTOOLS_GPUTOOLS_CUDNN_WRAP(__name)                      \\\n  struct WrapperShim__##__name {                                   \\\n    template <typename... Args>                                    \\\n    cudnnStatus_t operator()(CUDAExecutor* parent, Args... args) { \\\n      cuda::ScopedActivateExecutorContext sac{parent};             \\\n      cudnnStatus_t retval = ::__name(args...);                    \\\n      return retval;                                               \\\n    }                                                              \\\n  } __name;\n\n// clang-format off\n#define CUDNN_DNN_ROUTINE_EACH(__macro)                   \\\n  __macro(cudnnBatchNormalizationBackward)                \\\n  __macro(cudnnBatchNormalizationForwardInference)        \\\n  __macro(cudnnBatchNormalizationForwardTraining)         \\\n  __macro(cudnnGetConvolutionNdForwardOutputDim)          \\\n  __macro(cudnnGetConvolutionForwardAlgorithm)            \\\n  __macro(cudnnCreateTensorDescriptor)                    \\\n  __macro(cudnnDestroyTensorDescriptor)                   \\\n  __macro(cudnnCreateFilterDescriptor)                    \\\n  __macro(cudnnSetPoolingNdDescriptor)                    \\\n  __macro(cudnnSetLRNDescriptor)                          \\\n  __macro(cudnnDestroyFilterDescriptor)                   \\\n  __macro(cudnnCreateConvolutionDescriptor)               \\\n  __macro(cudnnCreatePoolingDescriptor)                   \\\n  __macro(cudnnDestroyPoolingDescriptor)                  \\\n  __macro(cudnnCreateLRNDescriptor)                       \\\n  __macro(cudnnDestroyLRNDescriptor)                      \\\n  __macro(cudnnDestroyConvolutionDescriptor)              \\\n  __macro(cudnnCreate)                                    \\\n  __macro(cudnnDestroy)                                   \\\n  __macro(cudnnSetStream)                                 \\\n  __macro(cudnnActivationForward)                         \\\n  __macro(cudnnConvolutionForward)                        \\\n  __macro(cudnnConvolutionBackwardBias)                   \\\n  __macro(cudnnGetConvolutionForwardWorkspaceSize)        \\\n  __macro(cudnnTransformTensor)                           \\\n  __macro(cudnnSetConvolutionNdDescriptor)                \\\n  __macro(cudnnSetTensor4dDescriptor)                     \\\n  __macro(cudnnSetTensorNdDescriptor)                     \\\n  __macro(cudnnSetFilterNdDescriptor)                     \\\n  __macro(cudnnPoolingForward)                            \\\n  __macro(cudnnPoolingBackward)                           \\\n  __macro(cudnnLRNCrossChannelForward)                    \\\n  __macro(cudnnLRNCrossChannelBackward)                   \\\n  __macro(cudnnAddTensor)                                 \\\n  __macro(cudnnConvolutionBackwardData)                   \\\n  __macro(cudnnConvolutionBackwardFilter)\n// clang-format on\n\nCUDNN_DNN_ROUTINE_EACH(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n\n// APIs available after R3:\n#if CUDNN_VERSION >= 3000\n#define CUDNN_DNN_ROUTINE_EACH_AFTER_R3(__macro)              \\\n  __macro(cudnnGetConvolutionBackwardFilterWorkspaceSize)     \\\n  __macro(cudnnGetConvolutionBackwardDataAlgorithm)           \\\n  __macro(cudnnGetConvolutionBackwardFilterAlgorithm)         \\\n  __macro(cudnnGetConvolutionBackwardDataWorkspaceSize)\nCUDNN_DNN_ROUTINE_EACH_AFTER_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n#undef CUDNN_DNN_ROUTINE_EACH_AFTER_R3\n#endif\n\n// APIs in R3 but not in R5\n// clang-format off\n#if CUDNN_VERSION >= 3000 && CUDNN_VERSION < 5000\n#define CUDNN_DNN_ROUTINE_EACH_R3(__macro)                    \\\n  __macro(cudnnAddTensor_v3)                                  \\\n  __macro(cudnnConvolutionBackwardData_v3)                    \\\n  __macro(cudnnConvolutionBackwardFilter_v3)\n// clang-format on\n\nCUDNN_DNN_ROUTINE_EACH_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n#undef CUDNN_DNN_ROUTINE_EACH_R3\n#endif\n\n// APIs in R5\n// clang-format off\n#if CUDNN_VERSION >= 5000\n#define CUDNN_DNN_ROUTINE_EACH_R5(__macro)                    \\\n  __macro(cudnnCreateActivationDescriptor)                    \\\n  __macro(cudnnSetActivationDescriptor)                       \\\n  __macro(cudnnGetActivationDescriptor)                       \\\n  __macro(cudnnDestroyActivationDescriptor)                   \\\n  __macro(cudnnCreateDropoutDescriptor)                       \\\n  __macro(cudnnDestroyDropoutDescriptor)                      \\\n  __macro(cudnnSetDropoutDescriptor)                          \\\n  __macro(cudnnDropoutGetStatesSize)                          \\\n  __macro(cudnnCreateRNNDescriptor)                           \\\n  __macro(cudnnDestroyRNNDescriptor)                          \\\n  __macro(cudnnGetRNNParamsSize)                              \\\n  __macro(cudnnGetRNNWorkspaceSize)                           \\\n  __macro(cudnnGetRNNTrainingReserveSize)                     \\\n  __macro(cudnnGetRNNLinLayerMatrixParams)                    \\\n  __macro(cudnnGetRNNLinLayerBiasParams)                      \\\n  __macro(cudnnRNNForwardInference)                           \\\n  __macro(cudnnRNNForwardTraining)                            \\\n  __macro(cudnnRNNBackwardData)                               \\\n  __macro(cudnnRNNBackwardWeights)                            \\\n  __macro(cudnnSetRNNDescriptor)                              \\\n  __macro(cudnnGetFilterNdDescriptor)\n\n// clang-format on\n\nCUDNN_DNN_ROUTINE_EACH_R5(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n#undef CUDNN_DNN_ROUTINE_EACH_R5\n#endif\n\n// APIs in R6\n// clang-format off\n#if CUDNN_VERSION >= 6000\n#define CUDNN_DNN_ROUTINE_EACH_R6(__macro)                    \\\n  __macro(cudnnConvolutionBiasActivationForward)              \\\n  __macro(cudnnSetRNNDescriptor_v6)\n\n// clang-format on\nCUDNN_DNN_ROUTINE_EACH_R6(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n#undef CUDNN_DNN_ROUTINE_EACH_R6\n#endif\n\n// APIs in R7\n// clang-format off\n#if CUDNN_VERSION >= 7000\n#define CUDNN_DNN_ROUTINE_EACH_R7(__macro)                    \\\n  __macro(cudnnSetConvolutionMathType)\n\n// clang-format on\nCUDNN_DNN_ROUTINE_EACH_R7(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n#undef CUDNN_DNN_ROUTINE_EACH_R7\n#endif\n\n#undef CUDNN_DNN_ROUTINE_EACH\n\n}  // namespace wrap\n\nnamespace {\n\ncudnnHandle_t ToHandle(void* opaque_handle) {\n  return static_cast<cudnnHandle_t>(opaque_handle);\n}\n\ncudnnConvolutionFwdAlgo_t ToConvForwardAlgo(dnn::AlgorithmDesc algorithm) {\n  cudnnConvolutionFwdAlgo_t algo =\n      cudnnConvolutionFwdAlgo_t(algorithm.algo_id());\n  switch (algo) {\n    case CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM:\n    case CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM:\n    case CUDNN_CONVOLUTION_FWD_ALGO_GEMM:\n    case CUDNN_CONVOLUTION_FWD_ALGO_DIRECT:\n    case CUDNN_CONVOLUTION_FWD_ALGO_FFT:\n    case CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING:\n#if CUDNN_VERSION >= 5000\n    case CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD:\n#endif\n#if CUDNN_VERSION >= 5100\n    case CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED:\n#endif\n      return algo;\n    default:\n      LOG(FATAL) << \"Unsupported Cudnn convolution forward algorithm: \"\n                 << algorithm.algo_id();\n  }\n}\n\ncudnnConvolutionBwdDataAlgo_t ToConvBackwardDataAlgo(\n    dnn::AlgorithmDesc algorithm) {\n  cudnnConvolutionBwdDataAlgo_t algo =\n      cudnnConvolutionBwdDataAlgo_t(algorithm.algo_id());\n  switch (algo) {\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_0:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_1:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT:\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING:\n#if CUDNN_VERSION >= 5000\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD:\n#endif\n#if CUDNN_VERSION >= 5100\n    case CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED:\n#endif\n      return algo;\n    default:\n      LOG(FATAL)\n          << \"Unsupported Cudnn convolution backward algorithm for data: \"\n          << algorithm.algo_id();\n  }\n}\n\ncudnnConvolutionBwdFilterAlgo_t ToConvBackwardFilterAlgo(\n    dnn::AlgorithmDesc algorithm) {\n  cudnnConvolutionBwdFilterAlgo_t algo =\n      cudnnConvolutionBwdFilterAlgo_t(algorithm.algo_id());\n  switch (algo) {\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3:\n#if CUDNN_VERSION >= 5100\n    // Based on cudnn.h, the following is not implemented.\n    // case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD:\n    case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED:\n#endif\n      return algo;\n    default:\n      LOG(FATAL)\n          << \"Unsupported Cudnn convolution backward algorithm for filter: \"\n          << algorithm.algo_id();\n  }\n}\n\n}  // namespace\n\nCudnnSupport::CudnnSupport(CUDAExecutor* parent)\n    : parent_(parent), dnn_handle_(nullptr) {}\n\nCudnnSupport::~CudnnSupport() {\n  auto status = wrap::cudnnDestroy(parent_, ToHandle(dnn_handle_));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"could not destroy cudnn handle: \" << ToString(status);\n  }\n}\n\nport::Status CudnnSupport::Init() {\n  auto status = wrap::cudnnCreate(\n      parent_, reinterpret_cast<cudnnHandle_t*>(&dnn_handle_));\n  if (status == CUDNN_STATUS_SUCCESS) {\n    // Check whether loaded version of CuDNN matches what the source\n    // was built with.\n    size_t loaded_version = ::cudnnGetVersion();\n    size_t loaded_compat_version = cudnnCompatibilityVersion(loaded_version);\n    size_t compiled_compat_version = cudnnCompatibilityVersion(CUDNN_VERSION);\n    bool library_loaded_matches_source =\n        (loaded_compat_version == compiled_compat_version);\n    if (!library_loaded_matches_source) {\n      const string error =\n          port::StrCat(\"Loaded runtime CuDNN library: \", loaded_version,\n                       \" (compatibility version \", loaded_compat_version,\n                       \") but source was compiled with \", CUDNN_VERSION,\n                       \" (compatibility version \", compiled_compat_version,\n                       \").  If using a binary install, upgrade your CuDNN \"\n                       \"library to match.  If building from sources, \"\n                       \"make sure the library loaded at runtime matches a \"\n                       \"compatible version specified during compile \"\n                       \"configuration.\");\n      LOG(ERROR) << error;\n      return port::Status{port::error::INTERNAL, error};\n    }\n\n    return port::Status::OK();\n  }\n\n  LOG(ERROR) << \"could not create cudnn handle: \" << ToString(status);\n  if (status == CUDNN_STATUS_NOT_INITIALIZED) {\n    auto result = cuda::Diagnostician::FindKernelDriverVersion();\n    if (!result.ok()) {\n      LOG(ERROR) << \"error retrieving driver version: \"\n                 << DriverVersionStatusToString(result);\n    } else {\n      const auto& version = result.ValueOrDie();\n      LOG(ERROR) << \"possibly insufficient driver version: \"\n                 << DriverVersionToString(version);\n      // OS X kernel driver does not report version accurately\n#if !defined(__APPLE__)\n      if (std::get<0>(version) < 340) {\n        LOG(ERROR)\n            << \"cudnn library is only supported on 340.XX+ driver versions\";\n      }\n#endif\n    }\n  }\n\n  return port::Status{port::error::INTERNAL,\n                      port::StrCat(\"cudnn library could not create a handle: \",\n                                   ToString(status))};\n}\n\n// Turns a BatchDescriptor structure into a cudnn tensor handle within a scope.\nclass ScopedTensorDescriptor {\n public:\n  ScopedTensorDescriptor(CUDAExecutor* parent,\n                         const BatchDescriptor& batch_descriptor,\n                         cudnnDataType_t elem_type)\n      : parent_(parent), handle_(nullptr) {\n    cudnnStatus_t status = wrap::cudnnCreateTensorDescriptor(parent_, &handle_);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(FATAL) << \"could not create cudnn tensor descriptor: \"\n                 << ToString(status);\n    }\n\n    switch (batch_descriptor.layout()) {\n      case dnn::DataLayout::kBatchYXDepth:\n      case dnn::DataLayout::kBatchDepthYX: {\n        const int nd = batch_descriptor.ndims() + 2;\n        // cuDNN requires the strides and dims to be ordered as BDYX.\n        std::vector<int64> strides64 =\n            batch_descriptor.full_strides(dnn::DataLayout::kBatchDepthYX);\n        std::vector<int64> dims64 =\n            batch_descriptor.full_dims(dnn::DataLayout::kBatchDepthYX);\n\n        // cuDNN requires arrays of ints.\n        std::vector<int> strides(nd);\n        std::vector<int> dims(nd);\n        std::transform(strides64.cbegin(), strides64.cend(), strides.begin(),\n                       &CheckedNarrowing<int64, int>);\n        std::transform(dims64.cbegin(), dims64.cend(), dims.begin(),\n                       &CheckedNarrowing<int64, int>);\n        status = wrap::cudnnSetTensorNdDescriptor(\n            parent_, handle_, elem_type, nd, dims.data(), strides.data());\n\n        if (status != CUDNN_STATUS_SUCCESS) {\n          LOG(FATAL) << \"could not convert BatchDescriptor \"\n                     << batch_descriptor.ToString()\n                     << \" to cudnn tensor descriptor: \" << ToString(status);\n        }\n      } break;\n#if CUDNN_VERSION >= 6000\n      case dnn::DataLayout::kBatchDepthYX4: {\n        status = wrap::cudnnSetTensor4dDescriptor(\n            parent_, handle_, CUDNN_TENSOR_NCHW_VECT_C, elem_type,\n            batch_descriptor.count(), batch_descriptor.feature_map_count(),\n            batch_descriptor.height(), batch_descriptor.width());\n        if (status != CUDNN_STATUS_SUCCESS) {\n          LOG(FATAL) << \"could not convert BatchDescriptor \"\n                     << batch_descriptor.ToString()\n                     << \" to cudnn tensor descriptor: \" << ToString(status);\n        }\n      } break;\n#endif\n      default:\n        LOG(FATAL) << \"Unsupported tensor format \"\n                   << DataLayoutString(batch_descriptor.layout());\n        break;\n    }\n  }\n\n  ~ScopedTensorDescriptor() {\n    cudnnStatus_t status = wrap::cudnnDestroyTensorDescriptor(parent_, handle_);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(ERROR) << \"could not destroy cudnn tensor descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  cudnnTensorDescriptor_t handle() const { return handle_; }\n\n private:\n  CUDAExecutor* parent_;            // Parent executor. Not owned.\n  cudnnTensorDescriptor_t handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedTensorDescriptor);\n};\n\n// Turns a FilterDescriptor structure into a cudnn filter handle within a scope.\nclass ScopedFilterDescriptor {\n public:\n  ScopedFilterDescriptor(CUDAExecutor* parent,\n                         const FilterDescriptor& filter_descriptor,\n                         const BatchDescriptor& batch_descriptor,\n                         cudnnDataType_t elem_type)\n      : parent_(parent), handle_(nullptr) {\n    cudnnStatus_t status = wrap::cudnnCreateFilterDescriptor(parent_, &handle_);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(FATAL) << \"could not create cudnn filter descriptor: \"\n                 << ToString(status);\n    }\n\n#if CUDNN_VERSION >= 5000\n    // TODO(b/23032134): Even if the filter layout is not supported,\n    // cudnnSetFilter4DDescriptor_v4 will return CUDNN_STATUS_SUCCESS because it\n    // does not take layout as an input. Maybe force cuDNN by giving wrong\n    // inputs intentionally?\n    cudnnTensorFormat_t format;\n    switch (filter_descriptor.layout()) {\n      case dnn::FilterLayout::kOutputInputYX:\n        format = CUDNN_TENSOR_NCHW;\n        break;\n#if CUDNN_VERSION >= 6000\n      case dnn::FilterLayout::kOutputInputYX4:\n        format = CUDNN_TENSOR_NCHW_VECT_C;\n        break;\n#endif\n      default:\n        LOG(FATAL) << \"Unsupported filter format \"\n                   << FilterLayoutString(filter_descriptor.layout());\n        break;\n    }\n#endif\n\n    std::vector<int> dims(2 + filter_descriptor.ndims());\n    dims[0] = filter_descriptor.output_feature_map_count();\n    dims[1] = filter_descriptor.input_feature_map_count();\n    const auto& spatial_dims = filter_descriptor.input_filter_dims();\n    std::copy(spatial_dims.begin(), spatial_dims.end(), dims.begin() + 2);\n\n    status = wrap::cudnnSetFilterNdDescriptor(parent_, handle_, elem_type,\n#if CUDNN_VERSION >= 5000\n                                              format,\n#endif\n                                              dims.size(), dims.data());\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(FATAL) << \"could not set cudnn filter descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  ~ScopedFilterDescriptor() {\n    cudnnStatus_t status = wrap::cudnnDestroyFilterDescriptor(parent_, handle_);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(ERROR) << \"could not destroy cudnn filter descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  cudnnFilterDescriptor_t handle() const { return handle_; }\n\n private:\n  // Parent executor object. Not owned.\n  CUDAExecutor* parent_;\n\n  // cudnn filter descriptor this object creates. Owned.\n  cudnnFilterDescriptor_t handle_;\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedFilterDescriptor);\n};\n\n// A helper function to decide whether to enable the TENSOR_OP_MATH math type\nstatic bool TensorOpMathEnabled() {\n  static bool is_enabled = [] {\n    bool is_disabled = false;\n    TF_CHECK_OK(\n        tensorflow::ReadBoolFromEnvVar(\"TF_DISABLE_CUDNN_TENSOR_OP_MATH\",\n                                       /*default_val=*/false, &is_disabled));\n    return !is_disabled;\n  }();\n  return is_enabled;\n}\n\n// A helper function to decide whether to use CUDNN_BATCHNORM_SPATIAL_PERSISTENT\n// in batchnorm. This mode can be faster in some tasks because an optimized path\n// may be selected for CUDNN_DATA_FLOAT and CUDNN_DATA_HALF data types, compute\n// capability 6.0 or higher. The reason we set it to false by default is that\n// this mode may use scaled atomic integer reduction that may cause a numerical\n// overflow for certain input data range.\n// TODO(yangzihao): Use autotune to choose between this mode and\n// CUDNN_BATCHNORM_SPATIAL mode.\nstatic bool BatchnormSpatialPersistentEnabled() {\n  static bool is_enabled = [] {\n    bool is_enabled = false;\n    TF_CHECK_OK(tensorflow::ReadBoolFromEnvVar(\n        \"TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT\",\n        /*default_val=*/false, &is_enabled));\n    return is_enabled;\n  }();\n  return is_enabled;\n}\n\n// Turns a ConvolutionDescriptor structure into a cudnn convolution handle\n// within a scope.\nclass ScopedConvolutionDescriptor {\n public:\n  ScopedConvolutionDescriptor(\n      CUDAExecutor* parent, const ConvolutionDescriptor& convolution_descriptor,\n      cudnnDataType_t data_type)\n      : parent_(parent), handle_(nullptr) {\n    cudnnStatus_t status =\n        wrap::cudnnCreateConvolutionDescriptor(parent_, &handle_);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(FATAL) << \"could not create cudnn convolution descriptor: \"\n                 << ToString(status);\n    }\n    const auto& strides64 = convolution_descriptor.strides();\n    const auto& padding64 = convolution_descriptor.padding();\n    const auto& dilations64 = convolution_descriptor.dilations();\n    if (convolution_descriptor.pad_alignment() ==\n        dnn::PadAlignment::kTensorFlowPadding) {\n      LOG(ERROR) << \"TensorFlow padding alignment is not supported.\";\n    }\n\n    // cuDNN requires arrays of ints.\n    std::vector<int> strides(convolution_descriptor.ndims());\n    std::vector<int> padding(convolution_descriptor.ndims());\n    std::vector<int> dilations(convolution_descriptor.ndims());\n    std::transform(strides64.cbegin(), strides64.cend(), strides.begin(),\n                   &CheckedNarrowing<int64, int>);\n    std::transform(padding64.cbegin(), padding64.cend(), padding.begin(),\n                   &CheckedNarrowing<int64, int>);\n    // TODO(yangzihao): Test with negative dilation to make sure that cudnn\n    // doesn't crash.\n    std::transform(dilations64.cbegin(), dilations64.cend(), dilations.begin(),\n                   &CheckedNarrowing<int64, int>);\n\n    status = wrap::cudnnSetConvolutionNdDescriptor(\n        parent_, handle_, convolution_descriptor.ndims(), padding.data(),\n        strides.data(), dilations.data(),\n        // NOTE(keveman): cuDNN supports convolution and cross correlation.\n        // However, almost all the use cases do cross correlation, so just\n        // hard coding it here.\n        CUDNN_CROSS_CORRELATION, data_type);\n\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(FATAL) << \"could not set cudnn convolution descriptor: \"\n                 << ToString(status);\n    }\n    // NOTE(benbarsdell): This only applies if tensor op math is enabled\n    //                      and algo selection is set to Default.\n    this->set_use_tensor_op_math(true);\n  }\n\n  void set_use_tensor_op_math(bool use_tensor_op_math) {\n#if CUDNN_VERSION >= 7000\n    cudnnMathType_t math_type =\n        (use_tensor_op_math ? CUDNN_TENSOR_OP_MATH : CUDNN_DEFAULT_MATH);\n    if (TensorOpMathEnabled()) {\n      cudnnStatus_t status =\n          wrap::cudnnSetConvolutionMathType(parent_, handle_, math_type);\n      if (status != CUDNN_STATUS_SUCCESS) {\n        LOG(FATAL) << \"could not set cudnn convolution math type: \"\n                   << ToString(status);\n      }\n    }\n#endif\n  }\n\n  ~ScopedConvolutionDescriptor() {\n    cudnnStatus_t status =\n        wrap::cudnnDestroyConvolutionDescriptor(parent_, handle_);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(ERROR) << \"could not destroy cudnn convolution descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  cudnnConvolutionDescriptor_t handle() const { return handle_; }\n\n private:\n  CUDAExecutor* parent_;                 // Parent executor. Not owned.\n  cudnnConvolutionDescriptor_t handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedConvolutionDescriptor);\n};\n\n// Turns a PoolingDescriptor structure into a cudnn pooling descriptor handle\n// within a scope.\nclass ScopedPoolingDescriptor {\n public:\n  ScopedPoolingDescriptor(CUDAExecutor* parent,\n                          const PoolingDescriptor& pooling_descriptor)\n      : parent_(parent), handle_(nullptr) {\n    cudnnStatus_t status =\n        wrap::cudnnCreatePoolingDescriptor(parent_, &handle_);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(FATAL) << \"could not create cudnn pooling descriptor: \"\n                 << ToString(status);\n    }\n    const std::vector<int64> strides64 = pooling_descriptor.strides();\n    const std::vector<int64> padding64 = pooling_descriptor.padding();\n    const std::vector<int64> shape64 = pooling_descriptor.window();\n\n    const int nd = pooling_descriptor.ndims();\n    std::vector<int> shape(nd);\n    std::vector<int> padding(nd);\n    std::vector<int> strides(nd);\n    std::transform(strides64.cbegin(), strides64.cend(), strides.begin(),\n                   &CheckedNarrowing<int64, int>);\n    std::transform(padding64.cbegin(), padding64.cend(), padding.begin(),\n                   &CheckedNarrowing<int64, int>);\n    std::transform(shape64.cbegin(), shape64.cend(), shape.begin(),\n                   &CheckedNarrowing<int64, int>);\n    bool propagate_nans = pooling_descriptor.propagate_nans();\n    status = wrap::cudnnSetPoolingNdDescriptor(\n        parent_, handle_,\n        (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum\n             ? CUDNN_POOLING_MAX\n             : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING),\n#if CUDNN_VERSION >= 5000\n        propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN,\n#endif\n        nd, shape.data(), padding.data(), strides.data());\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(FATAL) << \"could not set cudnn pooling descriptor: \"\n                 << ToString(status);\n    }\n  }\n  ~ScopedPoolingDescriptor() {\n    cudnnStatus_t status =\n        wrap::cudnnDestroyPoolingDescriptor(parent_, handle_);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(ERROR) << \"could not destroy cudnn pooling descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  cudnnPoolingDescriptor_t handle() const { return handle_; }\n\n private:\n  CUDAExecutor* parent_;             // Parent executor. Not owned.\n  cudnnPoolingDescriptor_t handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedPoolingDescriptor);\n};\n\n// Turns a NormalizeDescriptor structure into a cudnn LRN descriptor handle.\nclass ScopedNormalizeDescriptor {\n public:\n  ScopedNormalizeDescriptor(CUDAExecutor* parent,\n                            const NormalizeDescriptor& normalize_descriptor)\n      : parent_(parent), handle_(nullptr) {\n    cudnnStatus_t status = wrap::cudnnCreateLRNDescriptor(parent_, &handle_);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(FATAL) << \"could not create cudnn LRN descriptor: \"\n                 << ToString(status);\n    }\n\n    // The range specifies that the indices in the closed range\n    // [i - range, i + range] should be included in the normalization for index\n    // i. The lrnN value is the total number of elements in the range, so\n    // lrnN = 2*range + 1.\n    unsigned lrnN = 2 * normalize_descriptor.range() + 1;\n\n    // Note that SE defines the normalization operation as\n    //\n    //  U_i = V_i / ((bias +  alpha      * (sum_j V_j^2)) ^ beta)\n    //\n    // but cuDNN defines it as\n    //\n    //  U_i = V_i / ((bias + (alpha / n) * (sum_j V_j^2)) ^ beta)\n    //\n    // i.e. there is a factor of n difference between the meaning of the alphas\n    // in the two contexts. The cuDNN alpha is n times the SE alpha.\n    double lrnAlpha = lrnN * normalize_descriptor.alpha();\n\n    double lrnBeta = normalize_descriptor.beta();\n    double lrnK = normalize_descriptor.bias();\n    status = wrap::cudnnSetLRNDescriptor(parent_, handle_, lrnN, lrnAlpha,\n                                         lrnBeta, lrnK);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(FATAL) << \"could not set cudnn LRN descriptor: \" << ToString(status);\n    }\n  }\n\n  ~ScopedNormalizeDescriptor() {\n    cudnnStatus_t status = wrap::cudnnDestroyLRNDescriptor(parent_, handle_);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(ERROR) << \"could not destroy cudnn LRN descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  cudnnLRNDescriptor_t handle() const { return handle_; }\n\n private:\n  CUDAExecutor* parent_;         // Parent executor. Not owned.\n  cudnnLRNDescriptor_t handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedNormalizeDescriptor);\n};\n\n#if CUDNN_VERSION >= 5000\n// Turns a ActivationDescriptor structure into a cudnn activation\n// descriptor handle within a scope.\nclass ScopedActivationDescriptor {\n public:\n  ScopedActivationDescriptor(CUDAExecutor* parent,\n                             dnn::ActivationMode activation_mode,\n                             cudnnNanPropagation_t nan_propagation,\n                             double value_max)\n      : parent_(parent), handle_(nullptr) {\n    cudnnStatus_t status =\n        wrap::cudnnCreateActivationDescriptor(parent_, &handle_);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(FATAL) << \"could not create cudnn activation descriptor: \"\n                 << ToString(status);\n    }\n\n    double relu_ceiling = 0.0;\n    cudnnActivationMode_t mode;\n    switch (activation_mode) {\n      case dnn::ActivationMode::kRelu6:\n        relu_ceiling = 6.0;\n        mode = CUDNN_ACTIVATION_CLIPPED_RELU;\n        break;\n      case dnn::ActivationMode::kReluX:\n        relu_ceiling = value_max;\n        mode = CUDNN_ACTIVATION_CLIPPED_RELU;\n        break;\n      case dnn::ActivationMode::kRelu:\n        mode = CUDNN_ACTIVATION_RELU;\n        break;\n      case dnn::ActivationMode::kSigmoid:\n        mode = CUDNN_ACTIVATION_SIGMOID;\n        break;\n      case dnn::ActivationMode::kTanh:\n        mode = CUDNN_ACTIVATION_TANH;\n        break;\n      default:\n        LOG(FATAL) << \"unrecognized activation mode: \"\n                   << static_cast<int>(activation_mode);\n    }\n\n    status = wrap::cudnnSetActivationDescriptor(parent_, handle_, mode,\n                                                nan_propagation, relu_ceiling);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(FATAL) << \"could not set cudnn activation descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  ~ScopedActivationDescriptor() {\n    cudnnStatus_t status =\n        wrap::cudnnDestroyActivationDescriptor(parent_, handle_);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(ERROR) << \"could not destroy cudnn activation descriptor: \"\n                 << ToString(status);\n    }\n  }\n\n  cudnnActivationDescriptor_t handle() const { return handle_; }\n\n private:\n  CUDAExecutor* parent_;                // Parent executor. Not owned.\n  cudnnActivationDescriptor_t handle_;  // Owned.\n\n  SE_DISALLOW_COPY_AND_ASSIGN(ScopedActivationDescriptor);\n};\n#endif\n\nnamespace {\ncudnnDataType_t ToCudnnDataType(\n    dnn::DataType data_type,\n    dnn::DataLayout data_layout = dnn::DataLayout::kBatchDepthYX) {\n  switch (data_type) {\n    case dnn::DataType::kFloat:\n    case dnn::DataType::kDouble:\n    case dnn::DataType::kHalf:\n      return static_cast<cudnnDataType_t>(data_type);\n#if CUDNN_VERSION >= 6000\n    case dnn::DataType::kInt8:\n      return data_layout == dnn::DataLayout::kBatchDepthYX4 ? CUDNN_DATA_INT8x4\n                                                            : CUDNN_DATA_INT8;\n#endif\n    default:\n      LOG(FATAL) << \"Invalid DNN data type: \" << static_cast<int>(data_type);\n  }\n}\n\n#if CUDNN_VERSION >= 5000\n\ncudnnRNNInputMode_t ToCudnnRnnInputMode(dnn::RnnInputMode input_mode) {\n  switch (input_mode) {\n    case dnn::RnnInputMode::kRnnLinearSkip:\n    case dnn::RnnInputMode::kRnnSkipInput:\n      return static_cast<cudnnRNNInputMode_t>(input_mode);\n    default:\n      LOG(FATAL) << \"Invalid RNN input mode: \" << static_cast<int>(input_mode);\n  }\n}\n\ncudnnDirectionMode_t ToCudnnRnnDirectionMode(\n    dnn::RnnDirectionMode direction_mode) {\n  switch (direction_mode) {\n    case dnn::RnnDirectionMode::kRnnUnidirectional:\n    case dnn::RnnDirectionMode::kRnnBidirectional:\n      return static_cast<cudnnDirectionMode_t>(direction_mode);\n    default:\n      LOG(FATAL) << \"Invalid RNN direction mode: \"\n                 << static_cast<int>(direction_mode);\n  }\n}\n\ncudnnRNNMode_t ToCudnnRnnMode(dnn::RnnMode rnn_mode) {\n  switch (rnn_mode) {\n    case dnn::RnnMode::kRnnRelu:\n    case dnn::RnnMode::kRnnTanh:\n    case dnn::RnnMode::kRnnLstm:\n    case dnn::RnnMode::kRnnGru:\n      return static_cast<cudnnRNNMode_t>(rnn_mode);\n    default:\n      LOG(FATAL) << \"Invalid RNN Mode: \" << static_cast<int>(rnn_mode);\n  }\n}\n\nint CudnnDataTypeToByteSize(cudnnDataType_t data_type) {\n  switch (data_type) {\n    case CUDNN_DATA_FLOAT:\n      return sizeof(float);\n    case CUDNN_DATA_DOUBLE:\n      return sizeof(double);\n    case CUDNN_DATA_HALF:\n      return sizeof(Eigen::half);\n    default:\n      LOG(FATAL) << \"Invalid DNN data type: \" << static_cast<int>(data_type);\n  }\n}\n\n#endif  // CUDNN_VERSION\n\ntemplate <typename Base>\nclass MixinBase : public Base {};\ntemplate <>\nclass MixinBase<void> {};\n\n}  // namespace\n\n#if CUDNN_VERSION >= 5000\n\n#define CUDNN_RETURN_IF_FAIL(STATUS, ...)                                \\\n  if (!SE_PREDICT_TRUE((STATUS) == CUDNN_STATUS_SUCCESS)) {              \\\n    string error_msg = port::StrCat(ToString(STATUS), \" \", __VA_ARGS__); \\\n    SetFailure(port::Status(port::error::UNKNOWN, error_msg));           \\\n    LOG(ERROR) << error_msg;                                             \\\n    return;                                                              \\\n  }\n\ntemplate <typename Base>\nclass CudnnDescriptorCommon : public MixinBase<Base> {\n public:\n  bool ok() const { return status_.ok(); }\n  port::Status Status() const { return status_; }\n\n protected:\n  void SetFailure(const port::Status& status) { status_.Update(status); }\n  port::Status status_;\n};\n\nclass CudnnDropoutDescriptor : public CudnnDescriptorCommon<void> {\n public:\n  CudnnDropoutDescriptor(CUDAExecutor* parent, cudnnHandle_t cudnn_handle,\n                         float dropout, uint64 seed,\n                         ScratchAllocator* state_allocator)\n      : parent_(parent), handle_(nullptr) {\n    cudnnStatus_t status;\n    status = wrap::cudnnCreateDropoutDescriptor(parent_, &handle_);\n    CUDNN_RETURN_IF_FAIL(status, \"Failed to create dropout descriptor\");\n\n    if (dropout == 0.f) {\n      return;\n    }\n\n    DeviceMemory<uint8> state_memory;\n    if (state_allocator) {\n      size_t state_sizes_in_bytes = 0;\n      status = wrap::cudnnDropoutGetStatesSize(parent_, cudnn_handle,\n                                               &state_sizes_in_bytes);\n      CUDNN_RETURN_IF_FAIL(status, \"Failed to query dropout state sizes\");\n\n      auto allocated =\n          state_allocator->AllocateBytes(nullptr, state_sizes_in_bytes);\n      if (!allocated.ok() ||\n          (state_memory = allocated.ValueOrDie()) == nullptr) {\n        string error_msg =\n            port::StrCat(\"Failed to allocate Cudnn dropout state memory of \",\n                         state_sizes_in_bytes, \" bytes.\");\n        status_ = port::Status(port::error::UNKNOWN, error_msg);\n        LOG(ERROR) << error_msg;\n        return;\n      }\n    }\n    status = wrap::cudnnSetDropoutDescriptor(parent_, handle_, cudnn_handle,\n                                             dropout, state_memory.opaque(),\n                                             state_memory.size(), seed);\n    CUDNN_RETURN_IF_FAIL(\n        status, port::StrCat(\n                    \"Failed to set dropout descriptor with state memory size: \",\n                    state_memory.size(), \" bytes.\"));\n  }\n\n  ~CudnnDropoutDescriptor() {\n    if (handle_) {\n      cudnnStatus_t status =\n          wrap::cudnnDestroyDropoutDescriptor(parent_, handle_);\n      CUDNN_RETURN_IF_FAIL(status, \"Failed to destroy Cudnn dropout handle: \");\n    }\n  }\n\n  cudnnDropoutDescriptor_t handle() const {\n    if (!ok()) return nullptr;\n    return handle_;\n  }\n\n private:\n  CUDAExecutor* parent_;\n  cudnnDropoutDescriptor_t handle_;\n  float dropout_;\n  uint64 seed_;\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnDropoutDescriptor);\n};\n\nclass CudnnRnnParamsDescriptor : public CudnnDescriptorCommon<void> {\n public:\n  typedef dnn::RnnDescriptor::ParamsRegion ParamsRegion;\n  typedef dnn::RnnDescriptor::ParamsRegions ParamsRegions;\n  CudnnRnnParamsDescriptor(CUDAExecutor* parent, cudnnHandle_t cudnn_handle,\n                           const CudnnRnnDescriptor& rnn_desc);\n  ~CudnnRnnParamsDescriptor() {\n    cudnnStatus_t status = wrap::cudnnDestroyFilterDescriptor(parent_, handle_);\n    CUDNN_RETURN_IF_FAIL(status, \"Failed to destroy RNN filter descriptor\");\n  }\n  cudnnFilterDescriptor_t handle() const {\n    if (!ok()) return nullptr;\n    return handle_;\n  }\n  int64 params_size_in_bytes() const { return params_size_in_bytes_; }\n  ParamsRegions params_weights() const {\n    if (!ok()) return ParamsRegions();\n    return weights_;\n  }\n  ParamsRegions params_biases() const {\n    if (!ok()) return ParamsRegions();\n    return biases_;\n  }\n\n private:\n  int GetRegionCountPerLayer() const;\n  CUDAExecutor* parent_;\n  cudnnFilterDescriptor_t handle_;\n  const CudnnRnnDescriptor* rnn_desc_;\n  int64 params_size_in_bytes_;\n  ParamsRegions weights_;\n  ParamsRegions biases_;\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnParamsDescriptor);\n};\n\nclass CudnnRnnDescriptor : public CudnnDescriptorCommon<dnn::RnnDescriptor> {\n public:\n  CudnnRnnDescriptor(CUDAExecutor* parent, cudnnHandle_t cudnn_handle,\n                     int num_layers, int hidden_size, int input_size,\n                     cudnnRNNInputMode_t input_mode,\n                     cudnnDirectionMode_t direction_mode,\n                     cudnnRNNMode_t rnn_mode, cudnnDataType_t data_type,\n                     float dropout, uint64 seed,\n                     ScratchAllocator* state_allocator)\n      : parent_(parent),\n        rnn_desc_(nullptr),\n        num_layers_(num_layers),\n        hidden_size_(hidden_size),\n        input_size_(input_size),\n        input_mode_(input_mode),\n        direction_mode_(direction_mode),\n        rnn_mode_(rnn_mode),\n        data_type_(data_type) {\n    // Create the dropout handle.\n    cudnn_dropout_desc_.reset(new CudnnDropoutDescriptor(\n        parent, cudnn_handle, dropout, seed, state_allocator));\n    if (!cudnn_dropout_desc_->ok()) {\n      SetFailure(cudnn_dropout_desc_->Status());\n      return;\n    }\n\n    // Create the RNN handle\n    cudnnStatus_t status = wrap::cudnnCreateRNNDescriptor(parent_, &rnn_desc_);\n    CUDNN_RETURN_IF_FAIL(status, \"Unable to create RNN descriptor\");\n#if CUDNN_VERSION >= 6000\n    // TODO: allow the user to choose an algorithm.\n    cudnnRNNAlgo_t rnn_algo = CUDNN_RNN_ALGO_STANDARD;\n    status = wrap::cudnnSetRNNDescriptor_v6(\n        parent, cudnn_handle, rnn_desc_ /*rnnDesc*/, hidden_size /*hiddenSize*/,\n        num_layers /*numLayers*/, dropout_handle() /*dropoutDesc*/,\n        input_mode /*inputMode*/, direction_mode /*direction*/,\n        rnn_mode /*mode*/, rnn_algo /*algo*/, data_type /*dataType*/);\n#else\n    status = wrap::cudnnSetRNNDescriptor(\n        parent, rnn_desc_ /*rnnDesc*/, hidden_size /*hiddenSize*/,\n        num_layers /*numLayers*/, dropout_handle() /*dropoutDesc*/,\n        input_mode /*inputMode*/, direction_mode /*direction*/,\n        rnn_mode /*mode*/, data_type /*dataType*/);\n#endif\n    CUDNN_RETURN_IF_FAIL(status, \"Unable to update RNN descriptor\");\n\n    // Create the params handle.\n    cudnn_params_desc_.reset(\n        new CudnnRnnParamsDescriptor(parent, cudnn_handle, *this));\n    if (!cudnn_params_desc_->ok()) {\n      SetFailure(cudnn_params_desc_->Status());\n      return;\n    }\n  }\n  ~CudnnRnnDescriptor() override {\n    if (rnn_desc_) {\n      cudnnStatus_t status =\n          wrap::cudnnDestroyRNNDescriptor(parent_, rnn_desc_);\n      CUDNN_RETURN_IF_FAIL(status, \"Unable to destroy RNN descriptor\");\n    }\n  }\n  cudnnRNNDescriptor_t handle() const {\n    if (!ok()) return nullptr;\n    return rnn_desc_;\n  }\n  int num_layers() const { return num_layers_; }\n  int hidden_size() const { return hidden_size_; }\n  int input_size() const { return input_size_; }\n  cudnnRNNInputMode_t input_mode() const { return input_mode_; }\n  cudnnDirectionMode_t direction_mode() const { return direction_mode_; }\n  cudnnRNNMode_t rnn_mode() const { return rnn_mode_; }\n  cudnnDataType_t data_type() const { return data_type_; }\n  int64 ParamsSizeInBytes() const override {\n    return cudnn_params_desc_->params_size_in_bytes();\n  }\n  cudnnDropoutDescriptor_t dropout_handle() const {\n    if (!cudnn_dropout_desc_) return nullptr;\n    return cudnn_dropout_desc_->handle();\n  }\n  cudnnFilterDescriptor_t params_handle() const {\n    if (!cudnn_params_desc_) return nullptr;\n    return cudnn_params_desc_->handle();\n  }\n  ParamsRegions ParamsWeightRegions() const override {\n    if (!ok()) return ParamsRegions();\n    return cudnn_params_desc_->params_weights();\n  }\n  ParamsRegions ParamsBiasRegions() const override {\n    if (!ok()) return ParamsRegions();\n    return cudnn_params_desc_->params_biases();\n  }\n\n private:\n  CUDAExecutor* parent_;\n  cudnnRNNDescriptor_t rnn_desc_;\n  int num_layers_;\n  int hidden_size_;\n  int input_size_;\n  cudnnRNNInputMode_t input_mode_;\n  cudnnDirectionMode_t direction_mode_;\n  cudnnRNNMode_t rnn_mode_;\n  cudnnDataType_t data_type_;\n  std::unique_ptr<CudnnDropoutDescriptor> cudnn_dropout_desc_;\n  std::unique_ptr<CudnnRnnParamsDescriptor> cudnn_params_desc_;\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnDescriptor);\n};\n\nCudnnRnnParamsDescriptor::CudnnRnnParamsDescriptor(\n    CUDAExecutor* parent, cudnnHandle_t cudnn_handle,\n    const CudnnRnnDescriptor& rnn_desc)\n    : parent_(parent),\n      handle_(nullptr),\n      rnn_desc_(&rnn_desc),\n      params_size_in_bytes_(0) {\n  cudnnTensorDescriptor_t input_desc = nullptr;\n  {\n    // Query the params size.\n    auto status = wrap::cudnnCreateTensorDescriptor(parent, &input_desc);\n    CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to create tensor descriptor\");\n    int dims[] = {1, rnn_desc.input_size(), 1};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    status = wrap::cudnnSetTensorNdDescriptor(\n        parent, input_desc /*tensorDesc*/, rnn_desc.data_type() /*dataType*/,\n        sizeof(dims) / sizeof(dims[0]) /*nbDims*/, dims /*dimA*/,\n        strides /*strideA*/);\n    CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to set tensor descriptor\");\n\n    size_t params_size = 0;\n    status = wrap::cudnnGetRNNParamsSize(\n        parent, cudnn_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n        input_desc /*xDesc*/, &params_size /*sizeInBytes*/,\n        rnn_desc.data_type() /*dataType*/);\n    CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to get RNN parameter size\");\n    params_size_in_bytes_ = static_cast<int64>(params_size);\n  }\n\n  {\n    // Create the params descriptor.\n    auto status = wrap::cudnnCreateFilterDescriptor(parent, &handle_);\n    CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to create RNN filter descriptor\");\n    int dims[] = {static_cast<int>(params_size_in_bytes_), 1, 1};\n    status = wrap::cudnnSetFilterNdDescriptor(\n        parent, handle_ /*filterDesc*/, rnn_desc.data_type() /*dataType*/,\n        CUDNN_TENSOR_NCHW /*format*/, sizeof(dims) / sizeof(dims[0]) /*nbDims*/,\n        dims /*filterDimA*/);\n    CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to update RNN filter descriptor\");\n  }\n\n  {\n    // Create the weights and biases into the params buffer\n    int region_count_per_layer = GetRegionCountPerLayer();\n    cudnnFilterDescriptor_t region_desc_handle = nullptr;\n    auto status =\n        wrap::cudnnCreateFilterDescriptor(parent, &region_desc_handle);\n    CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to create filter descriptor\");\n    const int layer_count = rnn_desc.direction_mode() == CUDNN_UNIDIRECTIONAL\n                                ? rnn_desc.num_layers()\n                                : 2 * rnn_desc.num_layers();\n    for (int layer = 0; layer < layer_count; layer++) {\n      for (int region = 0; region < region_count_per_layer; region++) {\n        for (int type = 0; type < 2; type++) {\n          void* offset = nullptr;\n          if (type == 0) {\n            status = wrap::cudnnGetRNNLinLayerMatrixParams(\n                parent, cudnn_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n                layer /*layer*/, input_desc /*xDesc*/, handle_ /*wDesc*/,\n                nullptr /*w*/, region /*linLayerID*/,\n                region_desc_handle /*linLayerMatDesc*/,\n                &offset /*linLayerMat*/);\n            CUDNN_RETURN_IF_FAIL(\n                status, \"Cudnn fails to call cudnnGetRNNLinLayerMatrixParams\");\n          } else {\n            status = wrap::cudnnGetRNNLinLayerBiasParams(\n                parent, cudnn_handle /*rnnDesc*/, rnn_desc.handle() /*rnnDesc*/,\n                layer /*layer*/, input_desc /*xDesc*/, handle_ /*wDesc*/,\n                nullptr /*w*/, region /*linLayerID*/,\n                region_desc_handle /*linLayerBiasDesc*/,\n                &offset /*linLayerBias*/);\n            CUDNN_RETURN_IF_FAIL(\n                status, \"Cudnn fails to call cudnnGetRNNLinLayerBiasParams\");\n          }\n          int dims[] = {1, 1, 1};\n          cudnnDataType_t data_type;\n          cudnnTensorFormat_t tensor_format;\n          int n_dims;\n          status = wrap::cudnnGetFilterNdDescriptor(\n              parent, region_desc_handle /*filterDesc*/,\n              sizeof(dims) / sizeof(dims[0]) /*nbDimsRequested*/,\n              &data_type /*dataType*/, &tensor_format /*format*/,\n              &n_dims /*nbDims*/, dims /*filterDimA*/);\n          CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to get filter description\");\n          int64 size = dims[0] * dims[1] * dims[2] *\n                       CudnnDataTypeToByteSize(rnn_desc.data_type());\n          auto region = ParamsRegion{reinterpret_cast<int64>(offset), size};\n          if (type == 0) {\n            weights_.push_back(region);\n          } else {\n            biases_.push_back(region);\n          }\n        }\n      }\n    }\n    status = wrap::cudnnDestroyFilterDescriptor(parent, region_desc_handle);\n    CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to destroy filter descriptor\");\n  }\n\n  {\n    // Release the dummy input tensor descriptor.\n    auto status = wrap::cudnnDestroyTensorDescriptor(parent, input_desc);\n    CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to destroy tensor descriptor\");\n  }\n}\n\nint CudnnRnnParamsDescriptor::GetRegionCountPerLayer() const {\n  auto rnn_mode = rnn_desc_->rnn_mode();\n  switch (rnn_mode) {\n    case CUDNN_RNN_RELU:\n    case CUDNN_RNN_TANH:\n      return 2;\n    case CUDNN_LSTM:\n      return 8;\n    case CUDNN_GRU:\n      return 6;\n    default:\n      LOG(FATAL) << \"Invalid RNN Mode: \" << static_cast<int>(rnn_mode);\n  }\n}\n\nclass CudnnRnnSequenceTensorDescriptor\n    : public CudnnDescriptorCommon<dnn::RnnSequenceTensorDescriptor> {\n public:\n  CudnnRnnSequenceTensorDescriptor(CUDAExecutor* parent, int seq_length,\n                                   int batch_size, int data_size,\n                                   cudnnDataType_t data_type)\n      : parent_(parent),\n        seq_length_(seq_length),\n        batch_size_(batch_size),\n        data_size_(data_size),\n        data_type_(data_type) {\n    cudnnTensorDescriptor_t handle = nullptr;\n    if (seq_length <= 0) {\n      string error_msg =\n          port::StrCat(\"sequence length must be positive: \", seq_length);\n      LOG(ERROR) << error_msg;\n      SetFailure(port::Status(port::error::UNKNOWN, error_msg));\n      return;\n    }\n    cudnnStatus_t status = wrap::cudnnCreateTensorDescriptor(parent, &handle);\n    CUDNN_RETURN_IF_FAIL(status, \"Failed to create tensor descriptor\");\n    int dims[] = {batch_size, data_size, 1};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    status = wrap::cudnnSetTensorNdDescriptor(\n        parent, handle /*tensorDesc*/, data_type /*dataType*/,\n        sizeof(dims) / sizeof(dims[0]) /*nbDims*/, dims /*dimA*/,\n        strides /*strideA*/);\n    CUDNN_RETURN_IF_FAIL(status, \"Failed to update tensor descriptor\");\n    // Replicate handle across the number of steps.\n    handles_.assign(seq_length, handle);\n  }\n\n  ~CudnnRnnSequenceTensorDescriptor() override {\n    // Only the first one needs to be destroyed. All others are the same.\n    cudnnStatus_t status =\n        wrap::cudnnDestroyTensorDescriptor(parent_, handles_[0]);\n    CUDNN_RETURN_IF_FAIL(status,\n                         \"Failed to destroy sequence tensor descriptor\");\n  }\n\n  const cudnnTensorDescriptor_t* handles() const {\n    if (!ok()) return nullptr;\n    CHECK(!handles_.empty()) << \"handles cannot be empty\";\n    return handles_.data();\n  }\n\n  int seq_length() const { return seq_length_; }\n  int batch_size() const { return batch_size_; }\n  int data_size() const { return data_size_; }\n\n private:\n  CUDAExecutor* parent_;\n  int seq_length_;\n  int batch_size_;\n  int data_size_;\n  cudnnDataType_t data_type_;\n  std::vector<cudnnTensorDescriptor_t> handles_;\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnSequenceTensorDescriptor);\n};\n\nclass CudnnRnnStateTensorDescriptor\n    : public CudnnDescriptorCommon<dnn::RnnStateTensorDescriptor> {\n public:\n  CudnnRnnStateTensorDescriptor(CUDAExecutor* parent, int num_layers,\n                                int batch_size, int data_size,\n                                cudnnDataType_t data_type)\n      : parent_(parent),\n        handle_(nullptr),\n        num_layers_(num_layers),\n        batch_size_(batch_size),\n        data_size_(data_size),\n        data_type_(data_type) {\n    cudnnStatus_t status = wrap::cudnnCreateTensorDescriptor(parent, &handle_);\n    CUDNN_RETURN_IF_FAIL(status, \"Failed to create tensor descriptor\");\n    int dims[] = {num_layers, batch_size, data_size};\n    int strides[] = {dims[1] * dims[2], dims[2], 1};\n    status = wrap::cudnnSetTensorNdDescriptor(\n        parent, handle_ /*tensorDesc*/, data_type /*dataType*/,\n        sizeof(dims) / sizeof(dims[0]) /*nbDims*/, dims /*dimA*/,\n        strides /*strideA*/);\n    CUDNN_RETURN_IF_FAIL(status, \"Failed to update tensor descriptor\");\n  }\n\n  ~CudnnRnnStateTensorDescriptor() override {\n    if (!handle_) {\n      cudnnStatus_t status =\n          wrap::cudnnDestroyTensorDescriptor(parent_, handle_);\n      CUDNN_RETURN_IF_FAIL(status, \"Unable to destroy RNN state tensor\");\n    }\n  }\n\n  cudnnTensorDescriptor_t handle() const {\n    if (!ok()) return nullptr;\n    return handle_;\n  }\n  int num_layers() const { return num_layers_; }\n  int batch_size() const { return batch_size_; }\n  int data_size() const { return data_size_; }\n\n private:\n  CUDAExecutor* parent_;\n  cudnnTensorDescriptor_t handle_;\n  int num_layers_;\n  int batch_size_;\n  int data_size_;\n  cudnnDataType_t data_type_;\n  SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnStateTensorDescriptor);\n};\n\nnamespace {\n\nstruct RnnModelDims {\n  int num_layers = 0;\n  int batch_size = 0;\n  int seq_length = 0;\n  int hidden_size = 0;\n  int input_size = 0;\n  int dir_count = 0;\n};\n\ntemplate <class T>\nbool ExtractAndCheckRnnForward(\n    const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<T>& input_data,\n    const CudnnRnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<T>& input_h_data,\n    const CudnnRnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n    const CudnnRnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<T>& output_data,\n    const CudnnRnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<T>& output_h_data,\n    const CudnnRnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<T>& output_c_data, RnnModelDims* model_dims) {\n  // extract model parameters\n  model_dims->num_layers = rnn_desc.num_layers();\n  model_dims->batch_size = input_desc.batch_size();\n  model_dims->seq_length = input_desc.seq_length();\n  model_dims->hidden_size = rnn_desc.hidden_size();\n  model_dims->input_size = input_desc.data_size();\n  model_dims->dir_count =\n      (rnn_desc.direction_mode() == CUDNN_BIDIRECTIONAL) ? 2 : 1;\n\n  // check parameters\n  if (!(input_h_desc.num_layers() ==\n            model_dims->num_layers * model_dims->dir_count &&\n        input_h_desc.batch_size() == model_dims->batch_size &&\n        input_h_desc.data_size() == model_dims->hidden_size)) {\n    LOG(ERROR) << \"Invalid input_h shape\";\n    return false;\n  }\n  if (!(input_h_desc.num_layers() == input_c_desc.num_layers() &&\n        input_h_desc.batch_size() == input_c_desc.batch_size() &&\n        input_h_desc.data_size() == input_c_desc.data_size())) {\n    LOG(ERROR) << \"Invalid input_c shape\";\n    return false;\n  }\n  if (!(output_desc.seq_length() == model_dims->seq_length &&\n        output_desc.batch_size() == model_dims->batch_size &&\n        output_desc.data_size() ==\n            model_dims->hidden_size * model_dims->dir_count)) {\n    LOG(ERROR) << \"Invalid output shape\";\n    return false;\n  }\n  if (!(input_h_desc.num_layers() == output_h_desc.num_layers() &&\n        input_h_desc.batch_size() == output_h_desc.batch_size() &&\n        input_h_desc.data_size() == output_h_desc.data_size())) {\n    LOG(ERROR) << \"Invalid output_h shape\";\n    return false;\n  }\n  if (!(input_h_desc.num_layers() == output_c_desc.num_layers() &&\n        input_h_desc.batch_size() == output_c_desc.batch_size() &&\n        input_h_desc.data_size() == output_c_desc.data_size())) {\n    LOG(ERROR) << \"Invalid output_h shape\";\n    return false;\n  }\n\n  return true;\n}\n\nbool CheckRNNParameterSize(CUDAExecutor* parent, cudnnHandle_t cudnn_handle,\n                           const CudnnRnnDescriptor& rnn_desc,\n                           const CudnnRnnSequenceTensorDescriptor& input_desc) {\n  size_t params_size_in_bytes = 0;\n  cudnnStatus_t status = wrap::cudnnGetRNNParamsSize(\n      parent, cudnn_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n      input_desc.handles()[0] /*xDesc*/, &params_size_in_bytes /*sizeInBytes*/,\n      rnn_desc.data_type() /*dataType*/);\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"Unable to check RNN param size: \" << ToString(status);\n    return false;\n  }\n  return static_cast<int64>(params_size_in_bytes) ==\n         rnn_desc.ParamsSizeInBytes();\n}\n\nbool CreateRnnWorkspace(Stream* stream, CUDAExecutor* parent,\n                        cudnnHandle_t cudnn_handle,\n                        const CudnnRnnDescriptor& rnn_desc,\n                        const CudnnRnnSequenceTensorDescriptor& input_desc,\n                        ScratchAllocator* workspace_allocator,\n                        DeviceMemory<uint8>* workspace) {\n  // Query the workspace size.\n  size_t workspace_size_in_bytes = 0;\n  cudnnStatus_t status = wrap::cudnnGetRNNWorkspaceSize(\n      parent, cudnn_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n      input_desc.seq_length() /*seqLength*/, input_desc.handles() /*xDesc*/,\n      &workspace_size_in_bytes /*sizeInBytes*/);\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"Unable to query workspace size: \" << ToString(status);\n    return false;\n  }\n  // Allocate the workspace.\n  if (workspace_size_in_bytes > 0) {\n    auto allocated =\n        workspace_allocator->AllocateBytes(stream, workspace_size_in_bytes);\n    if (!allocated.ok() || (*workspace = allocated.ValueOrDie()) == nullptr) {\n      LOG(ERROR) << port::StrCat(\"Failed to allocate RNN workspace of \",\n                                 workspace_size_in_bytes, \" bytes.\");\n      return false;\n    }\n  } else {\n    *workspace = DeviceMemory<uint8>();\n  }\n  return true;\n}\n\n}  // namespace\n\ntemplate <class T>\nbool CudnnSupport::DoRnnForwardImpl(\n    Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<T>& input_data,\n    const CudnnRnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<T>& input_h_data,\n    const CudnnRnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n    const CudnnRnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<T>* output_data,\n    const CudnnRnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<T>* output_h_data,\n    const CudnnRnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<T>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n  // extract model parameters\n  RnnModelDims model_dims;\n  bool res = ExtractAndCheckRnnForward(\n      rnn_desc, input_desc, input_data, input_h_desc, input_h_data,\n      input_c_desc, input_c_data, params, output_desc, *output_data,\n      output_h_desc, *output_h_data, output_c_desc, *output_c_data,\n      &model_dims);\n  if (!res) {\n    LOG(ERROR) << \"Invalid parameters for RNN Model\";\n    return false;\n  }\n\n  // check params size\n  mutex_lock lock{dnn_handle_mutex_};\n\n  if (!CheckRNNParameterSize(parent_, ToHandle(dnn_handle_), rnn_desc,\n                             input_desc)) {\n    LOG(ERROR) << \"Invalid parameters\";\n    return false;\n  }\n\n  // create the workspace\n  DeviceMemory<uint8> workspace;\n  if (!CreateRnnWorkspace(stream, parent_, ToHandle(dnn_handle_), rnn_desc,\n                          input_desc, workspace_allocator, &workspace)) {\n    LOG(ERROR) << \"Unable to create rnn workspace\";\n    return false;\n  }\n\n  // query the reserve space size\n  // allocate the reserve space\n  DeviceMemory<uint8> reserve_space;\n  if (is_training) {\n    size_t reserve_space_size_in_bytes = 0;\n    cudnnStatus_t status = wrap::cudnnGetRNNTrainingReserveSize(\n        parent_, ToHandle(dnn_handle_) /*handle*/,\n        rnn_desc.handle() /*rnnDesc*/, model_dims.seq_length /*seqLength*/,\n        input_desc.handles() /*xDesc*/,\n        &reserve_space_size_in_bytes /*sizeInBytes*/);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(ERROR) << \"Unable to query reserve space size: \" << ToString(status);\n      return false;\n    }\n\n    if (reserve_space_size_in_bytes > 0) {\n      auto allocated = reserve_space_allocator->AllocateBytes(\n          stream, reserve_space_size_in_bytes);\n      if (!allocated.ok() ||\n          (reserve_space = allocated.ValueOrDie()) == nullptr) {\n        LOG(ERROR) << \"Failed to allocate RNN reserve space of \"\n                   << reserve_space_size_in_bytes << \" bytes.\";\n        return false;\n      }\n    }\n  }\n\n  // make the forward call\n  if (!is_training) {\n    cudnnStatus_t status = wrap::cudnnRNNForwardInference(\n        parent_, ToHandle(dnn_handle_) /*handle*/,\n        rnn_desc.handle() /*rnnDesc*/, model_dims.seq_length /*seqLength*/,\n        input_desc.handles() /*xDesc*/, input_data.opaque() /*x*/,\n        input_h_desc.handle() /*hxDesc*/, input_h_data.opaque() /*hx*/,\n        input_c_desc.handle() /*cxDesc*/, input_c_data.opaque() /*cx*/,\n        rnn_desc.params_handle() /*wDesc*/, params.opaque() /*w*/,\n        output_desc.handles() /*yDesc*/, output_data->opaque() /*y*/,\n        output_h_desc.handle() /*hyDesc*/, output_h_data->opaque() /*hy*/,\n        output_c_desc.handle() /*cyDesc*/, output_c_data->opaque() /*cy*/,\n        workspace.opaque() /*workspace*/,\n        workspace.size() /*workSpaceSizeInBytes*/);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(ERROR) << \"Failed to call cudnnRNNForwardInference: \"\n                 << ToString(status);\n      return false;\n    }\n  } else {\n    cudnnStatus_t status = wrap::cudnnRNNForwardTraining(\n        parent_, ToHandle(dnn_handle_) /*handle*/,\n        rnn_desc.handle() /*rnnDesc*/, model_dims.seq_length /*seqLength*/,\n        input_desc.handles() /*xDesc*/, input_data.opaque() /*x*/,\n        input_h_desc.handle() /*hxDesc*/, input_h_data.opaque() /*hx*/,\n        input_c_desc.handle() /*cxDesc*/, input_c_data.opaque() /*cx*/,\n        rnn_desc.params_handle() /*wDesc*/, params.opaque() /*w*/,\n        output_desc.handles() /*yDesc*/, output_data->opaque() /*y*/,\n        output_h_desc.handle() /*hyDesc*/, output_h_data->opaque() /*hy*/,\n        output_c_desc.handle() /*cyDesc*/, output_c_data->opaque() /*cy*/,\n        workspace.opaque() /*workspace*/,\n        workspace.size() /*workSpaceSizeInBytes*/,\n        reserve_space.opaque() /*reserveSpace*/,\n        reserve_space.size() /*reserveSpaceSizeInBytes*/);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(ERROR) << \"Failed to call cudnnRNNForwardTraining\"\n                 << ToString(status);\n      return false;\n    }\n  }\n\n  return true;\n}\n\ntemplate <class T>\nbool CudnnSupport::DoRnnBackwardImpl(\n    Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n    const CudnnRnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<T>& input_data,\n    const CudnnRnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<T>& input_h_data,\n    const CudnnRnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n    const CudnnRnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<T>& output_data,\n    const CudnnRnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<T>& output_h_data,\n    const CudnnRnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<T>& output_c_data,\n    const DeviceMemory<T>& output_backprop_data,\n    const DeviceMemory<T>& output_h_backprop_data,\n    const DeviceMemory<T>& output_c_backprop_data,\n    DeviceMemory<T>* input_backprop_data,\n    DeviceMemory<T>* input_h_backprop_data,\n    DeviceMemory<T>* input_c_backprop_data,\n    DeviceMemory<T>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n  // extract model parameters\n  RnnModelDims model_dims;\n  bool res = ExtractAndCheckRnnForward(\n      rnn_desc, input_desc, input_data, input_h_desc, input_h_data,\n      input_c_desc, input_c_data, params, output_desc, output_data,\n      output_h_desc, output_h_data, output_c_desc, output_c_data, &model_dims);\n  if (!res) {\n    LOG(ERROR) << \"Invalid parameters for RNN Model\";\n    return false;\n  }\n\n  // check params size\n  mutex_lock lock{dnn_handle_mutex_};\n\n  if (!CheckRNNParameterSize(parent_, ToHandle(dnn_handle_), rnn_desc,\n                             input_desc)) {\n    LOG(ERROR) << \"Invalid parameters\";\n    return false;\n  }\n\n  // create the workspace\n  DeviceMemory<uint8> workspace;\n  if (!CreateRnnWorkspace(stream, parent_, ToHandle(dnn_handle_), rnn_desc,\n                          input_desc, workspace_allocator, &workspace)) {\n    LOG(ERROR) << \"Unable to create rnn workspace\";\n    return false;\n  }\n\n  // make the backward data call\n  cudnnStatus_t status = wrap::cudnnRNNBackwardData(\n      parent_, ToHandle(dnn_handle_) /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n      model_dims.seq_length /*seqLength*/, output_desc.handles() /*yDesc*/,\n      output_data.opaque() /*y*/, output_desc.handles() /*dyDesc*/,\n      output_backprop_data.opaque() /*dy*/, output_h_desc.handle() /*dhyDesc*/,\n      output_h_backprop_data.opaque() /*dhy*/,\n      output_c_desc.handle() /*dcyDesc*/,\n      output_c_backprop_data.opaque() /*dcy*/,\n      rnn_desc.params_handle() /*wDesc*/, params.opaque() /*w*/,\n      input_h_desc.handle() /*hxDesc*/, input_h_data.opaque() /*hx*/,\n      input_c_desc.handle() /*cxDesc*/, input_c_data.opaque() /*cx*/,\n      input_desc.handles() /*dxDesc*/, input_backprop_data->opaque() /*dx*/,\n      input_h_desc.handle() /*dhxDesc*/,\n      input_h_backprop_data->opaque() /*dhx*/,\n      input_c_desc.handle() /*dcxDesc*/,\n      input_c_backprop_data->opaque() /*dcx*/, workspace.opaque() /*workspace*/,\n      workspace.size() /*workSpaceSizeInBytes*/,\n      reserve_space_data->opaque() /*reserveSpace*/,\n      reserve_space_data->size() /*reserveSpaceSizeInBytes*/);\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"Failed to call cudnnRNNBackwardData: \" << ToString(status);\n    return false;\n  }\n\n  if (params_backprop_data != nullptr) {\n    // Clear the dw to zeros.\n    stream->ThenMemZero(params_backprop_data, params_backprop_data->size());\n    // make the backward weight call\n    status = wrap::cudnnRNNBackwardWeights(\n        parent_, ToHandle(dnn_handle_) /*handle*/,\n        rnn_desc.handle() /*rnnDesc*/, model_dims.seq_length /*seqLength*/,\n        input_desc.handles() /*xDesc*/, input_data.opaque() /*x*/,\n        input_h_desc.handle() /*hxDesc*/, input_h_data.opaque() /*hx*/,\n        output_desc.handles() /*yDesc*/, output_data.opaque() /*y*/,\n        workspace.opaque() /*workspace*/,\n        workspace.size() /*workSpaceSizeInBytes*/,\n        rnn_desc.params_handle() /*dwDesc*/,\n        params_backprop_data->opaque() /*dw*/,\n        reserve_space_data->opaque() /*reserveSpace*/,\n        reserve_space_data->size() /*reserveSpaceSizeInBytes*/);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      LOG(ERROR) << \"Failed to call cudnnRNNBackwardWeights: \"\n                 << ToString(status);\n      return false;\n    }\n  }\n\n  return true;\n}\n\n#endif  // CUDNN_VERSION\n\nport::StatusOr<std::unique_ptr<dnn::RnnDescriptor>>\nCudnnSupport::createRnnDescriptor(int num_layers, int hidden_size,\n                                  int input_size, dnn::RnnInputMode input_mode,\n                                  dnn::RnnDirectionMode direction_mode,\n                                  dnn::RnnMode rnn_mode,\n                                  dnn::DataType data_type, float dropout,\n                                  uint64 seed,\n                                  ScratchAllocator* state_allocator) {\n#if CUDNN_VERSION >= 5000\n  mutex_lock lock{dnn_handle_mutex_};\n  std::unique_ptr<CudnnRnnDescriptor> rnn_desc(new CudnnRnnDescriptor(\n      parent_, ToHandle(dnn_handle_), num_layers, hidden_size, input_size,\n      ToCudnnRnnInputMode(input_mode), ToCudnnRnnDirectionMode(direction_mode),\n      ToCudnnRnnMode(rnn_mode), ToCudnnDataType(data_type), dropout, seed,\n      state_allocator));\n  if (!rnn_desc->ok()) {\n    return rnn_desc->Status();\n  }\n  return port::StatusOr<std::unique_ptr<dnn::RnnDescriptor>>(\n      std::move(rnn_desc));\n#else\n  string error_msg =\n      port::StrCat(\"createRnnDescriptor needs at least Cudnn 5.0 to work. \",\n                   \"Current Cudnn version: \", CUDNN_VERSION, \". \");\n  LOG(ERROR) << error_msg;\n  return port::Status{port::error::UNIMPLEMENTED, error_msg};\n#endif  // CUDNN_VERSION\n}\n\nport::StatusOr<std::unique_ptr<dnn::RnnSequenceTensorDescriptor>>\nCudnnSupport::createRnnSequenceTensorDescriptor(int seq_length, int batch_size,\n                                                int data_size,\n                                                dnn::DataType data_type) {\n#if CUDNN_VERSION >= 5000\n  std::unique_ptr<CudnnRnnSequenceTensorDescriptor> seq_desc(\n      new CudnnRnnSequenceTensorDescriptor(parent_, seq_length, batch_size,\n                                           data_size,\n                                           ToCudnnDataType(data_type)));\n  if (!seq_desc->ok()) {\n    return seq_desc->Status();\n  }\n  return port::StatusOr<std::unique_ptr<dnn::RnnSequenceTensorDescriptor>>(\n      std::move(seq_desc));\n#else\n  string error_msg = port::StrCat(\n      \"createRnnSequenceTensorDescriptor needs at least Cudnn 5.0 to work. \",\n      \"Current Cudnn version: \", CUDNN_VERSION, \". \");\n  LOG(ERROR) << error_msg;\n  return port::Status{port::error::UNIMPLEMENTED, error_msg};\n#endif  // CUDNN_VERSION\n}\n\nport::StatusOr<std::unique_ptr<dnn::RnnStateTensorDescriptor>>\nCudnnSupport::createRnnStateTensorDescriptor(int num_layer, int batch_size,\n                                             int data_size,\n                                             dnn::DataType data_type) {\n#if CUDNN_VERSION >= 5000\n  std::unique_ptr<CudnnRnnStateTensorDescriptor> state_desc(\n      new CudnnRnnStateTensorDescriptor(parent_, num_layer, batch_size,\n                                        data_size, ToCudnnDataType(data_type)));\n  if (!state_desc->ok()) {\n    return state_desc->Status();\n  }\n  return port::StatusOr<std::unique_ptr<dnn::RnnStateTensorDescriptor>>(\n      std::move(state_desc));\n#else\n  string error_msg = port::StrCat(\n      \"createRnnStateTensorDescriptor needs at least Cudnn 5.0 to work. \",\n      \"Current Cudnn version: \", CUDNN_VERSION, \". \");\n  LOG(ERROR) << error_msg;\n  return port::Status{port::error::UNIMPLEMENTED, error_msg};\n#endif  // CUDNN_VERSION\n}\n\nbool CudnnSupport::DoRnnForward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<Eigen::half>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<Eigen::half>& input_c_data,\n    const DeviceMemory<Eigen::half>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<Eigen::half>* output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<Eigen::half>* output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<Eigen::half>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n#if CUDNN_VERSION >= 5000\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n\n  return DoRnnForwardImpl<Eigen::half>(\n      stream, cudnn_rnn_desc, cudnn_input_desc, input_data, cudnn_input_h_desc,\n      input_h_data, cudnn_input_c_desc, input_c_data, params, cudnn_output_desc,\n      output_data, cudnn_output_h_desc, output_h_data, cudnn_output_c_desc,\n      output_c_data, is_training, reserve_space_allocator, workspace_allocator);\n#else\n  return false;\n#endif  // CUDNN_VERSION\n}\n\nbool CudnnSupport::DoRnnForward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<float>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<float>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<float>& input_c_data, const DeviceMemory<float>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<float>* output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<float>* output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<float>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n#if CUDNN_VERSION >= 5000\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n\n  return DoRnnForwardImpl<float>(\n      stream, cudnn_rnn_desc, cudnn_input_desc, input_data, cudnn_input_h_desc,\n      input_h_data, cudnn_input_c_desc, input_c_data, params, cudnn_output_desc,\n      output_data, cudnn_output_h_desc, output_h_data, cudnn_output_c_desc,\n      output_c_data, is_training, reserve_space_allocator, workspace_allocator);\n#else\n  return false;\n#endif  // CUDNN_VERSION\n}\n\nbool CudnnSupport::DoRnnForward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<double>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<double>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<double>& input_c_data,\n    const DeviceMemory<double>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    DeviceMemory<double>* output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    DeviceMemory<double>* output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    DeviceMemory<double>* output_c_data, bool is_training,\n    ScratchAllocator* reserve_space_allocator,\n    ScratchAllocator* workspace_allocator) {\n#if CUDNN_VERSION >= 5000\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n\n  return DoRnnForwardImpl<double>(\n      stream, cudnn_rnn_desc, cudnn_input_desc, input_data, cudnn_input_h_desc,\n      input_h_data, cudnn_input_c_desc, input_c_data, params, cudnn_output_desc,\n      output_data, cudnn_output_h_desc, output_h_data, cudnn_output_c_desc,\n      output_c_data, is_training, reserve_space_allocator, workspace_allocator);\n#else\n  return false;\n#endif  // CUDNN_VERSION\n}\n\nbool CudnnSupport::DoRnnBackward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<Eigen::half>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<Eigen::half>& input_c_data,\n    const DeviceMemory<Eigen::half>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<Eigen::half>& output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<Eigen::half>& output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<Eigen::half>& output_c_data,\n    const DeviceMemory<Eigen::half>& output_backprop_data,\n    const DeviceMemory<Eigen::half>& output_h_backprop_data,\n    const DeviceMemory<Eigen::half>& output_c_backprop_data,\n    DeviceMemory<Eigen::half>* input_backprop_data,\n    DeviceMemory<Eigen::half>* input_h_backprop_data,\n    DeviceMemory<Eigen::half>* input_c_backprop_data,\n    DeviceMemory<Eigen::half>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n#if CUDNN_VERSION >= 5000\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n\n  return DoRnnBackwardImpl<Eigen::half>(\n      stream, cudnn_rnn_desc, cudnn_input_desc, input_data, cudnn_input_h_desc,\n      input_h_data, cudnn_input_c_desc, input_c_data, params, cudnn_output_desc,\n      output_data, cudnn_output_h_desc, output_h_data, cudnn_output_c_desc,\n      output_c_data, output_backprop_data, output_h_backprop_data,\n      output_c_backprop_data, input_backprop_data, input_h_backprop_data,\n      input_c_backprop_data, params_backprop_data, reserve_space_data,\n      workspace_allocator);\n#else\n  return false;\n#endif  // CUDNN_VERSION\n}\n\nbool CudnnSupport::DoRnnBackward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<float>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<float>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<float>& input_c_data, const DeviceMemory<float>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<float>& output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<float>& output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<float>& output_c_data,\n    const DeviceMemory<float>& output_backprop_data,\n    const DeviceMemory<float>& output_h_backprop_data,\n    const DeviceMemory<float>& output_c_backprop_data,\n    DeviceMemory<float>* input_backprop_data,\n    DeviceMemory<float>* input_h_backprop_data,\n    DeviceMemory<float>* input_c_backprop_data,\n    DeviceMemory<float>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n#if CUDNN_VERSION >= 5000\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n\n  return DoRnnBackwardImpl<float>(\n      stream, cudnn_rnn_desc, cudnn_input_desc, input_data, cudnn_input_h_desc,\n      input_h_data, cudnn_input_c_desc, input_c_data, params, cudnn_output_desc,\n      output_data, cudnn_output_h_desc, output_h_data, cudnn_output_c_desc,\n      output_c_data, output_backprop_data, output_h_backprop_data,\n      output_c_backprop_data, input_backprop_data, input_h_backprop_data,\n      input_c_backprop_data, params_backprop_data, reserve_space_data,\n      workspace_allocator);\n#else\n  return false;\n#endif  // CUDNN_VERSION\n}\n\nbool CudnnSupport::DoRnnBackward(\n    Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n    const dnn::RnnSequenceTensorDescriptor& input_desc,\n    const DeviceMemory<double>& input_data,\n    const dnn::RnnStateTensorDescriptor& input_h_desc,\n    const DeviceMemory<double>& input_h_data,\n    const dnn::RnnStateTensorDescriptor& input_c_desc,\n    const DeviceMemory<double>& input_c_data,\n    const DeviceMemory<double>& params,\n    const dnn::RnnSequenceTensorDescriptor& output_desc,\n    const DeviceMemory<double>& output_data,\n    const dnn::RnnStateTensorDescriptor& output_h_desc,\n    const DeviceMemory<double>& output_h_data,\n    const dnn::RnnStateTensorDescriptor& output_c_desc,\n    const DeviceMemory<double>& output_c_data,\n    const DeviceMemory<double>& output_backprop_data,\n    const DeviceMemory<double>& output_h_backprop_data,\n    const DeviceMemory<double>& output_c_backprop_data,\n    DeviceMemory<double>* input_backprop_data,\n    DeviceMemory<double>* input_h_backprop_data,\n    DeviceMemory<double>* input_c_backprop_data,\n    DeviceMemory<double>* params_backprop_data,\n    DeviceMemory<uint8>* reserve_space_data,\n    ScratchAllocator* workspace_allocator) {\n#if CUDNN_VERSION >= 5000\n  const CudnnRnnDescriptor& cudnn_rnn_desc =\n      static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n  const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n      static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n  const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n      static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n\n  return DoRnnBackwardImpl<double>(\n      stream, cudnn_rnn_desc, cudnn_input_desc, input_data, cudnn_input_h_desc,\n      input_h_data, cudnn_input_c_desc, input_c_data, params, cudnn_output_desc,\n      output_data, cudnn_output_h_desc, output_h_data, cudnn_output_c_desc,\n      output_c_data, output_backprop_data, output_h_backprop_data,\n      output_c_backprop_data, input_backprop_data, input_h_backprop_data,\n      input_c_backprop_data, params_backprop_data, reserve_space_data,\n      workspace_allocator);\n#else\n  return false;\n#endif  // CUDNN_VERSION\n}\n\nnamespace {\n\ninline cudnnConvolutionFwdAlgo_t GetCudnnConvolutionForwardAlgo(\n    Stream* stream, CUDAExecutor* parent, void* dnn_handle,\n    const ScopedTensorDescriptor& input_nd,\n    const ScopedFilterDescriptor& filter,\n    const ScopedConvolutionDescriptor& conv,\n    const ScopedTensorDescriptor& output_nd, bool specify_workspace_limit,\n    ScratchAllocator* scratch_allocator) {\n  cudnnConvolutionFwdPreference_t preference =\n      specify_workspace_limit ? CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\n                              : CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;\n  auto memory_limit_bytes =\n      scratch_allocator == nullptr\n          ? 0\n          : scratch_allocator->GetMemoryLimitInBytes(stream);\n  if (memory_limit_bytes < 0) {\n    memory_limit_bytes = 0;\n  }\n\n  cudnnConvolutionFwdAlgo_t algo_to_use;\n  auto status = wrap::cudnnGetConvolutionForwardAlgorithm(\n      parent, ToHandle(dnn_handle), input_nd.handle(), filter.handle(),\n      conv.handle(), output_nd.handle(), preference, memory_limit_bytes,\n      &algo_to_use);\n  CHECK_EQ(status, CUDNN_STATUS_SUCCESS)\n      << \"Unable to find a suitable algorithm for doing forward convolution\";\n  return algo_to_use;\n}\n\ndnn::AlgorithmDesc GetCudnnConvolutionForwardAlgorithm(\n    Stream* stream, CUDAExecutor* parent, void* dnn_handle,\n    const dnn::AlgorithmConfig& algorithm_config, bool is_profiling,\n    const ScopedTensorDescriptor& input_nd,\n    const ScopedFilterDescriptor& filter,\n    const ScopedConvolutionDescriptor& conv,\n    const ScopedTensorDescriptor& output_nd,\n    ScratchAllocator* scratch_allocator, DeviceMemory<uint8>* scratch) {\n  cudnnConvolutionFwdAlgo_t algo;\n  bool use_tensor_ops;\n  if (algorithm_config.algorithm().is_default()) {\n    use_tensor_ops = true;\n    algo = GetCudnnConvolutionForwardAlgo(\n        stream, parent, dnn_handle, input_nd, filter, conv, output_nd,\n        /*specify_workspace_limit=*/scratch_allocator != nullptr,\n        scratch_allocator);\n  } else {\n    use_tensor_ops = algorithm_config.algorithm().tensor_ops_enabled();\n    algo = ToConvForwardAlgo(algorithm_config.algorithm());\n  }\n  size_t size_in_bytes;\n  auto status = wrap::cudnnGetConvolutionForwardWorkspaceSize(\n      parent, ToHandle(dnn_handle), /*srcDesc=*/input_nd.handle(),\n      /*filterDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n      /*destDesc=*/output_nd.handle(), /*algo=*/algo,\n      /*sizeInBytes=*/&size_in_bytes);\n  int64 size_in_bytes_int64 = size_in_bytes;\n  if (TF_PREDICT_FALSE(status != CUDNN_STATUS_SUCCESS)) {\n    CHECK(is_profiling) << \"Cannot query the size of workspace needed \"\n                           \"for the specified algorithm: \"\n                        << algorithm_config.algorithm().algo_id() << \" \"\n                        << ToString(status);\n    // Silently return when we are profiling.\n    return dnn::AlgorithmDesc();\n  }\n  if (TF_PREDICT_FALSE(size_in_bytes_int64 < 0)) {\n    LOG(WARNING) << \"cudnnGetConvolutionForwardWorkspaceSize() returned \"\n                    \"negative sizeInBytes value. This could be a cudnn bug.\";\n    if (TF_PREDICT_TRUE(is_profiling)) {\n      return dnn::AlgorithmDesc();\n    }\n  } else if (size_in_bytes_int64 > 0) {\n    port::StatusOr<DeviceMemory<uint8>> allocated;\n    if (TF_PREDICT_TRUE(scratch_allocator)) {\n      allocated = scratch_allocator->AllocateBytes(stream, size_in_bytes);\n      if (TF_PREDICT_TRUE(allocated.ok())) {\n        *scratch = allocated.ValueOrDie();\n      } else {\n        if (TF_PREDICT_TRUE(is_profiling)) {\n          // Silently return when we are profiling.\n          return dnn::AlgorithmDesc();\n        }\n        LOG(WARNING) << allocated.status().error_message();\n        // For the int8 case, we fail at this point since the no_scratch\n        // algorithm should be set to dnn::kDefaultAlgorithm.\n        CHECK(!algorithm_config.algorithm_no_scratch().is_default())\n            << \"The primary convolution algorithm failed memory allocation, \"\n               \"while a secondary algorithm is not provided.\";\n      }\n    }\n    if (TF_PREDICT_FALSE(!allocated.ok())) {\n      if (algorithm_config.algorithm_no_scratch().is_default()) {\n        use_tensor_ops = true;\n        algo = GetCudnnConvolutionForwardAlgo(\n            stream, parent, dnn_handle, input_nd, filter, conv, output_nd,\n            /*specify_workspace_limit=*/false, nullptr);\n      } else {\n        use_tensor_ops = algorithm_config.algorithm().tensor_ops_enabled();\n        algo = ToConvForwardAlgo(algorithm_config.algorithm_no_scratch());\n      }\n    }\n  }\n\n  return dnn::AlgorithmDesc(algo, use_tensor_ops);\n}\n\n// A helper class to set env-vars and choose options for cudnn-related\n// algorithms.\ntemplate <typename EnvVar>\nclass CudnnEnvVar {\n public:\n  static bool IsEnabled() {\n    static bool is_enabled = IsEnabledImpl();\n    return is_enabled;\n  }\n\n private:\n  static bool IsEnabledImpl() {\n    const char* tf_env_var_val = getenv(EnvVar::kName);\n    if (tf_env_var_val != nullptr) {\n      port::StringPiece tf_env_var_val_str(tf_env_var_val);\n      if (tf_env_var_val_str == \"0\") {\n        return false;\n      }\n      return true;\n    }\n    return EnvVar::kDefaultFlag;\n  }\n};\n\n// A helper struct to decide whether to enable the FFT_TILING algorithms for\n// forward convolution. Before cudnn v5.1 it works fine but since cudnn v5.1\n// it is turned off due to memory corruption caused by some shapes with this\n// algorithm.\n// Before NVIDIA fixes the memory corruption bug, users can explicitly\n// enable the algorithm through an env-var \"TF_ENABLE_FFT_TILING_FORWARD=1\".\nstruct FftTilingForward {\n  static constexpr const char* kName = \"TF_ENABLE_FFT_TILING_FORWARD\";\n  // TODO(yangzihao): turn the default to True when the memory corruption bug\n  // is fixed.\n  static constexpr bool kDefaultFlag = CUDNN_VERSION < 5100;\n};\n\n// A helper struct to decide whether to enable the WINOGRAD_NONFUSED algorithms.\n// By default it is turned on, users can explicitly disable them through an\n// env-var \"TF_ENABLE_WINOGRAD_NONFUSED=0\".\n// https://github.com/tensorflow/tensorflow/pull/4901\nstruct WinogradNonfused {\n  static constexpr const char* kName = \"TF_ENABLE_WINOGRAD_NONFUSED\";\n  // NVIDIA has fixed winograd nonfused bug for cudnn v>=7.\n  // For cudnn v>=5.1, we have a workaround and for any lower version, we\n  // disable it by default.\n  static constexpr bool kDefaultFlag = CUDNN_VERSION >= 5100;\n};\n\n// A helper struct to decide whether to use FP32 as the internal compute type\n// for convolution when the input data type is FP16. By default it is turned on,\n// users can explicitly disable them (choose to use FP16 as the internal compute\n// type) through an env-var \"TF_FP16_CONV_USE_FP32_COMPUTE=0\".\nstruct ConvDoFP32ComputationFP16Input {\n  static constexpr const char* kName = \"TF_FP16_CONV_USE_FP32_COMPUTE\";\n  // Using FP16 as the internal compute type for convolution when the input data\n  // type is FP16 is only supported on architectures with true fp16 support\n  // (compute capability 5.3 and 6.0). Setting this to false in an unsupported\n  // architecture will cause internal errors.\n  static constexpr bool kDefaultFlag = true;\n};\n\n// A group of helper functions to return the internal compute type for\n// convolutions in cudnn.\n// TODO(yangzihao): Add support for float64.\ntemplate <typename T>\ncudnnDataType_t GetConvComputeType() {\n  return CUDNN_DATA_FLOAT;\n}\n\ntemplate <>\ncudnnDataType_t GetConvComputeType<Eigen::half>() {\n  if (CudnnEnvVar<ConvDoFP32ComputationFP16Input>::IsEnabled()) {\n    return CUDNN_DATA_FLOAT;\n  } else {\n    return CUDNN_DATA_HALF;\n  }\n}\n\n}  // namespace\n\ntemplate <class T>\nbool CudnnSupport::DoConvolveImpl(\n    Stream* stream, const BatchDescriptor& batch_descriptor,\n    const DeviceMemory<T>& input_data,\n    const FilterDescriptor& filter_descriptor,\n    const DeviceMemory<T>& filter_data,\n    const ConvolutionDescriptor& convolution_descriptor,\n    const BatchDescriptor& output_descriptor, DeviceMemory<T>* output_data,\n    ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  cudnnDataType_t cudnn_type = GetCudnnDataType<T>();\n  ScopedTensorDescriptor input_nd{parent_, batch_descriptor, cudnn_type};\n  ScopedTensorDescriptor output_nd{parent_, output_descriptor, cudnn_type};\n  ScopedFilterDescriptor filter{parent_, filter_descriptor, batch_descriptor,\n                                cudnn_type};\n  ScopedConvolutionDescriptor conv{parent_, convolution_descriptor,\n                                   GetConvComputeType<T>()};\n\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(FATAL) << \"failed to set stream for cudnn handle: \" << ToString(status);\n  }\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  const bool is_profiling = output_profile_result != nullptr;\n  cudnnConvolutionFwdAlgo_t algo;\n  bool use_tensor_ops;\n  DeviceMemory<uint8> scratch;\n\n  // TODO(pauldonnelly): Replace the following code with a call to\n  //   GetCudnnConvolutionForwardAlgorithm().\n  if (algorithm_config.algorithm().is_default()) {\n    // With the default algorithm, use Cudnn's heuristics.\n    auto get_algorithm =\n        [&](bool specify_limit) SHARED_LOCKS_REQUIRED(dnn_handle_mutex_) {\n          cudnnConvolutionFwdPreference_t preference =\n              specify_limit ? CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\n                            : CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;\n\n          auto memory_limit_bytes =\n              scratch_allocator == nullptr\n                  ? 0\n                  : scratch_allocator->GetMemoryLimitInBytes(stream);\n          if (memory_limit_bytes < 0) {\n            memory_limit_bytes = 0;\n          }\n\n          cudnnConvolutionFwdAlgo_t algo_to_use;\n          status = wrap::cudnnGetConvolutionForwardAlgorithm(\n              parent_, ToHandle(dnn_handle_), input_nd.handle(),\n              filter.handle(), conv.handle(), output_nd.handle(),\n              /*preference=*/preference,\n              /*memoryLimitInBytes=*/memory_limit_bytes,\n              /*algo=*/&algo_to_use);\n          CHECK_EQ(status, CUDNN_STATUS_SUCCESS)\n              << \"Unable to find a suitable \"\n                 \"algorithm for doing forward \"\n                 \"convolution\";\n          return algo_to_use;\n        };\n\n    algo = get_algorithm(/*specify_limit=*/scratch_allocator != nullptr);\n    use_tensor_ops = true;\n    if (scratch_allocator != nullptr) {\n      size_t size_in_bytes;\n      status = wrap::cudnnGetConvolutionForwardWorkspaceSize(\n          parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n          /*filterDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n          /*destDesc=*/output_nd.handle(), /*algo=*/algo,\n          /*sizeInBytes=*/&size_in_bytes);\n      int64 size_in_bytes_int64 = size_in_bytes;\n      if (status == CUDNN_STATUS_SUCCESS && size_in_bytes_int64 != 0) {\n        if (size_in_bytes_int64 > 0) {\n          auto allocated =\n              scratch_allocator->AllocateBytes(stream, size_in_bytes);\n          if (allocated.ok()) {\n            scratch = allocated.ValueOrDie();\n          } else {\n            LOG(WARNING) << allocated.status().error_message();\n          }\n        } else {\n          LOG(WARNING)\n              << \"cudnnGetConvolutionForwardWorkspaceSize() returned \"\n                 \"negative sizeInBytes value. This could be a cudnn bug.\";\n        }\n      }\n    }\n\n    // If we didn't allocate any scratch space (perhaps because of failed\n    // allocation), we force a switch back to the \"no workspace\" algorithm.\n    if (scratch == nullptr) {\n      algo = get_algorithm(/*specify_limit=*/false);\n    }\n  } else {\n    // An algorithm has been specified.\n    dnn::AlgorithmDesc algotype = algorithm_config.algorithm();\n    algo = ToConvForwardAlgo(algotype);\n    use_tensor_ops = algotype.tensor_ops_enabled();\n    conv.set_use_tensor_op_math(use_tensor_ops);\n    size_t size_in_bytes;\n    status = wrap::cudnnGetConvolutionForwardWorkspaceSize(\n        parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n        /*filterDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n        /*destDesc=*/output_nd.handle(), /*algo=*/algo,\n        /*sizeInBytes=*/&size_in_bytes);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      if (is_profiling) {\n        // Silently return when we are profiling.\n        return false;\n      }\n      LOG(FATAL) << \"Cannot query the size of workspace needed for the given \"\n                    \"algorithm: \"\n                 << algorithm_config.algorithm().algo_id();\n    }\n    int64 size_in_bytes_int64 = size_in_bytes;\n    if (size_in_bytes_int64 > 0) {\n      if (scratch_allocator == nullptr) {\n        LOG(FATAL) << \"An allocator must be specified when scratch memory is \"\n                      \"needed\";\n      }\n      auto allocated = scratch_allocator->AllocateBytes(stream, size_in_bytes);\n      if (is_profiling && !allocated.ok()) {\n        // Silently return when we are profiling.\n        return false;\n      }\n      if (allocated.ok()) {\n        scratch = allocated.ValueOrDie();\n      } else {\n        LOG(WARNING) << allocated.status().error_message();\n      }\n      if (scratch == nullptr) {\n        CHECK(!algorithm_config.algorithm_no_scratch().is_default())\n            << \"The primary convolution algorithm failed memory allocation, \"\n               \"while a secondary algorithm is not provided.\";\n        dnn::AlgorithmDesc algotype = algorithm_config.algorithm_no_scratch();\n        algo = ToConvForwardAlgo(algotype);\n        use_tensor_ops = algotype.tensor_ops_enabled();\n        conv.set_use_tensor_op_math(use_tensor_ops);\n      }\n    } else if (size_in_bytes_int64 < 0) {\n      LOG(WARNING) << \"cudnnGetConvolutionForwardWorkspaceSize() returned \"\n                      \"negative sizeInBytes value. This could be a cudnn bug.\";\n    }\n  }\n  std::unique_ptr<CUDATimer> timer;\n  if (is_profiling) {\n    timer.reset(new CUDATimer(parent_));  // NOLINT\n    if (!timer->Init()) {\n      return false;\n    }\n    // The start and stop of the timer should be as close to the Cudnn call as\n    // possible. It is still possible for other threads to issue workload on\n    // to this stream. So it could take multiple profiling measurements.\n    if (!timer->Start(AsCUDAStream(stream))) {\n      timer->Destroy();\n      return false;\n    }\n  }\n  status = wrap::cudnnConvolutionForward(\n      parent_, ToHandle(dnn_handle_),\n      /*alpha=*/&alpha, /*srcDesc=*/input_nd.handle(),\n      /*srcData=*/input_data.opaque(), /*filterDesc=*/filter.handle(),\n      /*filterData=*/filter_data.opaque(), /*convDesc=*/conv.handle(),\n      /*algo=*/algo, /*workSpace=*/scratch.opaque(),\n      /*workSpaceSizeInBytes=*/scratch.size(), /*beta=*/&beta,\n      /*destDesc=*/output_nd.handle(), /*destData=*/output_data->opaque());\n\n  if (is_profiling) {\n    if (!timer->Stop(AsCUDAStream(stream))) {\n      timer->Destroy();\n      return false;\n    }\n    if (status == CUDNN_STATUS_SUCCESS) {\n      dnn::AlgorithmDesc algotype(algo, use_tensor_ops);\n      output_profile_result->set_algorithm(algotype);\n      output_profile_result->set_elapsed_time_in_ms(\n          timer->GetElapsedMilliseconds());\n    }\n    timer->Destroy();\n  }\n\n  if (status != CUDNN_STATUS_SUCCESS) {\n    // Silently return when we are profiling.\n    if (!is_profiling) {\n      LOG(ERROR) << \"failed to enqueue convolution on stream: \"\n                 << ToString(status);\n    }\n    return false;\n  }\n\n  return true;\n}\n\ntemplate <typename Type, typename BiasType, typename ScaleType,\n          int cudnn_data_type, int cudnn_compute_type>\nbool CudnnSupport::DoFusedConvolveImpl(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<Type>& conv_input_data, ScaleType conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<Type>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<Type>& side_input_data, ScaleType side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<BiasType>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<Type>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n#if CUDNN_VERSION < 6000\n  LOG(ERROR) << \"cudnnConvolutionBiasActivationForward() is only \"\n                \"supported for cuDNN version >= 6\";\n  return false;\n#else\n  ScopedTensorDescriptor conv_input_nd{\n      parent_, conv_input_descriptor,\n      static_cast<cudnnDataType_t>(cudnn_data_type)};\n  ScopedTensorDescriptor output_nd{\n      parent_, output_descriptor,\n      static_cast<cudnnDataType_t>(cudnn_data_type)};\n  ScopedFilterDescriptor filter{parent_, filter_descriptor,\n                                conv_input_descriptor,\n                                static_cast<cudnnDataType_t>(cudnn_data_type)};\n  ScopedTensorDescriptor bias_nd{parent_, bias_descriptor, CUDNN_DATA_FLOAT};\n  ScopedConvolutionDescriptor conv{\n      parent_, convolution_descriptor,\n      static_cast<cudnnDataType_t>(cudnn_compute_type)};\n\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  CHECK(status == CUDNN_STATUS_SUCCESS)\n      << \"failed to set stream for cudnn handle: \" << ToString(status);\n\n  const bool is_profiling = output_profile_result != nullptr;\n  DeviceMemory<uint8> scratch;\n  dnn::AlgorithmDesc algotype = GetCudnnConvolutionForwardAlgorithm(\n      stream, parent_, dnn_handle_, algorithm_config, is_profiling,\n      conv_input_nd, filter, conv, output_nd, scratch_allocator, &scratch);\n  if (algotype.is_default()) {\n    if (!is_profiling) {\n      LOG(ERROR) << \"No suitable algorithm found\";\n    }\n    return false;\n  }\n  auto algo = static_cast<cudnnConvolutionFwdAlgo_t>(algotype.algo_id());\n  conv.set_use_tensor_op_math(algotype.tensor_ops_enabled());\n\n  if (activation_mode != dnn::ActivationMode::kRelu) {\n    LOG(ERROR) << \"cudnnConvolutionBiasActivationForward() only supports Relu \"\n                  \"activation.\";\n    return false;\n  }\n\n  std::unique_ptr<CUDATimer> timer;\n  if (is_profiling) {\n    timer.reset(new CUDATimer(parent_));  // NOLINT\n    if (!timer->Init()) {\n      return false;\n    }\n    // The start and stop of the timer should be as close to the Cudnn call as\n    // possible. It is still possible for other threads to issue workload on\n    // to this stream. So it could take multiple profiling measurements.\n    if (!timer->Start(AsCUDAStream(stream))) {\n      timer->Destroy();\n      return false;\n    }\n  }\n  // CUDNN v6 only supports CUDNN_NOT_PROPAGATE_NAN as the reluNanOpt for\n  // activation descriptor. Note that this will change the nan propagation\n  // behavior from separate conv, bias, and relu (which by default is\n  // CUDNN_PROPAGATE_NAN.\n  ScopedActivationDescriptor activation_desc{parent_, activation_mode,\n                                             CUDNN_NOT_PROPAGATE_NAN,\n                                             output_descriptor.value_max()};\n  auto side_input_data_ptr = (side_input_scale == 0) ? output_data->opaque()\n                                                     : side_input_data.opaque();\n\n  VLOG(2) << \"\\nconv_input_scale = \" << conv_input_scale\n          << \"\\nconv_input_nd.handle() = \" << conv_input_nd.handle()\n          << \"\\nconv_input_data.opaque() = \" << conv_input_data.opaque()\n          << \"\\nfilter.handle() = \" << filter.handle()\n          << \"\\nfilter_data.opaque() = \" << filter_data.opaque()\n          << \"\\nconv.handle() = \" << conv.handle() << \"\\nalgo = \" << algo\n          << \"\\nscratch.opaque() = \" << scratch.opaque()\n          << \"\\nscratch.size() = \" << scratch.size()\n          << \"\\nside_input_scale = \" << side_input_scale\n          << \"\\noutput_nd.handle() = \" << output_nd.handle()\n          << \"\\nside_input_data_ptr = \" << side_input_data_ptr\n          << \"\\nbias_nd.handle() = \" << bias_nd.handle()\n          << \"\\nbiases.opaque() = \" << biases.opaque()\n          << \"\\nactivation_desc.handle() = \" << activation_desc.handle()\n          << \"\\noutput_nd.handle() = \" << output_nd.handle()\n          << \"\\noutput_data->opaque() = \" << output_data->opaque();\n\n  status = wrap::cudnnConvolutionBiasActivationForward(\n      parent_, ToHandle(dnn_handle_), /*alpha1=*/&conv_input_scale,\n      /*srcDesc=*/conv_input_nd.handle(), /*srcData=*/conv_input_data.opaque(),\n      /*filterDesc=*/filter.handle(), /*filterData=*/filter_data.opaque(),\n      /*convDesc=*/conv.handle(), algo, /*workSpace=*/scratch.opaque(),\n      /*workSpaceSizeInBytes=*/scratch.size(), /*alpha2=*/&side_input_scale,\n      /*zDesc=*/output_nd.handle(), /*z=*/side_input_data_ptr,\n      /*biasDesc=*/bias_nd.handle(), /*bias=*/biases.opaque(),\n      /*activationDesc=*/activation_desc.handle(),\n      /*destDesc=*/output_nd.handle(), /*destData=*/output_data->opaque());\n\n  if (is_profiling) {\n    if (!timer->Stop(AsCUDAStream(stream))) {\n      timer->Destroy();\n      return false;\n    }\n    if (status == CUDNN_STATUS_SUCCESS) {\n      output_profile_result->set_algorithm(algotype);\n      output_profile_result->set_elapsed_time_in_ms(\n          timer->GetElapsedMilliseconds());\n    }\n    timer->Destroy();\n  }\n\n  if (status != CUDNN_STATUS_SUCCESS) {\n    // Silently return when we are profiling.\n    if (!is_profiling) {\n      LOG(ERROR) << \"failed to enqueue convolution on stream: \"\n                 << ToString(status);\n    }\n    return false;\n  }\n\n  return true;\n#endif  // CUDNN_VERSION < 6000\n}\n\nbool CudnnSupport::GetConvolveAlgorithms(\n    bool with_winograd_nonfused, int cc_major, int cc_minor,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  std::vector<dnn::AlgorithmDesc::Index> algo_types = {\n    // clang-format off\n    CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,\n    CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM,\n    CUDNN_CONVOLUTION_FWD_ALGO_GEMM,\n    CUDNN_CONVOLUTION_FWD_ALGO_DIRECT,\n    CUDNN_CONVOLUTION_FWD_ALGO_FFT,\n#if CUDNN_VERSION >= 5000\n    CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD,\n#endif\n    // clang-format on\n  };\n  if (CudnnEnvVar<FftTilingForward>::IsEnabled()) {\n    algo_types.push_back(CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING);\n  }\n#if CUDNN_VERSION >= 5100\n  if (CudnnEnvVar<WinogradNonfused>::IsEnabled() && with_winograd_nonfused) {\n    algo_types.push_back(CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED);\n  }\n#endif\n\n  out_algorithms->clear();\n  for (auto i : algo_types) {\n    out_algorithms->push_back({i, /*use_tensor_ops=*/false});\n    if (cc_major >= 7 && CUDNN_VERSION >= 7000 && TensorOpMathEnabled()) {\n      out_algorithms->push_back({i, /*use_tensor_ops=*/true});\n    }\n  }\n  return true;\n}\n\nbool CudnnSupport::GetConvolveBackwardDataAlgorithms(\n    bool with_winograd_nonfused, int cc_major, int cc_minor,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  std::vector<dnn::AlgorithmDesc::Index> algo_types = {\n    // clang-format off\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_0,\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_1,\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT,\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING,\n#if CUDNN_VERSION >= 5000\n    CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD,\n#endif\n    // clang-format on\n  };\n#if CUDNN_VERSION >= 5100\n  if (CudnnEnvVar<WinogradNonfused>::IsEnabled() && with_winograd_nonfused) {\n    algo_types.push_back(CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED);\n  }\n#endif\n\n  out_algorithms->clear();\n  for (auto i : algo_types) {\n    out_algorithms->push_back({i, /*use_tensor_ops=*/false});\n    if (cc_major >= 7 && CUDNN_VERSION >= 7000 && TensorOpMathEnabled()) {\n      out_algorithms->push_back({i, /*use_tensor_ops=*/true});\n    }\n  }\n  return true;\n}\n\nbool CudnnSupport::GetConvolveBackwardFilterAlgorithms(\n    bool with_winograd_nonfused, int cc_major, int cc_minor,\n    std::vector<dnn::AlgorithmDesc>* out_algorithms) {\n  std::vector<dnn::AlgorithmDesc::Index> algo_types = {\n      // clang-format off\n      CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0,\n      CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1,\n      CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT,\n      CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3,\n      // Based on cudnn.h, the following is not implemented.\n      // CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD,\n      // clang-format on\n  };\n#if CUDNN_VERSION >= 5100\n  if (CudnnEnvVar<WinogradNonfused>::IsEnabled() && with_winograd_nonfused) {\n    algo_types.push_back(CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED);\n  }\n#endif\n\n  out_algorithms->clear();\n  for (auto i : algo_types) {\n    out_algorithms->push_back({i, /*use_tensor_ops=*/false});\n    if (cc_major >= 7 && CUDNN_VERSION >= 7000 && TensorOpMathEnabled()) {\n      out_algorithms->push_back({i, /*use_tensor_ops=*/true});\n    }\n  }\n  return true;\n}\n\nbool CudnnSupport::DoBatchNormalizationForward(\n    Stream* stream, const DeviceMemory<float>& x,\n    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n    const DeviceMemory<float>& estimated_mean,\n    const DeviceMemory<float>& estimated_variance,\n    const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<float>* y, DeviceMemory<float>* batch_mean,\n    DeviceMemory<float>* batch_var, DeviceMemory<float>* saved_mean,\n    DeviceMemory<float>* saved_inv_var, bool is_training,\n    std::function<const DeviceMemory<float>&()> var_to_inv_var,\n    std::function<void()> inv_var_to_var) {\n  return DoBatchNormalizationForwardImpl<float, float>(\n      stream, dnn::DataType::kFloat, dnn::DataType::kFloat, x, scale, offset,\n      estimated_mean, estimated_variance, x_desc, scale_offset_desc, epsilon, y,\n      batch_mean, batch_var, saved_mean, saved_inv_var, is_training,\n      std::move(var_to_inv_var), std::move(inv_var_to_var));\n}\n\nbool CudnnSupport::DoBatchNormalizationForward(\n    Stream* stream, const DeviceMemory<Eigen::half>& x,\n    const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n    const DeviceMemory<float>& estimated_mean,\n    const DeviceMemory<float>& estimated_variance,\n    const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<Eigen::half>* y, DeviceMemory<float>* batch_mean,\n    DeviceMemory<float>* batch_var, DeviceMemory<float>* saved_mean,\n    DeviceMemory<float>* saved_inv_var, bool is_training,\n    std::function<const DeviceMemory<float>&()> var_to_inv_var,\n    std::function<void()> inv_var_to_var) {\n  return DoBatchNormalizationForwardImpl<Eigen::half, float>(\n      stream, dnn::DataType::kHalf, dnn::DataType::kFloat, x, scale, offset,\n      estimated_mean, estimated_variance, x_desc, scale_offset_desc, epsilon, y,\n      batch_mean, batch_var, saved_mean, saved_inv_var, is_training,\n      std::move(var_to_inv_var), std::move(inv_var_to_var));\n}\n\ntemplate <class T, class U>\nbool CudnnSupport::DoBatchNormalizationForwardImpl(\n    Stream* stream, dnn::DataType input_data_type,\n    dnn::DataType scale_data_type, const DeviceMemory<T>& x,\n    const DeviceMemory<U>& scale, const DeviceMemory<U>& offset,\n    const DeviceMemory<U>& estimated_mean,\n    const DeviceMemory<U>& estimated_variance,\n    const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<T>* y, DeviceMemory<U>* batch_mean, DeviceMemory<U>* batch_var,\n    DeviceMemory<U>* saved_mean, DeviceMemory<U>* saved_inv_var,\n    bool is_training, std::function<const DeviceMemory<U>&()> var_to_inv_var,\n    std::function<void()> inv_var_to_var) {\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n    return false;\n  }\n\n  ScopedTensorDescriptor x_descriptor{parent_, x_desc,\n                                      ToCudnnDataType(input_data_type)};\n  ScopedTensorDescriptor scale_offset_descriptor{\n      parent_, scale_offset_desc, ToCudnnDataType(scale_data_type)};\n  cudnnBatchNormMode_t mode = CUDNN_BATCHNORM_SPATIAL;\n#if CUDNN_VERSION >= 7000\n  if (BatchnormSpatialPersistentEnabled() && is_training) {\n    mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;\n  }\n#endif\n  float one = 1.0;\n  float zero = 0.0;\n\n  if (is_training) {\n    CHECK_EQ(batch_mean->is_null(), batch_var->is_null())\n        << \"batch_mean and batch_var must both be null or both be non-null\";\n\n    void* batch_mean_opaque;\n    void* batch_var_opaque;\n    if (!batch_mean->is_null() && !batch_var->is_null()) {\n      stream->ThenMemZero(batch_mean, batch_mean->size());\n      stream->ThenMemZero(batch_var, batch_var->size());\n      batch_mean_opaque = batch_mean->opaque();\n      batch_var_opaque = batch_var->opaque();\n    } else {\n      batch_mean_opaque = nullptr;\n      batch_var_opaque = nullptr;\n    }\n\n    status = wrap::cudnnBatchNormalizationForwardTraining(\n        parent_, ToHandle(dnn_handle_), mode, &one, &zero,\n        x_descriptor.handle(), x.opaque(), x_descriptor.handle(), y->opaque(),\n        scale_offset_descriptor.handle(), scale.opaque(), offset.opaque(), 1.0,\n        batch_mean_opaque, batch_var_opaque, epsilon, saved_mean->opaque(),\n        saved_inv_var->opaque());\n#if CUDNN_VERSION < 5000\n    CHECK(inv_var_to_var);\n    inv_var_to_var();\n#endif\n  } else {\n#if CUDNN_VERSION < 5000\n    CHECK(var_to_inv_var);\n    const void* maybe_inv_var = var_to_inv_var().opaque();\n#else\n    const void* maybe_inv_var = estimated_variance.opaque();\n#endif\n    status = wrap::cudnnBatchNormalizationForwardInference(\n        parent_, ToHandle(dnn_handle_), mode, &one, &zero,\n        x_descriptor.handle(), x.opaque(), x_descriptor.handle(), y->opaque(),\n        scale_offset_descriptor.handle(), scale.opaque(), offset.opaque(),\n        estimated_mean.opaque(), maybe_inv_var, epsilon);\n  }\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to enqueue forward batch normalization on stream: \"\n               << ToString(status);\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoBatchNormalizationBackward(\n    Stream* stream, const DeviceMemory<float>& y_backprop,\n    const DeviceMemory<float>& x, const DeviceMemory<float>& scale,\n    const DeviceMemory<float>& mean, const DeviceMemory<float>& inv_var,\n    const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<float>* x_backprop, DeviceMemory<float>* scale_backprop,\n    DeviceMemory<float>* offset_backprop) {\n  return DoBatchNormalizationBackwardImpl(\n      stream, CUDNN_DATA_FLOAT, CUDNN_DATA_FLOAT, y_backprop, x, scale, mean,\n      inv_var, x_desc, scale_offset_desc, epsilon, x_backprop, scale_backprop,\n      offset_backprop);\n}\n\nbool CudnnSupport::DoBatchNormalizationBackward(\n    Stream* stream, const DeviceMemory<Eigen::half>& y_backprop,\n    const DeviceMemory<Eigen::half>& x, const DeviceMemory<float>& scale,\n    const DeviceMemory<float>& mean, const DeviceMemory<float>& inv_var,\n    const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<Eigen::half>* x_backprop, DeviceMemory<float>* scale_backprop,\n    DeviceMemory<float>* offset_backprop) {\n  return DoBatchNormalizationBackwardImpl(\n      stream, CUDNN_DATA_HALF, CUDNN_DATA_FLOAT, y_backprop, x, scale, mean,\n      inv_var, x_desc, scale_offset_desc, epsilon, x_backprop, scale_backprop,\n      offset_backprop);\n}\n\ntemplate <class T, class U>\nbool CudnnSupport::DoBatchNormalizationBackwardImpl(\n    Stream* stream, int cudnn_input_type, int cudnn_scale_type,\n    const DeviceMemory<T>& y_backprop, const DeviceMemory<T>& x,\n    const DeviceMemory<U>& scale, const DeviceMemory<U>& mean,\n    const DeviceMemory<U>& inv_var, const dnn::BatchDescriptor& x_desc,\n    const dnn::BatchDescriptor& scale_offset_desc, const double epsilon,\n    DeviceMemory<T>* x_backprop, DeviceMemory<U>* scale_backprop,\n    DeviceMemory<U>* offset_backprop) {\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n    return false;\n  }\n\n  ScopedTensorDescriptor x_descriptor{\n      parent_, x_desc, static_cast<cudnnDataType_t>(cudnn_input_type)};\n  ScopedTensorDescriptor scale_offset_descriptor{\n      parent_, scale_offset_desc,\n      static_cast<cudnnDataType_t>(cudnn_scale_type)};\n  cudnnBatchNormMode_t mode = CUDNN_BATCHNORM_SPATIAL;\n#if CUDNN_VERSION >= 7000\n  if (BatchnormSpatialPersistentEnabled()) {\n    mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;\n  }\n#endif\n  float one = 1.0;\n  float zero = 0.0;\n\n  status = wrap::cudnnBatchNormalizationBackward(\n      parent_, ToHandle(dnn_handle_), mode, &one, &zero, &one, &zero,\n      x_descriptor.handle(), x.opaque(), x_descriptor.handle(),\n      y_backprop.opaque(), x_descriptor.handle(), x_backprop->opaque(),\n      scale_offset_descriptor.handle(), scale.opaque(),\n      scale_backprop->opaque(), offset_backprop->opaque(), epsilon,\n      mean.opaque(), inv_var.opaque());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to enqueue backward batch normalization on stream: \"\n               << ToString(status);\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoConvolve(\n    Stream* stream, const BatchDescriptor& batch_descriptor,\n    const DeviceMemory<float>& input_data,\n    const FilterDescriptor& filter_descriptor,\n    const DeviceMemory<float>& filter_data,\n    const ConvolutionDescriptor& convolution_descriptor,\n    const BatchDescriptor& output_descriptor, DeviceMemory<float>* output_data,\n    ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoConvolveImpl<float>(\n      stream, batch_descriptor, input_data, filter_descriptor, filter_data,\n      convolution_descriptor, output_descriptor, output_data, scratch_allocator,\n      algorithm_config, output_profile_result);\n}\n\nbool CudnnSupport::DoConvolve(\n    Stream* stream, const BatchDescriptor& batch_descriptor,\n    const DeviceMemory<double>& input_data,\n    const FilterDescriptor& filter_descriptor,\n    const DeviceMemory<double>& filter_data,\n    const ConvolutionDescriptor& convolution_descriptor,\n    const BatchDescriptor& output_descriptor,\n    DeviceMemory<double>* output_data) {\n  LOG(ERROR) << \"double-based DNN not yet implemented\";\n  return false;\n}\n\nbool CudnnSupport::DoConvolve(\n    Stream* stream, const BatchDescriptor& batch_descriptor,\n    const DeviceMemory<Eigen::half>& input_data,\n    const FilterDescriptor& filter_descriptor,\n    const DeviceMemory<Eigen::half>& filter_data,\n    const ConvolutionDescriptor& convolution_descriptor,\n    const BatchDescriptor& output_descriptor,\n    DeviceMemory<Eigen::half>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoConvolveImpl<Eigen::half>(\n      stream, batch_descriptor, input_data, filter_descriptor, filter_data,\n      convolution_descriptor, output_descriptor, output_data, scratch_allocator,\n      algorithm_config, output_profile_result);\n}\n\nbool CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<double>& conv_input_data, double conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<double>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<double>& side_input_data, double side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<double>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<double>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedConvolveImpl<double, double, double, CUDNN_DATA_DOUBLE,\n                             CUDNN_DATA_DOUBLE>(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data, scratch_allocator, algorithm_config,\n      output_profile_result);\n}\n\nbool CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<float>& conv_input_data, float conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<float>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<float>& side_input_data, float side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<float>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<float>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedConvolveImpl<float, float, float, CUDNN_DATA_FLOAT,\n                             CUDNN_DATA_FLOAT>(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data, scratch_allocator, algorithm_config,\n      output_profile_result);\n}\n\nbool CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<Eigen::half>& conv_input_data, float conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<Eigen::half>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<Eigen::half>& side_input_data, float side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<Eigen::half>& biases,\n    dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<Eigen::half>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoFusedConvolveImpl<Eigen::half, Eigen::half, float, CUDNN_DATA_HALF,\n                             CUDNN_DATA_FLOAT>(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data, scratch_allocator, algorithm_config,\n      output_profile_result);\n}\n\nbool CudnnSupport::DoFusedConvolve(\n    Stream* stream, const dnn::BatchDescriptor& conv_input_descriptor,\n    const DeviceMemory<int8>& conv_input_data, float conv_input_scale,\n    const dnn::FilterDescriptor& filter_descriptor,\n    const DeviceMemory<int8>& filter_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const DeviceMemory<int8>& side_input_data, float side_input_scale,\n    const dnn::BatchDescriptor& bias_descriptor,\n    const DeviceMemory<float>& biases, dnn::ActivationMode activation_mode,\n    const dnn::BatchDescriptor& output_descriptor,\n    DeviceMemory<int8>* output_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n#if CUDNN_VERSION < 6000\n  LOG(WARNING) << \"cudnnConvolutionBiasActivationForward() is only \"\n                  \"supported for cuDNN version >= 6\";\n  return false;\n#else\n  int cc_major, cc_minor;\n  stream->parent()->GetDeviceDescription().cuda_compute_capability(&cc_major,\n                                                                   &cc_minor);\n  if (cc_major < 6 || (cc_major == 6 && cc_minor < 1)) {\n    LOG(WARNING) << \"cudnnConvolutionBiasActivationForward() for int8 is only \"\n                    \"supported on GPUs with compute capability 6.1 or later.\";\n    return false;\n  }\n  return DoFusedConvolveImpl<int8, float, float, CUDNN_DATA_INT8x4,\n                             CUDNN_DATA_INT32>(\n      stream, conv_input_descriptor, conv_input_data, conv_input_scale,\n      filter_descriptor, filter_data, convolution_descriptor, side_input_data,\n      side_input_scale, bias_descriptor, biases, activation_mode,\n      output_descriptor, output_data, scratch_allocator, algorithm_config,\n      output_profile_result);\n#endif\n}\n\ntemplate<class T>\nDeviceMemory<T> CudnnSupport::MaybeTransformLayout(\n    Stream* stream,\n    BatchDescriptor* output_descriptor,\n    DeviceMemory<T> backward_output_data,\n    std::unique_ptr<TemporaryDeviceMemory<T>>* transform_scratch) {\n  if (output_descriptor->layout() == dnn::DataLayout::kBatchDepthYX) {\n    return backward_output_data;\n  }\n  CHECK(output_descriptor->layout() == dnn::DataLayout::kBatchYXDepth);\n  *transform_scratch =\n      stream->AllocateTemporaryArray<T>(backward_output_data.ElementCount())\n          .ConsumeValueOrDie();\n  BatchDescriptor transformed_output_descriptor;\n  transformed_output_descriptor.CloneFrom(*output_descriptor);\n  transformed_output_descriptor.set_layout(dnn::DataLayout::kBatchDepthYX);\n  cudnnDataType_t cudnn_type = GetCudnnDataType<T>();\n  ScopedTensorDescriptor orig_out_back_nd{parent_, *output_descriptor,\n                                          cudnn_type};\n  ScopedTensorDescriptor transformed_out_back_nd{\n      parent_, transformed_output_descriptor, cudnn_type};\n\n  float alpha = 1.0f;\n  float beta = 0.0f;\n  auto status = wrap::cudnnTransformTensor(\n      parent_, ToHandle(dnn_handle_), &alpha, orig_out_back_nd.handle(),\n      backward_output_data.opaque(), &beta, transformed_out_back_nd.handle(),\n      (*transform_scratch)->mutable_device_memory()->opaque());\n\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(FATAL) << \"Failed to transform the data layout.\";\n  }\n  output_descriptor->set_layout(dnn::DataLayout::kBatchDepthYX);\n  return (*transform_scratch)->device_memory();\n}\n\nbool CudnnSupport::DoTransformTensor(Stream* stream,\n                                     const dnn::BatchDescriptor& input_desc,\n                                     dnn::DataType input_type,\n                                     const DeviceMemoryBase& input_data,\n                                     const dnn::BatchDescriptor& output_desc,\n                                     dnn::DataType output_type, float scale,\n                                     DeviceMemoryBase* output_data) {\n  mutex_lock lock{dnn_handle_mutex_};\n  float beta = 0.0f;\n  ScopedTensorDescriptor input_tensor_desc(\n      parent_, input_desc, ToCudnnDataType(input_type, input_desc.layout()));\n  ScopedTensorDescriptor output_tensor_desc(\n      parent_, output_desc, ToCudnnDataType(output_type, output_desc.layout()));\n  cudnnStatus_t status = wrap::cudnnTransformTensor(\n      parent_, ToHandle(dnn_handle_), &scale, input_tensor_desc.handle(),\n      input_data.opaque(), &beta, output_tensor_desc.handle(),\n      output_data->opaque());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"Could not transform a tensor with layout \"\n               << input_desc.ToString() << \" and data type \"\n               << static_cast<int>(input_type) << \" to another with layout \"\n               << output_desc.ToString() << \" and data type \"\n               << static_cast<int>(output_type) << \": \" << ToString(status);\n    return false;\n  }\n  return true;\n}\n\ntemplate <class T>\nbool CudnnSupport::DoConvolveBackwardDataImpl(\n    Stream* stream,\n    const FilterDescriptor& filter_descriptor,\n    const DeviceMemory<T>& filter_data,\n    const BatchDescriptor& output_descriptor_in,\n    DeviceMemory<T> backward_output_data,\n    const ConvolutionDescriptor& convolution_descriptor,\n    const BatchDescriptor& input_descriptor,\n    DeviceMemory<T>* backward_input_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(FATAL) << \"failed to set stream for cudnn handle: \" << ToString(status);\n  }\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  // TBD(keveman): remove once cuDNN supports kBatchYXDepth for backward pass.\n  BatchDescriptor output_descriptor;\n  output_descriptor.CloneFrom(output_descriptor_in);\n  std::unique_ptr<TemporaryDeviceMemory<T>> transform_scratch;\n  backward_output_data = MaybeTransformLayout(\n      stream, &output_descriptor, backward_output_data, &transform_scratch);\n\n  cudnnDataType_t cudnn_type = GetCudnnDataType<T>();\n  ScopedTensorDescriptor out_back_nd{parent_, output_descriptor, cudnn_type};\n  ScopedTensorDescriptor in_back_nd{parent_, input_descriptor, cudnn_type};\n  ScopedFilterDescriptor filter{parent_, filter_descriptor, input_descriptor,\n                                cudnn_type};\n  ScopedConvolutionDescriptor conv{parent_, convolution_descriptor,\n                                   GetConvComputeType<T>()};\n\n  const bool is_profiling = output_profile_result != nullptr;\n  cudnnConvolutionBwdDataAlgo_t algo;\n  DeviceMemory<uint8> scratch;\n\n  if (algorithm_config.algorithm().is_default()) {\n    // With the default algorithm, use Cudnn's heuristics.\n    auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\n        dnn_handle_mutex_) -> cudnnConvolutionBwdDataAlgo_t {\n      cudnnConvolutionBwdDataPreference_t preference =\n          specify_limit ? CUDNN_CONVOLUTION_BWD_DATA_SPECIFY_WORKSPACE_LIMIT\n                        : CUDNN_CONVOLUTION_BWD_DATA_NO_WORKSPACE;\n\n      auto memory_limit_bytes =\n          scratch_allocator == nullptr\n              ? 0\n              : scratch_allocator->GetMemoryLimitInBytes(stream);\n      if (memory_limit_bytes < 0) {\n        memory_limit_bytes = 0;\n      }\n      cudnnConvolutionBwdDataAlgo_t algo_to_use;\n      cudnnStatus_t status = wrap::cudnnGetConvolutionBackwardDataAlgorithm(\n          parent_, ToHandle(dnn_handle_),\n          /*filterDesc=*/filter.handle(),\n          /*diffDesc=*/out_back_nd.handle(),\n          /*convDesc=*/conv.handle(),\n          /*gradDesc=*/in_back_nd.handle(),\n          /*preference=*/preference,\n          /*memoryLimitInBytes=*/memory_limit_bytes,\n          /*algo=*/&algo_to_use);\n      CHECK_EQ(status, CUDNN_STATUS_SUCCESS) << \"Unable to find a suitable \"\n                                                \"algorithm for doing backward \"\n                                                \"data convolution\";\n      return algo_to_use;\n    };\n\n    algo = get_algorithm(/*specify_limit=*/scratch_allocator != nullptr);\n\n    if (scratch_allocator != nullptr) {\n      size_t size_in_bytes;\n      status = wrap::cudnnGetConvolutionBackwardDataWorkspaceSize(\n          parent_, ToHandle(dnn_handle_),\n          /*filterDesc=*/filter.handle(),\n          /*diffDesc=*/out_back_nd.handle(),\n          /*convDesc=*/conv.handle(),\n          /*gradDesc=*/in_back_nd.handle(),\n          /*algo=*/algo,\n          /*sizeInBytes=*/&size_in_bytes);\n      int64 size_in_bytes_int64 = size_in_bytes;\n      if (status == CUDNN_STATUS_SUCCESS && size_in_bytes_int64 != 0) {\n        if (size_in_bytes_int64 > 0) {\n          auto allocated =\n              scratch_allocator->AllocateBytes(stream, size_in_bytes);\n          if (allocated.ok()) {\n            scratch = allocated.ValueOrDie();\n          } else {\n            LOG(WARNING) << allocated.status().error_message();\n          }\n        } else {\n          LOG(WARNING)\n              << \"cudnnGetConvolutionBackwardDataWorkspaceSize() returned \"\n                 \"negative sizeInBytes value. This could be a cudnn bug.\";\n        }\n      }\n    }\n\n    // If we didn't allocate any scratch space (perhaps because of failed\n    // allocation), we force a switch back to the \"no workspace\" algorithm.\n    if (scratch == nullptr) {\n      algo = get_algorithm(/*specify_limit=*/false);\n    }\n  } else {\n    // An algorithm has been specified.\n    dnn::AlgorithmDesc algotype = algorithm_config.algorithm();\n    algo = ToConvBackwardDataAlgo(algotype);\n    conv.set_use_tensor_op_math(algotype.tensor_ops_enabled());\n    size_t size_in_bytes;\n    status = wrap::cudnnGetConvolutionBackwardDataWorkspaceSize(\n        parent_, ToHandle(dnn_handle_),\n        /*filterDesc=*/filter.handle(),\n        /*diffDesc=*/out_back_nd.handle(),\n        /*convDesc=*/conv.handle(),\n        /*gradDesc=*/in_back_nd.handle(),\n        /*algo=*/algo,\n        /*sizeInBytes=*/&size_in_bytes);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      if (is_profiling) {\n        // Silently return when we are profiling.\n        return false;\n      }\n      LOG(FATAL) << \"Cannot query the size of workspace needed for the given \"\n                    \"algorithm: \"\n                 << algorithm_config.algorithm().algo_id();\n    }\n    int64 size_in_bytes_int64 = size_in_bytes;\n    if (size_in_bytes_int64 > 0) {\n      if (scratch_allocator == nullptr) {\n        LOG(FATAL) << \"An allocator must be specified when scratch memory is \"\n                      \"needed\";\n      }\n      auto allocated = scratch_allocator->AllocateBytes(stream, size_in_bytes);\n      if (is_profiling && !allocated.ok()) {\n        // Silently return when we are profiling.\n        return false;\n      }\n      if (allocated.ok()) {\n        scratch = allocated.ValueOrDie();\n      } else {\n        LOG(WARNING) << allocated.status().error_message();\n      }\n      if (scratch == nullptr) {\n        CHECK(!algorithm_config.algorithm_no_scratch().is_default())\n            << \"The primary convolution algorithm failed memory allocation, \"\n               \"while a secondary algorithm is not provided.\";\n        dnn::AlgorithmDesc algotype = algorithm_config.algorithm_no_scratch();\n        algo = ToConvBackwardDataAlgo(algotype);\n        conv.set_use_tensor_op_math(algotype.tensor_ops_enabled());\n      }\n    } else if (size_in_bytes_int64 < 0) {\n      LOG(WARNING) << \"cudnnGetConvolutionBackwardDataWorkspaceSize() returned \"\n                      \"negative sizeInBytes value. This could be a cudnn bug.\";\n    }\n  }\n\n  std::unique_ptr<CUDATimer> timer;\n  if (is_profiling) {\n    timer.reset(new CUDATimer(parent_));  // NOLINT\n    timer->Init();\n    // The start and stop of the timer should be as close to the Cudnn call as\n    // possible. It is still possible for other threads to issue workload on\n    // to this stream. So it could take multiple profiling measurements.\n    timer->Start(AsCUDAStream(stream));\n  }\n\n#if CUDNN_VERSION >= 5000\n  status = wrap::cudnnConvolutionBackwardData(\n#else\n  status = wrap::cudnnConvolutionBackwardData_v3(\n#endif\n      parent_, ToHandle(dnn_handle_),\n      /*alpha=*/&alpha,\n      /*filterDesc=*/filter.handle(),\n      /*filterData=*/filter_data.opaque(),\n      /*diffDesc=*/out_back_nd.handle(),\n      /*diffData=*/backward_output_data.opaque(),\n      /*convDesc=*/conv.handle(),\n      /*algo=*/algo,\n      /*workSpace=*/scratch.opaque(),\n      /*workSpaceSizeInBytes=*/scratch.size(),\n      /*beta=*/&beta,\n      /*gradDesc=*/in_back_nd.handle(),\n      /*gradData=*/backward_input_data->opaque());\n  if (is_profiling) {\n    timer->Stop(AsCUDAStream(stream));\n    if (status == CUDNN_STATUS_SUCCESS) {\n      bool use_tensor_ops = algorithm_config.algorithm().tensor_ops_enabled();\n      dnn::AlgorithmDesc algotype(algo, use_tensor_ops);\n      output_profile_result->set_algorithm(algotype);\n      output_profile_result->set_elapsed_time_in_ms(\n          timer->GetElapsedMilliseconds());\n    }\n    timer->Destroy();\n  }\n  if (status != CUDNN_STATUS_SUCCESS) {\n    // Silently return when we are profiling.\n    if (!is_profiling) {\n      LOG(ERROR) << \"failed to enqueue convolution on stream: \"\n                 << ToString(status);\n    }\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoConvolveBackwardData(\n    Stream* stream, const FilterDescriptor& filter_descriptor,\n    const DeviceMemory<float>& filter_data,\n    const BatchDescriptor& output_descriptor_in,\n    DeviceMemory<float> backward_output_data,\n    const ConvolutionDescriptor& convolution_descriptor,\n    const BatchDescriptor& input_descriptor,\n    DeviceMemory<float>* backward_input_data,\n    ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoConvolveBackwardDataImpl(stream, filter_descriptor, filter_data,\n                                    output_descriptor_in, backward_output_data,\n                                    convolution_descriptor, input_descriptor,\n                                    backward_input_data, scratch_allocator,\n                                    algorithm_config, output_profile_result);\n}\n\nbool CudnnSupport::DoConvolveBackwardData(\n    Stream* stream, const FilterDescriptor& filter_descriptor,\n    const DeviceMemory<Eigen::half>& filter_data,\n    const BatchDescriptor& output_descriptor_in,\n    DeviceMemory<Eigen::half> backward_output_data,\n    const ConvolutionDescriptor& convolution_descriptor,\n    const BatchDescriptor& input_descriptor,\n    DeviceMemory<Eigen::half>* backward_input_data,\n    ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoConvolveBackwardDataImpl(stream, filter_descriptor, filter_data,\n                                    output_descriptor_in, backward_output_data,\n                                    convolution_descriptor, input_descriptor,\n                                    backward_input_data, scratch_allocator,\n                                    algorithm_config, output_profile_result);\n}\n\ntemplate <class T>\nbool CudnnSupport::DoConvolveBackwardFilterImpl(\n    Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n    const DeviceMemory<T>& input_data,\n    const dnn::BatchDescriptor& output_descriptor_in,\n    DeviceMemory<T> backward_output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemory<T>* backward_filter_data, ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(FATAL) << \"failed to set stream for cudnn handle: \" << ToString(status);\n  }\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  // TBD(keveman): remove once cuDNN supports kBatchYXDepth for backward pass.\n  BatchDescriptor output_descriptor;\n  output_descriptor.CloneFrom(output_descriptor_in);\n  std::unique_ptr<TemporaryDeviceMemory<T>> transform_scratch;\n  backward_output_data = MaybeTransformLayout(\n      stream, &output_descriptor, backward_output_data, &transform_scratch);\n\n  cudnnDataType_t cudnn_type = GetCudnnDataType<T>();\n  ScopedTensorDescriptor out_back_nd{parent_, output_descriptor, cudnn_type};\n  ScopedTensorDescriptor input_nd{parent_, input_descriptor, cudnn_type};\n  ScopedFilterDescriptor filter{parent_, filter_descriptor, input_descriptor,\n                                cudnn_type};\n  ScopedConvolutionDescriptor conv{parent_, convolution_descriptor,\n                                   GetConvComputeType<T>()};\n\n  const bool is_profiling = output_profile_result != nullptr;\n  cudnnConvolutionBwdFilterAlgo_t algo;\n  DeviceMemory<uint8> scratch;\n\n  if (algorithm_config.algorithm().is_default()) {\n    // With the default algorithm, use Cudnn's heuristics.\n\n    // Lambda that retrieves the algorithm.\n    // specify_limit will occur when we have a scratch allocator and it succeeds\n    // in allocating; otherwise, we'll fall back to the \"no workspace\" version.\n    auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\n        dnn_handle_mutex_) {\n      cudnnConvolutionBwdFilterPreference_t preference =\n          specify_limit ? CUDNN_CONVOLUTION_BWD_FILTER_SPECIFY_WORKSPACE_LIMIT\n                        : CUDNN_CONVOLUTION_BWD_FILTER_NO_WORKSPACE;\n\n      auto memory_limit_bytes =\n          scratch_allocator == nullptr\n              ? 0\n              : scratch_allocator->GetMemoryLimitInBytes(stream);\n      if (memory_limit_bytes < 0) {\n        memory_limit_bytes = 0;\n      }\n\n      cudnnConvolutionBwdFilterAlgo_t algo_to_use;\n      cudnnStatus_t status = wrap::cudnnGetConvolutionBackwardFilterAlgorithm(\n          parent_, ToHandle(dnn_handle_),\n          /*srcDesc=*/input_nd.handle(),\n          /*diffDesc=*/out_back_nd.handle(),\n          /*convDesc=*/conv.handle(),\n          /*gradDesc=*/filter.handle(),\n          /*preference=*/preference,\n          /*memoryLimitInBytes=*/memory_limit_bytes,\n          /*algo=*/&algo_to_use);\n      CHECK_EQ(status, CUDNN_STATUS_SUCCESS) << \"Unable to find a suitable \"\n                                                \"algorithm for doing backward \"\n                                                \"filter convolution\";\n      return algo_to_use;\n    };\n\n    algo = get_algorithm(/*specify_limit=*/scratch_allocator != nullptr);\n\n    if (scratch_allocator != nullptr) {\n      size_t size_in_bytes;\n      status = wrap::cudnnGetConvolutionBackwardFilterWorkspaceSize(\n          parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n          /*diffDesc=*/out_back_nd.handle(), /*convDesc=*/conv.handle(),\n          /*gradDesc=*/filter.handle(), /*algo=*/algo,\n          /*sizeInBytes=*/&size_in_bytes);\n      int64 size_in_bytes_int64 = size_in_bytes;\n      if (status == CUDNN_STATUS_SUCCESS && size_in_bytes_int64 != 0) {\n        if (size_in_bytes_int64 > 0) {\n          auto allocated =\n              scratch_allocator->AllocateBytes(stream, size_in_bytes);\n          if (allocated.ok()) {\n            scratch = allocated.ValueOrDie();\n          } else {\n            LOG(WARNING) << allocated.status().error_message();\n          }\n        } else {\n          LOG(WARNING)\n              << \"cudnnGetConvolutionBackwardFilterWorkspaceSize() returned \"\n                 \"negative sizeInBytes value. This could be a cudnn bug.\";\n        }\n      }\n    }\n\n    // If we didn't allocate any scratch space (perhaps because of failed\n    // allocation), we force a switch back to the \"no workspace\" algorithm.\n    if (scratch == nullptr) {\n      algo = get_algorithm(/*specify_limit=*/false);\n    }\n  } else {\n    // An algorithm has been specified.\n    dnn::AlgorithmDesc algotype = algorithm_config.algorithm();\n    algo = ToConvBackwardFilterAlgo(algotype);\n    conv.set_use_tensor_op_math(algotype.tensor_ops_enabled());\n\n    size_t size_in_bytes;\n    status = wrap::cudnnGetConvolutionBackwardFilterWorkspaceSize(\n        parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n        /*diffDesc=*/out_back_nd.handle(), /*convDesc=*/conv.handle(),\n        /*gradDesc=*/filter.handle(), /*algo=*/algo,\n        /*sizeInBytes=*/&size_in_bytes);\n    if (status != CUDNN_STATUS_SUCCESS) {\n      if (is_profiling) {\n        // Silently return when we are profiling.\n        return false;\n      }\n      LOG(FATAL) << \"Cannot query the size of workspace needed for the given \"\n                    \"algorithm: \"\n                 << algorithm_config.algorithm().algo_id();\n    }\n    int64 size_in_bytes_int64 = size_in_bytes;\n    if (size_in_bytes_int64 > 0) {\n      if (scratch_allocator == nullptr) {\n        LOG(FATAL) << \"An allocator must be specified when scratch memory is \"\n                      \"needed\";\n      }\n      auto allocated = scratch_allocator->AllocateBytes(stream, size_in_bytes);\n      if (is_profiling && !allocated.ok()) {\n        // Silently return when we are profiling.\n        return false;\n      }\n      if (allocated.ok()) {\n        scratch = allocated.ValueOrDie();\n      } else {\n        LOG(WARNING) << allocated.status().error_message();\n      }\n      if (scratch == nullptr) {\n        CHECK(!algorithm_config.algorithm_no_scratch().is_default())\n            << \"The primary convolution algorithm failed memory allocation, \"\n               \"while a secondary algorithm is not provided.\";\n        dnn::AlgorithmDesc algotype = algorithm_config.algorithm_no_scratch();\n        algo = ToConvBackwardFilterAlgo(algotype);\n        conv.set_use_tensor_op_math(algotype.tensor_ops_enabled());\n      }\n    } else if (size_in_bytes_int64 < 0) {\n      LOG(WARNING)\n          << \"cudnnGetConvolutionBackwardFilterWorkspaceSize() returned \"\n             \"negative sizeInBytes value. This could be a cudnn bug.\";\n    }\n  }\n\n  std::unique_ptr<CUDATimer> timer;\n  if (is_profiling) {\n    timer.reset(new CUDATimer(parent_));  // NOLINT\n    timer->Init();\n    // The start and stop of the timer should be as close to the Cudnn call as\n    // possible. It is still possible for other threads to issue workload on\n    // to this stream. So it could take multiple profiling measurements.\n    timer->Start(AsCUDAStream(stream));\n  }\n\n#if CUDNN_VERSION >= 5000\n  status = wrap::cudnnConvolutionBackwardFilter(\n#else\n  status = wrap::cudnnConvolutionBackwardFilter_v3(\n#endif\n      parent_, ToHandle(dnn_handle_), /*alpha=*/&alpha,\n      /*srcDesc=*/input_nd.handle(),\n      /*srcData=*/input_data.opaque(),\n      /*diffDesc=*/out_back_nd.handle(),\n      /*diffData=*/backward_output_data.opaque(),\n      /*convDesc=*/conv.handle(),\n      /*algo=*/algo,\n      /*workSpace=*/scratch.opaque(),\n      /*workSpaceSizeInBytes=*/scratch.size(),\n      /*beta=*/&beta,\n      /*gradDesc=*/filter.handle(),\n      /*gradData=*/backward_filter_data->opaque());\n\n  if (is_profiling) {\n    timer->Stop(AsCUDAStream(stream));\n    if (status == CUDNN_STATUS_SUCCESS) {\n      bool use_tensor_ops = algorithm_config.algorithm().tensor_ops_enabled();\n      dnn::AlgorithmDesc algotype(algo, use_tensor_ops);\n      output_profile_result->set_algorithm(algotype);\n      output_profile_result->set_elapsed_time_in_ms(\n          timer->GetElapsedMilliseconds());\n    }\n    timer->Destroy();\n  }\n  if (status != CUDNN_STATUS_SUCCESS) {\n    // Silently return when we are profiling.\n    if (!is_profiling) {\n      LOG(ERROR) << \"failed to enqueue convolution on stream: \"\n                 << ToString(status);\n    }\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoConvolveBackwardFilter(\n    Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n    const DeviceMemory<float>& input_data,\n    const dnn::BatchDescriptor& output_descriptor_in,\n    DeviceMemory<float> backward_output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemory<float>* backward_filter_data,\n    ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoConvolveBackwardFilterImpl(\n      stream, input_descriptor, input_data, output_descriptor_in,\n      backward_output_data, convolution_descriptor, filter_descriptor,\n      backward_filter_data, scratch_allocator, algorithm_config,\n      output_profile_result);\n}\n\nbool CudnnSupport::DoConvolveBackwardFilter(\n    Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::BatchDescriptor& output_descriptor_in,\n    DeviceMemory<Eigen::half> backward_output_data,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    const dnn::FilterDescriptor& filter_descriptor,\n    DeviceMemory<Eigen::half>* backward_filter_data,\n    ScratchAllocator* scratch_allocator,\n    const dnn::AlgorithmConfig& algorithm_config,\n    dnn::ProfileResult* output_profile_result) {\n  return DoConvolveBackwardFilterImpl(\n      stream, input_descriptor, input_data, output_descriptor_in,\n      backward_output_data, convolution_descriptor, filter_descriptor,\n      backward_filter_data, scratch_allocator, algorithm_config,\n      output_profile_result);\n}\n\ntemplate <class T>\nbool CudnnSupport::DoConvolveBackwardBiasImpl(\n    Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n    const DeviceMemory<T>& input_data,\n    const dnn::BatchDescriptor& bias_descriptor,\n    DeviceMemory<T>* backward_bias_data) {\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(FATAL) << \"failed to set stream for cudnn handle: \" << ToString(status);\n  }\n\n  cudnnDataType_t cudnn_type = GetCudnnDataType<T>();\n  ScopedTensorDescriptor input_nd{parent_, input_descriptor, cudnn_type};\n  ScopedTensorDescriptor bias_nd{parent_, bias_descriptor, cudnn_type};\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  status = wrap::cudnnConvolutionBackwardBias(\n      parent_, ToHandle(dnn_handle_), &alpha, input_nd.handle(),\n      input_data.opaque(), &beta, bias_nd.handle(),\n      backward_bias_data->opaque());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to enqueue backward convolution on stream: \"\n               << ToString(status);\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoConvolveBackwardBias(\n    Stream* stream, const BatchDescriptor& input_descriptor,\n    const DeviceMemory<double>& input_data,\n    const BatchDescriptor& bias_descriptor,\n    DeviceMemory<double>* backward_bias_data) {\n  return DoConvolveBackwardBiasImpl(stream, input_descriptor, input_data,\n                                    bias_descriptor, backward_bias_data);\n}\n\nbool CudnnSupport::DoConvolveBackwardBias(\n    Stream* stream, const BatchDescriptor& input_descriptor,\n    const DeviceMemory<float>& input_data,\n    const BatchDescriptor& bias_descriptor,\n    DeviceMemory<float>* backward_bias_data) {\n  return DoConvolveBackwardBiasImpl(stream, input_descriptor, input_data,\n                                    bias_descriptor, backward_bias_data);\n}\n\nbool CudnnSupport::DoConvolveBackwardBias(\n    Stream* stream, const BatchDescriptor& input_descriptor,\n    const DeviceMemory<Eigen::half>& input_data,\n    const BatchDescriptor& bias_descriptor,\n    DeviceMemory<Eigen::half>* backward_bias_data) {\n  return DoConvolveBackwardBiasImpl(stream, input_descriptor, input_data,\n                                    bias_descriptor, backward_bias_data);\n}\n\nbool CudnnSupport::DoMatMul(Stream* stream,\n                            const DeviceMemory<float>& input_data,\n                            const DeviceMemory<float>& weights,\n                            const dnn::BatchDescriptor& input_dimensions,\n                            const dnn::BatchDescriptor& output_dimensions,\n                            DeviceMemory<float>* output_data) {\n  if (input_dimensions.count() != output_dimensions.count()) {\n    LOG(ERROR) << \"MatMul input and output dimensions are not compatible.\";\n    return false;\n  }\n\n  // We do not permute the input or output, instead we just\n  // reinterpret the layout. We are working with row-major matrices\n  // and the rows of the input and output correspond to batch, so\n  // batch has to be outermost in both the input and output.\n  //\n  // By adding transposes to the BLAS gemm call we could perhaps make\n  // the kYXDepthBatch layout work as well, but there has been no need\n  // for that so far.\n  if (input_dimensions.layout() != dnn::DataLayout::kBatchYXDepth &&\n      input_dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n    LOG(ERROR) << \"Unsupported MatMul input layout.\";\n    return false;\n  }\n  if (output_dimensions.layout() != dnn::DataLayout::kBatchYXDepth &&\n      output_dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n    LOG(ERROR) << \"Unsupported MatMul output layout.\";\n    return false;\n  }\n\n  if (output_dimensions.width() == 1 && output_dimensions.height() == 1) {\n    // This is a fast path that also supports the kBatchYXDepth layout.\n\n    // The matrices here are in row-major format while BLAS expects\n    // column-major, i.e. our matrices are transposed as far as BLAS\n    // is concerned. So we need to compute output^T =\n    // input^T*weights^T. There is no parameter for transposing the\n    // output in BLAS gemm, but instead we can transpose both sides of\n    // the equality to see that this is equivalent to\n    // output=weights*input. So we only need to swap the order of\n    // weights and input in the matrix product to correct for the\n    // row-major versus column-major difference.\n    const float alpha = 1.0f;  // Take the matrix product without scaling it.\n    const float beta = 0.0f;   // Ignore the original values in output_data.\n    const int64 m = output_dimensions.NodesAcrossFeatureMaps();\n    const int64 n = input_dimensions.count();\n    const int64 k = input_dimensions.NodesAcrossFeatureMaps();\n    stream->ThenBlasGemm(blas::Transpose::kNoTranspose,\n                         blas::Transpose::kNoTranspose, m, n, k, alpha, weights,\n                         m, input_data, k, beta, output_data, m);\n  } else {\n    // This is a slower and more complex path that supports output\n    // width() * height() > 1, though it only supports the\n    // kBatchYXDepth layout. Does support kBatchDepthYX if output\n    // feature_map_count() == 1, as then there is no difference\n    // between the two layouts.\n    //\n    // The operation here is the same as above, except that we have to\n    // do the matrix multiplication for each (y,x) output coordinate\n    // separately. We then interpret weights as containing K = width()\n    // * height() different matrices, which we all multiply onto the\n    // matrix from input_data, yielding K matrix products. We then\n    // combine these together into one matrix by concatenating all the\n    // first rows of these matrices, then all the seconds rows and so\n    // on. We can do this with a batched matrix multiplication, where\n    // the result is written to a different submatrix of the output\n    // for each matrix multiplication.\n    //\n    // The reason that we only support the kBatchYXDepth output layout\n    // is that we have to do something in the depth for each (y,x)\n    // coordinate. The kBatchYXDepth layout has the depth information\n    // for each point (y,x) in contiguous memory while the\n    // kBatchDepthYX layout does not.\n    //\n    // TODO(broune): Consider a special case for when output depth ==\n    // 1, as then possibly this could all be done as one matrix\n    // multiplication instead of a batched one, which should be\n    // faster. Another possibility would be to add a weights layout\n    // parameter and then support kBatchDepthYX for a different\n    // weights layout.\n    if (output_dimensions.layout() != dnn::DataLayout::kBatchYXDepth &&\n        !(output_dimensions.layout() == dnn::DataLayout::kBatchDepthYX &&\n          output_dimensions.feature_map_count() == 1)) {\n      LOG(ERROR) << \"Unsupported MatMul output layout.\";\n      return false;\n    }\n\n    const float alpha = 1.0f;  // Take the matrix product without scaling it.\n    const float beta = 0.0f;   // Ignore the original values in output_data.\n    const uint64 m = output_dimensions.feature_map_count();\n    const uint64 n = input_dimensions.count();\n    const uint64 k = input_dimensions.NodesAcrossFeatureMaps();\n    const int lda = m;\n    const int ldb = k;\n    const int ldc = output_dimensions.NodesAcrossFeatureMaps();\n    const int batch_count = output_dimensions.NodesPerFeatureMap();\n\n    std::vector<DeviceMemory<float>> a(batch_count);\n    std::vector<DeviceMemory<float>> b(batch_count);\n    std::vector<DeviceMemory<float>> c(batch_count);\n    for (int i = 0; i < batch_count; ++i) {\n      const int weights_offset = i * input_dimensions.NodesAcrossFeatureMaps() *\n                                 output_dimensions.feature_map_count();\n      a[i] = DeviceMemory<float>::MakeFromByteSize(\n          const_cast<float*>(reinterpret_cast<const float*>(weights.opaque())) +\n              weights_offset,\n          weights.ElementCount() - weights_offset);\n\n      b[i] = input_data;\n\n      const int output_offset = i * output_dimensions.feature_map_count();\n      c[i] = DeviceMemory<float>::MakeFromByteSize(\n          const_cast<float*>(\n              reinterpret_cast<const float*>(output_data->opaque())) +\n              output_offset,\n          output_data->ElementCount() - output_offset);\n    }\n    const auto toPtrs = [](std::vector<DeviceMemory<float>>& v) {\n      std::vector<DeviceMemory<float>*> ptrs;\n      ptrs.reserve(v.size());\n      for (auto& mem : v) {\n        ptrs.push_back(&mem);\n      }\n      return ptrs;\n    };\n\n    stream->ThenBlasGemmBatched(blas::Transpose::kNoTranspose,\n                                blas::Transpose::kNoTranspose, m, n, k, alpha,\n                                toPtrs(a), lda, toPtrs(b), ldb, beta, toPtrs(c),\n                                ldc, batch_count);\n  }\n\n  return stream->ok();\n}\n\nbool CudnnSupport::DoBiasAdd(Stream* stream,\n                             const DeviceMemory<float>& input_data,\n                             const DeviceMemory<float>& biases,\n                             const dnn::BatchDescriptor& dimensions,\n                             DeviceMemory<float>* output_data) {\n  ScopedTensorDescriptor input_descriptor{parent_, dimensions,\n                                          CUDNN_DATA_FLOAT};\n\n  BatchDescriptor bias_dimensions;\n  bias_dimensions.set_count(1)\n      .set_feature_map_count(dimensions.feature_map_count())\n      .set_height(1)\n      .set_width(1)\n      .set_layout(dnn::DataLayout::kBatchYXDepth);\n  ScopedTensorDescriptor bias_descriptor{parent_, bias_dimensions,\n                                         CUDNN_DATA_FLOAT};\n\n  // cudnnAddTensor after R3 is in-place, so we need to copy input_data to\n  // output_data before doing the addition, unless the input and\n  // output are at the same address.\n  if (input_data.opaque() != output_data->opaque()) {\n    stream->ThenMemcpy(output_data, input_data,\n                       dimensions.ElementCount() * sizeof(float));\n    if (!stream->ok()) {\n      LOG(ERROR)\n          << \"stream \" << stream\n          << \" could not enqueue a tensor copy as part of bias addition.\";\n      return false;\n    }\n  }\n\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n    return false;\n  }\n\n  const float alpha = 1.0f;\n  const float beta = 1.0f;\n\n#if CUDNN_VERSION >= 5000\n  status = wrap::cudnnAddTensor(\n#else\n  status = wrap::cudnnAddTensor_v3(\n#endif\n      parent_, ToHandle(dnn_handle_), &alpha, bias_descriptor.handle(),\n      biases.opaque(), &beta, input_descriptor.handle(), output_data->opaque());\n\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"stream \" << stream << \" could not enqueue bias addition.\";\n    return false;\n  }\n\n  return true;\n}\n\nbool CudnnSupport::DoActivate(Stream* stream,\n                              dnn::ActivationMode activation_mode,\n                              const dnn::BatchDescriptor& dimensions,\n                              const DeviceMemory<float>& input_data,\n                              DeviceMemory<float>* output_data,\n                              uint64 options) {\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n    return false;\n  }\n\n#if CUDNN_VERSION >= 5000\n  ScopedActivationDescriptor activation_desc{\n      parent_, activation_mode, CUDNN_PROPAGATE_NAN, dimensions.value_max()};\n#else\n  cudnnActivationMode_t mode;\n  switch (activation_mode) {\n    case dnn::ActivationMode::kRelu6:\n      // TODO(leary) should probably do a post-pass to clip at 6?\n      LOG(WARNING) << \"user requested Relu6, but providing Relu instead\";\n      mode = CUDNN_ACTIVATION_RELU;\n      break;\n    case dnn::ActivationMode::kReluX:\n      // TODO(broune) should probably do a post-pass to clip at X?\n      LOG(WARNING) << \"user requested ReluX, but providing Relu instead\";\n      mode = CUDNN_ACTIVATION_RELU;\n      break;\n    case dnn::ActivationMode::kRelu:\n      mode = CUDNN_ACTIVATION_RELU;\n      break;\n    case dnn::ActivationMode::kSigmoid:\n      mode = CUDNN_ACTIVATION_SIGMOID;\n      break;\n    case dnn::ActivationMode::kTanh:\n      mode = CUDNN_ACTIVATION_TANH;\n      break;\n    default:\n      LOG(ERROR) << \"unrecognized activation mode: \"\n                 << static_cast<int>(activation_mode);\n      return false;\n  }\n#endif\n\n  ScopedTensorDescriptor input_nd{parent_, dimensions, CUDNN_DATA_FLOAT};\n  // Alpha is the input scaling factor.\n  float alpha = 1.0;\n  // Beta is the output scaling factor.\n  float beta = 0.0;\n  status = wrap::cudnnActivationForward(\n      parent_, ToHandle(dnn_handle_),\n#if CUDNN_VERSION >= 5000\n      activation_desc.handle(),\n#else\n      mode,\n#endif\n      &alpha, input_nd.handle(), input_data.opaque(), &beta, input_nd.handle(),\n      output_data->opaque());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"stream \" << stream\n               << \" could not enqueue activation: \" << ToString(status);\n    return false;\n  }\n\n  return true;\n}\n\nbool CudnnSupport::DoPoolForward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<double>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<double>* output_data) {\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n    return false;\n  }\n\n  // Alpha is the scaling factor for input.\n  double alpha = 1.0;\n  // Beta is the scaling factor for output.\n  double beta = 0.0;\n\n  ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_DOUBLE};\n  ScopedTensorDescriptor dest_desc{parent_, output_dimensions,\n                                   CUDNN_DATA_DOUBLE};\n  ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n  status = wrap::cudnnPoolingForward(\n      parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n      src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(),\n      output_data->opaque());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to enqueue forward pooling on stream: \"\n               << ToString(status);\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoPoolForward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<float>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<float>* output_data) {\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n    return false;\n  }\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_FLOAT};\n  ScopedTensorDescriptor dest_desc{parent_, output_dimensions,\n                                   CUDNN_DATA_FLOAT};\n  ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n  status = wrap::cudnnPoolingForward(\n      parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n      src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(),\n      output_data->opaque());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to enqueue forward pooling on stream: \"\n               << ToString(status);\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoPoolForward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<Eigen::half>* output_data) {\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n    return false;\n  }\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_HALF};\n  ScopedTensorDescriptor dest_desc{parent_, output_dimensions, CUDNN_DATA_HALF};\n  ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n  status = wrap::cudnnPoolingForward(\n      parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n      src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(),\n      output_data->opaque());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to enqueue forward pooling on stream: \"\n               << ToString(status);\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoPoolBackward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<double>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    const DeviceMemory<double>& output_data,\n    const DeviceMemory<double>& input_diff_data,\n    DeviceMemory<double>* output_diff_data) {\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n    return false;\n  }\n\n  // Alpha is the scaling factor for input.\n  double alpha = 1.0;\n  // Beta is the scaling factor for output.\n  double beta = 0.0;\n\n  ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_DOUBLE};\n  ScopedTensorDescriptor dest_desc{parent_, output_dimensions,\n                                   CUDNN_DATA_DOUBLE};\n  ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n  status = wrap::cudnnPoolingBackward(\n      parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n      dest_desc.handle(), output_data.opaque(), dest_desc.handle(),\n      input_diff_data.opaque(), src_desc.handle(), input_data.opaque(), &beta,\n      src_desc.handle(), output_diff_data->opaque());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to enqueue backward pooling on stream: \"\n               << ToString(status);\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoPoolBackward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<float>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    const DeviceMemory<float>& output_data,\n    const DeviceMemory<float>& input_diff_data,\n    DeviceMemory<float>* output_diff_data) {\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n    return false;\n  }\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_FLOAT};\n  ScopedTensorDescriptor dest_desc{parent_, output_dimensions,\n                                   CUDNN_DATA_FLOAT};\n  ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n  status = wrap::cudnnPoolingBackward(\n      parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n      dest_desc.handle(), output_data.opaque(), dest_desc.handle(),\n      input_diff_data.opaque(), src_desc.handle(), input_data.opaque(), &beta,\n      src_desc.handle(), output_diff_data->opaque());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to enqueue backward pooling on stream: \"\n               << ToString(status);\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoPoolBackward(\n    Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n    const dnn::BatchDescriptor& input_dimensions,\n    const DeviceMemory<Eigen::half>& input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    const DeviceMemory<Eigen::half>& output_data,\n    const DeviceMemory<Eigen::half>& input_diff_data,\n    DeviceMemory<Eigen::half>* output_diff_data) {\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n    return false;\n  }\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0;\n  // Beta is the scaling factor for output.\n  float beta = 0.0;\n\n  ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_HALF};\n  ScopedTensorDescriptor dest_desc{parent_, output_dimensions, CUDNN_DATA_HALF};\n  ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n  status = wrap::cudnnPoolingBackward(\n      parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n      dest_desc.handle(), output_data.opaque(), dest_desc.handle(),\n      input_diff_data.opaque(), src_desc.handle(), input_data.opaque(), &beta,\n      src_desc.handle(), output_diff_data->opaque());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to enqueue backward pooling on stream: \"\n               << ToString(status);\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoNormalize(\n    Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n    const DeviceMemory<float>& input_data, DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool CudnnSupport::DoNormalizeWithDimensions(\n    Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n    const dnn::BatchDescriptor& dimensions,\n    const DeviceMemory<float>& input_data, DeviceMemory<float>* output_data) {\n  // Check for unsupported modes.\n  if (normalize_descriptor.wrap_around()) {\n    LOG(ERROR) << \"CUDA LRN does not support wrap-around mode\";\n    return false;\n  }\n  if (normalize_descriptor.segment_size()) {\n    LOG(ERROR) << \"CUDA LRN does not support segmentation\";\n    return false;\n  }\n\n  // Launch the normalization.\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n    return false;\n  }\n\n  ScopedTensorDescriptor dims{parent_, dimensions, CUDNN_DATA_FLOAT};\n  ScopedNormalizeDescriptor normalize{parent_, normalize_descriptor};\n\n  // Alpha is the scaling factor for input.\n  float alpha = 1.0f;\n  // Beta is the scaling factor for output.\n  float beta = 0.0f;\n\n  status = wrap::cudnnLRNCrossChannelForward(\n      parent_, ToHandle(dnn_handle_), normalize.handle(),\n      CUDNN_LRN_CROSS_CHANNEL_DIM1, &alpha, dims.handle(), input_data.opaque(),\n      &beta, dims.handle(), output_data->opaque());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to run cudnnLRNCrossChannelForward\";\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoNormalizeBackwardWithDimensions(\n    Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n    const dnn::BatchDescriptor& dimensions, const DeviceMemory<float>& raw_data,\n    const DeviceMemory<float>& normalized_data,\n    const DeviceMemory<float>& normalized_variable_gradient,\n    DeviceMemory<float>* raw_variable_gradient) {\n  // Check for unsupported modes.\n  if (normalize_descriptor.wrap_around()) {\n    LOG(ERROR) << \"CUDA LRN does not support wrap-around mode\";\n    return false;\n  }\n  if (normalize_descriptor.segment_size()) {\n    LOG(ERROR) << \"CUDA LRN does not support segmentation\";\n    return false;\n  }\n\n  mutex_lock lock{dnn_handle_mutex_};\n  auto status = wrap::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n                                     AsCUDAStreamValue(stream));\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n    return false;\n  }\n\n  ScopedTensorDescriptor dims{parent_, dimensions, CUDNN_DATA_FLOAT};\n  ScopedNormalizeDescriptor normalize{parent_, normalize_descriptor};\n\n  float alpha = 1.0f;\n  float beta = 0.0f;\n\n  status = wrap::cudnnLRNCrossChannelBackward(\n      parent_, ToHandle(dnn_handle_), normalize.handle(),\n      CUDNN_LRN_CROSS_CHANNEL_DIM1, &alpha, dims.handle(),\n      normalized_data.opaque(), dims.handle(),\n      normalized_variable_gradient.opaque(), dims.handle(), raw_data.opaque(),\n      &beta, dims.handle(), raw_variable_gradient->opaque());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"failed to run cudnnLRNCrossChannelBackward\";\n    return false;\n  }\n  return true;\n}\n\nbool CudnnSupport::DoDepthConcatenate(\n    Stream* stream, port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n    port::ArraySlice<const DeviceMemory<float>*> input_data,\n    DeviceMemory<float>* output_data) {\n  CHECK_EQ(input_dimensions.size(), input_data.size());\n\n  for (const auto& dimensions : input_dimensions) {\n    if (dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n      LOG(ERROR) << \"CudnnSupport::DoDepthConcatenate currently only \"\n                    \"supports the kBatchDepthYX layout.\";\n      return false;\n    }\n  }\n\n  if (input_dimensions.empty()) {\n    return true;  // Nothing to do.\n  }\n\n  dnn::BatchDescriptor output_dimensions =\n      dnn::BatchDescriptor::DepthConcatenateOutputDescriptor(input_dimensions);\n\n  const int64 area = output_dimensions.width() * output_dimensions.height();\n  const auto index = [area](int64 batch, int64 depth, int64 yx,\n                            int64 max_depth) {\n    return (batch * max_depth + depth) * area + yx;\n  };\n\n  std::vector<float> output_host(output_dimensions.ElementCount());\n  std::vector<float> tmp;\n  int64 depth_sum = 0;\n  for (size_t i = 0; i < input_data.size(); ++i) {\n    const auto& dimensions = input_dimensions[i];\n    tmp.resize(dimensions.ElementCount());\n    stream->ThenMemcpyD2H<float>(*input_data[i], &tmp);\n    port::Status block_status = stream->BlockHostUntilDone();\n    if (!block_status.ok()) {\n      LOG(ERROR) << \"BlockHostUntilDone failed: \" << block_status;\n      return false;\n    }\n\n    for (int64 batch = 0; batch < output_dimensions.count(); ++batch) {\n      for (int64 yx = 0; yx < area; ++yx) {\n        for (int64 depth = 0; depth < dimensions.feature_map_count(); ++depth) {\n          LOG(INFO) << output_dimensions.ElementCount() << ' ' << batch << ' '\n                    << yx << ' ' << depth;\n          output_host[index(batch, depth + depth_sum, yx,\n                            output_dimensions.feature_map_count())] =\n              tmp[index(batch, depth, yx, dimensions.feature_map_count())];\n        }\n      }\n    }\n    depth_sum += dimensions.feature_map_count();\n  }\n  stream->ThenMemcpyH2D<float>(output_host, output_data);\n  return true;\n}\n\nbool CudnnSupport::DoElementwiseOperate(\n    Stream* stream, dnn::ElementwiseOperation operation,\n    port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n    port::ArraySlice<const DeviceMemory<float>*> input_data,\n    const dnn::BatchDescriptor& output_dimensions,\n    DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool CudnnSupport::DoXYPad(Stream* stream,\n                           const dnn::BatchDescriptor& dimensions,\n                           const DeviceMemory<float>& input_data,\n                           int64 left_pad, int64 right_pad, int64 top_pad,\n                           int64 bottom_pad, DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool CudnnSupport::DoXYSlice(Stream* stream,\n                             const dnn::BatchDescriptor& dimensions,\n                             const DeviceMemory<float>& input_data,\n                             int64 left_trim, int64 right_trim, int64 top_trim,\n                             int64 bottom_trim,\n                             DeviceMemory<float>* output_data) {\n  LOG(FATAL) << \"not yet implemented\";  // TODO(leary)\n  return false;\n}\n\nbool CudnnSupport::DoMemcpyD2HQuantized(\n    Stream* stream, const DeviceMemory<float>& gpu_unquantized_src,\n    dnn::QuantizedActivationMode mode, void* host_dst, int64 size) {\n  LOG(ERROR) << \"quantized memcpy not supported by cuDNN\";\n  return false;\n}\n\nbool CudnnSupport::DoMemcpyH2DQuantized(\n    Stream* stream, const void* host_src, int64 size,\n    dnn::QuantizedActivationMode mode,\n    DeviceMemory<float>* gpu_unquantized_dst) {\n  LOG(ERROR) << \"quantized memcpy not supported by cuDNN\";\n  return false;\n}\n\nbool CudnnSupport::DeriveOutputBatchDescriptor(\n    const BatchDescriptor& batch_descriptor,\n    const FilterDescriptor& filter_descriptor,\n    const dnn::ConvolutionDescriptor& convolution_descriptor,\n    dnn::BatchDescriptor* output_batch_descriptor) {\n  ScopedTensorDescriptor input_nd{parent_, batch_descriptor, CUDNN_DATA_FLOAT};\n  ScopedFilterDescriptor filter{parent_, filter_descriptor, batch_descriptor,\n                                CUDNN_DATA_FLOAT};\n  ScopedConvolutionDescriptor conv{parent_, convolution_descriptor,\n                                   CUDNN_DATA_FLOAT};\n\n  int dn = batch_descriptor.ndims() + 2;\n  std::vector<int> dims(dn);  // in BDYX\n  auto status = wrap::cudnnGetConvolutionNdForwardOutputDim(\n      parent_, conv.handle(), input_nd.handle(), filter.handle(), dn,\n      dims.data());\n  if (status != CUDNN_STATUS_SUCCESS) {\n    LOG(ERROR) << \"could not get output tensor for convolution: \"\n               << ToString(status);\n    return false;\n  }\n\n  output_batch_descriptor->set_count(dims[0])\n      .set_feature_map_count(dims[1])\n      .set_layout(batch_descriptor.layout());\n\n  for (int i = 0; i < batch_descriptor.ndims(); i++) {\n    output_batch_descriptor->set_spatial_dim(static_cast<dnn::DimIndex>(i),\n                                             dims.rbegin()[i]);\n  }\n\n  return true;\n}\n\n}  // namespace cuda\n\nnamespace gpu = ::perftools::gputools;\n\nvoid initialize_cudnn() {\n  gpu::port::Status status =\n      gpu::PluginRegistry::Instance()\n          ->RegisterFactory<gpu::PluginRegistry::DnnFactory>(\n              gpu::cuda::kCudaPlatformId, gpu::cuda::kCuDnnPlugin, \"cuDNN\",\n              [](gpu::internal::StreamExecutorInterface*\n                     parent) -> gpu::dnn::DnnSupport* {\n                gpu::cuda::CUDAExecutor* cuda_executor =\n                    dynamic_cast<gpu::cuda::CUDAExecutor*>(parent);\n                if (cuda_executor == nullptr) {\n                  LOG(ERROR)\n                      << \"Attempting to initialize an instance of the cuBLAS \"\n                      << \"support library with a non-CUDA StreamExecutor\";\n                  return nullptr;\n                }\n\n                gpu::cuda::CudnnSupport* dnn =\n                    new gpu::cuda::CudnnSupport(cuda_executor);\n                if (!dnn->Init().ok()) {\n                  // Note: Init() will log a more specific error.\n                  delete dnn;\n                  return nullptr;\n                }\n                return dnn;\n              });\n\n  if (!status.ok()) {\n    LOG(ERROR) << \"Unable to register cuDNN factory: \"\n               << status.error_message();\n  }\n\n  gpu::PluginRegistry::Instance()->SetDefaultFactory(gpu::cuda::kCudaPlatformId,\n                                                     gpu::PluginKind::kDnn,\n                                                     gpu::cuda::kCuDnnPlugin);\n}\n\n}  // namespace gputools\n}  // namespace perftools\n\nREGISTER_MODULE_INITIALIZER(register_cudnn,\n                            { perftools::gputools::initialize_cudnn(); });\n", "subject": "Merge pull request #2997 from maciekcc/branch_125575345", "message": "Merge pull request #2997 from maciekcc/branch_125575345\n\nBranch 125575345" }
{ "index": {"_index": "github_commits", "_type": "commit"} }
{ "repo_name": "tensorflow/tensorflow", "file": "common_shape_fns.h", "language": "h", "commit_date": "2016-06-22 22:16:40.000 UTC", "content": "/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_CORE_OPS_COMMON_SHAPE_FNS_H_\n#define TENSORFLOW_CORE_OPS_COMMON_SHAPE_FNS_H_\n\n#include <array>\n\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\n\n// GetWindowedOutputSize(): Given an input tensor, kernel, stride and padding\n// type, the function computes the output and padding dimensions.\n//\n// For example, ignoring batches or multiple features, a 1D convolution\n// takes as input a 1D tensor of shape (H), and convolves it with a filter of\n// shape (K).\n//\n// It also takes in a few additional parameters:\n//\n// Stride (S): the stride with which we apply the filters. This is the offset\n// between locations where we apply the filters. A larger stride\n// means that the output will be spatially smaller.\n//\n// Padding (P): the padding we apply to the input tensor along each\n// dimension. This is usually used to make sure that the spatial dimensions\n// do not shrink when we progress with convolutions. Two types of padding are\n// often used:\n//   SAME: the pad value is computed so that the output will have size H/S.\n//   VALID: no padding is carried out.\n// The padded area is zero-filled.\n//\n// The output dimensions for convolution and many other operations, when given\n// all the parameters above, are as follows:\n// - When Padding = SAME: the output size is (H'), where\n//     H' = ceil(float(H) / float(S))\n//   where ceil is the ceiling function. The number of padded cells\n//   is computed as:\n//     Pc = ((H' - 1) * S + K - H) / 2\n//   When the stride is 1, the expression simplifies to\n//     H' = H, Pc = (K-1)/2.\n//   This is where SAME comes from - the output has the same size as the input\n//   has.\n//\n// - When Padding = VALID: the output size is computed as\n//     H' = ceil(float(H - K + 1) / float(S))\n//   and the number of padded cells is always zero.\n//   When the stride is 1, the expression simplifies to\n//     H' = H-K+1.\n//\n// For convolution, mathematically, the output value at location (r')\n// is the inner product of two vectors: the chunk of input at\n//    ((r'*S-Pr) : (r'*S-Pr+K)),\n// and the filter.\n//\n// For 2D and 3D convolutions, the spatial dimensions are orthogonal, so the\n// size and padding of each spatial dimension can be computed by calling\n// GetWindowedOutputSize separately for each dimension.\n//\nStatus GetWindowedOutputSize(int64 input_size, int64 filter_size, int64 stride,\n                             Padding padding_type, int64* output_size,\n                             int64* padding_size);\n\n// The V2 version computes the same outputs with arbitrary dilation_rate.\n// The output dimensions are computed as follows:\n// - When adding dilation_rate (D), we compute an effective filter size (K'):\n//     K' = (K - 1) * D + 1\n// - When Padding = SAME: the output size is (H'), where\n//     H' = ceil(float(H) / float(S))\n//   where ceil is the ceiling function. The number of padded cells\n//   is computed as:\n//     Pc = ((H' - 1) * S + K' - H) / 2\n//   When the stride is 1, the expression simplifies to\n//     H' = H, Pc = (K'-1)/2.\n//   This is where SAME comes from - the output has the same size as the input\n//   has.\n//\n// - When Padding = VALID: the output size is computed as\n//     H' = ceil(float(H - K' + 1) / float(S))\n//   and the number of padded cells is always zero.\n//   When the stride is 1, the expression simplifies to\n//     H' = H-K'+1.\n//\n// TODO(b/67112639): Merge V2 versions and the original versions eventually.\nStatus GetWindowedOutputSizeV2(int64 input_size, int64 filter_size,\n                               int64 dilation_rate, int64 stride,\n                               Padding padding_type, int64* output_size,\n                               int64* padding_size);\n\n// Returns the same output dimensions as in GetWindowedOutputSize, but returns\n// verbose padding dimensions (before/after). Any excess padding\n// (caused by an odd padding size value) is added to the 'padding_after'\n// dimension.\nStatus GetWindowedOutputSizeVerbose(int64 input_size, int64 filter_size,\n                                    int64 stride, Padding padding_type,\n                                    int64* output_size, int64* padding_before,\n                                    int64* padding_after);\n\n// The V2 version computes the same outputs with arbitrary dilation_rate. For\n// detailed equations, refer to the comments for GetWindowedOutputSizeV2().\nStatus GetWindowedOutputSizeVerboseV2(int64 input_size, int64 filter_size,\n                                      int64 dilation_rate, int64 stride,\n                                      Padding padding_type, int64* output_size,\n                                      int64* padding_before,\n                                      int64* padding_after);\n\n// Given an input tensor, kernel, stride and padding type, populates the 3D size\n// of the output tensor and padding to be applied to the input tensor at the\n// lower end of every dimension. Use for 3D convolutions, where the input data\n// is padded with zeros, as well as for 3D avg/max pooling, where the input data\n// is padded with invalid values that are not considered for pooling.\nStatus Get3dOutputSize(const std::array<int64, 3>& input,\n                       const std::array<int64, 3>& window,\n                       const std::array<int64, 3>& strides,\n                       Padding padding_type, std::array<int64, 3>* output_ptr,\n                       std::array<int64, 3>* padding_ptr);\n\n// The V2 version computes the same outputs with arbitrary dilation_rate. For\n// detailed equations, refer to the comments for GetWindowedOutputSizeV2().\nStatus Get3dOutputSizeV2(const std::array<int64, 3>& input,\n                         const std::array<int64, 3>& window,\n                         const std::array<int64, 3>& dilations,\n                         const std::array<int64, 3>& strides,\n                         Padding padding_type, std::array<int64, 3>* output_ptr,\n                         std::array<int64, 3>* padding_ptr);\n\nnamespace shape_inference {\n\n// Like GetWindowedOutputSize, but deals with DimensionHandles.\nStatus GetWindowedOutputSizeFromDims(InferenceContext* c,\n                                     DimensionHandle input_size,\n                                     DimensionOrConstant filter_size,\n                                     int64 stride, Padding padding_type,\n                                     DimensionHandle* output_size);\n\n// The V2 version computes the same outputs with arbitrary dilation_rate. For\n// detailed equations, refer to the comments for GetWindowedOutputSizeV2().\nStatus GetWindowedOutputSizeFromDimsV2(InferenceContext* c,\n                                       DimensionHandle input_size,\n                                       DimensionOrConstant filter_size,\n                                       int64 dilation_rate, int64 stride,\n                                       Padding padding_type,\n                                       DimensionHandle* output_size);\n\n// Transfers shape of input(0) to output(0).\nStatus UnchangedShape(shape_inference::InferenceContext* c);\n\n// Transfers shape of input(0) to output(0), after asserting its rank is <rank>.\ninline Status UnchangedShapeWithRank(shape_inference::InferenceContext* c,\n                                     int32 rank) {\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), rank, &out));\n  c->set_output(0, out);\n  return Status::OK();\n}\n\n// Transfers shape of input(0) to output(0), after asserting its rank >= <rank>.\ninline Status UnchangedShapeWithRankAtLeast(\n    shape_inference::InferenceContext* c, int32 rank) {\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), rank, &out));\n  c->set_output(0, out);\n  return Status::OK();\n}\n\n// Transfers shape of input(0) to output(0), after asserting its rank <= <rank>.\ninline Status UnchangedShapeWithRankAtMost(shape_inference::InferenceContext* c,\n                                           int32 rank) {\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), rank, &out));\n  c->set_output(0, out);\n  return Status::OK();\n}\n\n// Shape function for use with ops no outputs.\ninline Status NoOutputs(shape_inference::InferenceContext* c) {\n  return Status::OK();\n}\n\n// Shape function for ops that output a single scalar value.\ninline Status ScalarShape(shape_inference::InferenceContext* c) {\n  c->set_output(0, c->Scalar());\n  return Status::OK();\n}\n\n// Shape function for binary ops where both inputs and the output match.\ninline Status MergeBothInputsShapeFn(InferenceContext* c) {\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(1), &out));\n  c->set_output(0, out);\n  return Status::OK();\n}\n\n// Returns a new shape with the specified dims arranged in the specified\n// format. The returned value is owned by this context.\n// Note: if format = \"FORMAT_NCHW_VECT_C\" then C represents the outer_depth.\nStatus MakeShapeFromFormat(TensorFormat format, DimensionOrConstant N,\n                           const std::vector<DimensionOrConstant>& spatial,\n                           DimensionOrConstant C, ShapeHandle* out,\n                           shape_inference::InferenceContext* context);\n\n// Shape function for MatMul-like operations.\nStatus MatMulShape(shape_inference::InferenceContext* c);\n\n// Shape function for BiasAdd-like operations.\nStatus BiasAddShape(shape_inference::InferenceContext* c);\n\n// Shape function for BiasAddGrad-like operations.\nStatus BiasAddGradShape(shape_inference::InferenceContext* c);\n\n// Shape function for Conv2D-like operations.\nStatus Conv2DShape(shape_inference::InferenceContext* c);\n\n// Shape function for Conv3D-like operations.\nStatus Conv3DShape(shape_inference::InferenceContext* c);\n\n// Shape function for DepthwiseConv2D-like operations.\nStatus DepthwiseConv2DNativeShape(shape_inference::InferenceContext* c);\n\n// Shape function for AvgPool-like operations.\nStatus AvgPoolShape(shape_inference::InferenceContext* c);\n\n// Shape function for FusedBatchNorm and FusedBatchNormV2 operations.\nStatus FusedBatchNormShape(shape_inference::InferenceContext* c);\n\n// Shape function for FusedBatchNormGrad and FusedBatchNormGradV2 operations.\nStatus FusedBatchNormGradShape(shape_inference::InferenceContext* c);\n\n// Shape function for MaxPool-like operations.\nStatus MaxPoolShape(shape_inference::InferenceContext* c);\n\n// Shape function for MaxPoolV2-like operations.\nStatus MaxPoolV2Shape(shape_inference::InferenceContext* c, int num_inputs);\n\n// Shape function for 3D Pooling operations.\nStatus Pool3DShape(shape_inference::InferenceContext* c);\n\n// Shape function for use with ops whose output shapes are unknown.\nStatus UnknownShape(shape_inference::InferenceContext* c);\n\n// Shape function for reduction operations.\nStatus ReductionShape(shape_inference::InferenceContext* c);\n\n// Shape function for concat operations.\n// <num_inputs_to_concat> is the number of inputs to concatenate and are taken\n// from inputs\n// [1,num_inputs_to_concat] of the op.  Input 0 is the concat_dim input.\nStatus ConcatShape(shape_inference::InferenceContext* c,\n                   int num_inputs_to_concat);\n\n// Shape function for concat operations.\nStatus ConcatV2Shape(shape_inference::InferenceContext* c);\n\n// Shape function for binary operators that broadcast their inputs.\n// Tested by ops/math_ops_test.cc.\nStatus BroadcastBinaryOpShapeFn(InferenceContext* c);\n\n// Shape function for random operations.\nStatus RandomShape(shape_inference::InferenceContext* c);\n\n// Validates the 3 component tensors of a sparse tensor have the proper\n// shapes. This mimics SparseTensor.__init__ in python/framework/ops.py.\nStatus ValidateSparseTensor(InferenceContext* c, ShapeHandle indices_shape,\n                            ShapeHandle values_shape, ShapeHandle shape_shape);\n\n// Shape function for ScatterNd update/add/sub/... operations.\nStatus ScatterNdUpdateShape(InferenceContext* c);\n\n// Shape function for ops with an explicit \"shape\" attribute.\nStatus ExplicitShape(InferenceContext* c);\n\n}  // namespace shape_inference\n\n}  // namespace tensorflow\n\n#endif  // TENSORFLOW_CORE_OPS_COMMON_SHAPE_FNS_H_\n", "subject": "Merge pull request #2997 from maciekcc/branch_125575345", "message": "Merge pull request #2997 from maciekcc/branch_125575345\n\nBranch 125575345" }

